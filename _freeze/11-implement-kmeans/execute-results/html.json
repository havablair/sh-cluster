{
  "hash": "7879f79f67f70dc035c2331d3bf795a8",
  "result": {
    "markdown": "# Implement k-means {#sec-run-km}\n\n## Overview\n\nIn this section, I will use the cleaned dataset created in the last chapter to build a k-means pipeline that runs a model for a range of different cluster sizes (`k`).\n\nAfter fitting the models, I review some common metrics for determining best cluster sizes, and save the model objects for further investigation in the next chapter.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glue)\nlibrary(tidyclust)\nlibrary(factoextra) # trying fviz_nbclust(), which gives elbow, silhouette, and gap statistic\nlibrary(hopkins)\nlibrary(fpc)\n\n\nd <- read_csv(\"./data/clean_mu_weighted_soil_props.csv\") %>% \n  select(-contains(\"comp_pct\"))\n\nold_names <- colnames(d)\n\nnew_names <- str_replace_all(old_names, \"_r_value\", \"\")\n\ncolnames(d) <- new_names\n```\n:::\n\n\nReminder of what the data look like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(d)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mukey\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"claytotal\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"om\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"cec7\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"dbthirdbar\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ec\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ph1to1h2o\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"caco3\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lep\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ksat\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"awc\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"395973\",\"2\":\"3.000000\",\"3\":\"1.50\",\"4\":\"4.50000\",\"5\":\"1.600\",\"6\":\"0\",\"7\":\"6.000000\",\"8\":\"0\",\"9\":\"0.3\",\"10\":\"282.00000\",\"11\":\"0.0800000\"},{\"1\":\"395975\",\"2\":\"3.000000\",\"3\":\"1.00\",\"4\":\"4.50000\",\"5\":\"1.600\",\"6\":\"0\",\"7\":\"6.000000\",\"8\":\"0\",\"9\":\"0.3\",\"10\":\"282.00000\",\"11\":\"0.0800000\"},{\"1\":\"395900\",\"2\":\"5.000000\",\"3\":\"4.50\",\"4\":\"11.37500\",\"5\":\"1.435\",\"6\":\"0\",\"7\":\"7.250000\",\"8\":\"0\",\"9\":\"1.5\",\"10\":\"74.75000\",\"11\":\"0.0975000\"},{\"1\":\"395948\",\"2\":\"19.500000\",\"3\":\"5.00\",\"4\":\"21.00000\",\"5\":\"1.400\",\"6\":\"0\",\"7\":\"6.500000\",\"8\":\"0\",\"9\":\"1.5\",\"10\":\"9.00000\",\"11\":\"0.2200000\"},{\"1\":\"885885\",\"2\":\"7.155556\",\"3\":\"4.95\",\"4\":\"17.81667\",\"5\":\"1.437\",\"6\":\"0\",\"7\":\"6.416667\",\"8\":\"0\",\"9\":\"1.5\",\"10\":\"38.27778\",\"11\":\"0.1396111\"},{\"1\":\"395974\",\"2\":\"3.000000\",\"3\":\"1.00\",\"4\":\"4.50000\",\"5\":\"1.600\",\"6\":\"0\",\"7\":\"6.000000\",\"8\":\"0\",\"9\":\"0.3\",\"10\":\"282.00000\",\"11\":\"0.0800000\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nReminder of the transformations I chose to improve variable distributions and make them normal-ish.\n\n-   Square root: clay, carbonates\n-   Log10: organic matter, cec, lep, ksat, awc\n-   Cube (\\^3): bulk density\n-   None: ec, ph\n\n## Pre-process data (recipe)\n\nThis is something specific to the `tidymodels` modeling workflow, and I like it because it's very explicit about how variables are transformed (pre-processed) prior to initializing the model.\n\nHere I apply some transformations to achieve more normal distributions, and then standardize (`step_normalize`) by subtracting the mean and dividing by 1 sd.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_spec <-   recipe(~., data = d) %>% \n    update_role(mukey, new_role = \"ID\") %>% \n  # note this is log10 (the default is ln)\n    step_log(om, cec7, ksat, awc, lep, base = 10) %>% \n    step_mutate(dbthirdbar = dbthirdbar^3) %>% \n    step_sqrt(claytotal, caco3) %>% \n    step_normalize(all_numeric_predictors())\n\nrec_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n predictor         10\n\nOperations:\n\nLog transformation on om, cec7, ksat, awc, lep\nVariable mutation for dbthirdbar^3\nSquare root transformation on claytotal, caco3\nCentering and scaling for all_numeric_predictors()\n```\n:::\n:::\n\n\n### Check that it worked\n\nAlthough I did make plots of the data distributions after transforming in @sec-var-trans , I wanted to do it again here as a check that the pre-processing that I am specifying is working as intended.\nHere, I do that with two functions from `{recipes}` :\n\n-   `prep()` estimates a pre-processing recipe. It takes my input dataset (\"training set\") , and estimates the parameters (model inputs), reporting back on the specific operations and missing data\n-   `bake()` to return the training data (because I set `new_data` to NULL)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# retain argument here tells prep to keep \n# the pre-processed training data \n# note this can make the final recipe size large, \n# so this is not the recipe object I probably want to use\n# in my list col below\ncheck_prep <- prep(rec_spec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \ncheck_prepped_df <- bake(check_prep, new_data = NULL)\n\nhead(check_prepped_df)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mukey\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"claytotal\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"om\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"cec7\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"dbthirdbar\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ec\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ph1to1h2o\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"caco3\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lep\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ksat\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"awc\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"395973\",\"2\":\"-2.22113438\",\"3\":\"-1.2629310\",\"4\":\"-2.2652615\",\"5\":\"2.0029620\",\"6\":\"-0.2585251\",\"7\":\"-1.3797942\",\"8\":\"-0.6709124\",\"9\":\"-2.4016031\",\"10\":\"2.8156237\",\"11\":\"-2.7471204\"},{\"1\":\"395975\",\"2\":\"-2.22113438\",\"3\":\"-1.8661187\",\"4\":\"-2.2652615\",\"5\":\"2.0029620\",\"6\":\"-0.2585251\",\"7\":\"-1.3797942\",\"8\":\"-0.6709124\",\"9\":\"-2.4016031\",\"10\":\"2.8156237\",\"11\":\"-2.7471204\"},{\"1\":\"395900\",\"2\":\"-1.78551370\",\"3\":\"0.3714129\",\"4\":\"-0.6369944\",\"5\":\"0.4852741\",\"6\":\"-0.2585251\",\"7\":\"0.6811591\",\"8\":\"-0.6709124\",\"9\":\"-0.2975952\",\"10\":\"1.6217541\",\"11\":\"-2.0423657\"},{\"1\":\"395948\",\"2\":\"0.09849231\",\"3\":\"0.5281519\",\"4\":\"0.4395225\",\"5\":\"0.2046342\",\"6\":\"-0.2585251\",\"7\":\"-0.5554129\",\"8\":\"-0.6709124\",\"9\":\"-0.2975952\",\"10\":\"-0.2817043\",\"11\":\"0.8567094\"},{\"1\":\"885885\",\"2\":\"-1.40615716\",\"3\":\"0.5132006\",\"4\":\"0.1508824\",\"5\":\"0.5017313\",\"6\":\"-0.2585251\",\"7\":\"-0.6928099\",\"8\":\"-0.6709124\",\"9\":\"-0.2975952\",\"10\":\"1.0199632\",\"11\":\"-0.7633977\"},{\"1\":\"395974\",\"2\":\"-2.22113438\",\"3\":\"-1.8661187\",\"4\":\"-2.2652615\",\"5\":\"2.0029620\",\"6\":\"-0.2585251\",\"7\":\"-1.3797942\",\"8\":\"-0.6709124\",\"9\":\"-2.4016031\",\"10\":\"2.8156237\",\"11\":\"-2.7471204\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# save the pre-processed data for making a \n# correlation matrix\n\nwrite_csv(check_prepped_df, \"./data/data_preprocessed_all_var.csv\")\n```\n:::\n\n\nOK, now making a plot of the transformed variables to take a look at their distributions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_prepped_df %>% \n  select(-mukey) %>% \n  pivot_longer(cols = everything(), names_to = \"var\", values_to = \"value\")  %>% \n  ggplot(aes(x = value)) + \n  geom_histogram(bins =25) + \n  facet_wrap(vars(var), scales = \"free\") + \n  theme_bw() + \n  ggtitle(\"Distributions of transformed and standardized vars\")\n```\n\n::: {.cell-output-display}\n![](11-implement-kmeans_files/figure-html/dist-plot-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n## Set model options {#sec-mod-opt}\n\nDo I need to think about the initializer here?\nBased on what I was reading in the book chapter by Tan et al., 2018 (\"Cluster Analysis: Basic Concepts and Algorithms\"), it sounds like using kmeans++ for the initialization could results in better clustering (lower SSE).\nSee page 543.\nSee also [this paper](http://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf) referenced in the clusterR documentation about seeding (initialization).\n\n`stats::kmeans()` the default seems to be Hartigan-Wong method, which uses **random partition** for the initialization.\nFrom [the {tidyclust} documentation](https://emilhvitfeldt.github.io/tidyclust/articles/k_means.html#a-brief-introduction-to-the-k-means-algorithm):\n\n> The observations are assigned to a cluster uniformly at random.\n> The centroid of each cluster is computed, and these are used as the initial centers.\n\nI guess the other benefit of the Hartigan-Wong method is that it results in more consistent human-verified clusters (again, per tidyclust documentation listed above).\nCould read [this blog post](https://towardsdatascience.com/three-versions-of-k-means-cf939b65f4ea) for a deeper dive into these methods if needed.\n\n`ClusterR::KMeans_rcpp()` uses the Lloyd/Forgy method (per the tidyclust docs, this info wasn't easy to find in the ClusterR docs)\n\nFor now I'm going with `stats::kmeans`, and setting the nstart to 10.\nBecause the random initial configuration (random starting centroids) can have an impact on the final clusters, it sounds like it's a good idea to do multiple starts, and then let kmeans return the best one (using lowest within cluster SSE as the metric for \"best\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# writing a custom function here so I can be explicit \n# about the options I'm choosing, and also use within the \n# list-col framework I set up with map() below. \nkm_spec <- function(nclust){\n  \n  tidyclust::k_means(num_clusters = nclust) %>%\n    parsnip::set_engine(engine = \"stats\",\n               nstart = 10, # 1 is default, >1 recommended\n               algorithm = \"Hartigan-Wong\", # H-W is default\n               iter.max = 20) # default is 10, wasn't always enough\n  \n}\n```\n:::\n\n\n## Set up data structure\n\nHere I set up a dataframe that will catch my modeling results in list columns of the different model objects and return values.\nThe first column I define specifies the range of different cluster sizes (k) that we will try.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntry_clusts <- c(2:20)\n\nkm_df <- data.frame(n_clust = try_clusts)\n```\n:::\n\n\n## Specify model (for each value of k)\n\nFor each unique value of k (2-20), this returns a model specification object (in the `kmeans_spec` column) based on the custom function I wrote above.\nThe model specification has all the options set about how we want the algorithm to run (methods, number of starts, etc.).\nWe need a different one for each value of k.\n\nThe `kmeans_wflow` column here holds our workflow objects.\nThese objects combine our model specification (from `kmeans_spec`) with the data recipe (preprocessor) we made above (`rec_spec`, is same for all models).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for each unique value of clusters (2:20), returns a model\n# specification (kmeans_spec) and a workflow (kmeans_wflow) \n# note that the workflow \nkm_df <- km_df %>%\n  mutate(\n    kmeans_spec = map(n_clust, ~ km_spec(nclust = .x)),\n    kmeans_wflow = map(kmeans_spec,\n                       ~ workflow(\n                         preprocessor = rec_spec, spec = .x\n                       ))\n  )\n\n# our current data structure\nhead(km_df, n=3L )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"n_clust\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"kmeans_spec\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"kmeans_wflow\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"_rn_\":\"1\"},{\"1\":\"3\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"_rn_\":\"2\"},{\"1\":\"4\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"_rn_\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# take a look at an example workflow\nkm_df$kmeans_wflow[3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_mutate()\n• step_sqrt()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = nclust\n\nEngine-Specific Arguments:\n  nstart = 10\n  algorithm = Hartigan-Wong\n  iter.max = 20\n\nComputational engine: stats \n```\n:::\n:::\n\n\n## Fit the models\n\nAll the steps above were related to specifying different aspects of this model.\nNow we can actually fit the models.\n\nSome troubleshooting here:\n\n-   Started by specifying `tidyclust::fit()` but something weird was happening where my `step_normalize()` wasn't included in the pre-processor recipe when I looked at the fitted model object.\n-   If I specify `parsnip::fit()` , then `step_normalize()` is included and the values of the cluster centroids are in the expected ranges (centered, scaled).\n-   I also tried this without explicitly specifying the package (so just `fit()` ) and it worked as expected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a quiet version of fit(), so we can capture results \n# and any warning messages from the models \n# see troubleshooting notes below\nquiet_fit <- purrr::quietly(.f = parsnip::fit)\n\nset.seed(4) # for reproducibility \nkm_fit_df <- km_df %>%\n  mutate(km_result = map(.x = kmeans_wflow,\n                      .f = quiet_fit,\n       # data comes after .f b/c not vectorized over            \n                      data = d),\n       km_fit = map(km_result, ~pluck(.x, 'result')),\n       warn = map(km_fit, ~pluck(.x, 'warnings')),\n       msg = map(km_fit, ~pluck(.x, 'messages')),\n       n_iter = map_dbl(km_fit, \n                      ~pluck(.x, 'fit', 'fit', 'fit', 'iter' ))) \n         \n\n# check out current data structure\nhead(km_fit_df, n = 3L)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"n_clust\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"kmeans_spec\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"kmeans_wflow\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"km_result\"],\"name\":[4],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"km_fit\"],\"name\":[5],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"warn\"],\"name\":[6],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"msg\"],\"name\":[7],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"n_iter\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"4\":\"<named list [4]>\",\"5\":\"<S3: workflow>\",\"6\":\"<NULL>\",\"7\":\"<NULL>\",\"8\":\"1\",\"_rn_\":\"1\"},{\"1\":\"3\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"4\":\"<named list [4]>\",\"5\":\"<S3: workflow>\",\"6\":\"<NULL>\",\"7\":\"<NULL>\",\"8\":\"4\",\"_rn_\":\"2\"},{\"1\":\"4\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"4\":\"<named list [4]>\",\"5\":\"<S3: workflow>\",\"6\":\"<NULL>\",\"7\":\"<NULL>\",\"8\":\"3\",\"_rn_\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# don't need anymore, cleaning up\nrm(km_df)\n```\n:::\n\n\n### Notes about model fit troubleshooting\n\nIf I `set.seed(123)` and run kmeans with `max.iter=10`, get warnings about 'no convergence at 10 iterations', for k = 17 and k = 20.\nChanged to `max.iter=20` and ran again, this time no convergence warnings.\n\nAs a result of this experience, I added some additional columns to the `km_fit_df` object using `{purrr}` 's function `quietly()` so I could capture warnings and messages that would otherwise only appear in the console (and are hard to trace when I'm iterating through all these models at once).\n[This blog post](https://aosmith.rbind.io/2020/08/31/handling-errors/#using-quietly-to-capture-messages) was very helpful for an example of how to do this.\n\nIn an earlier troubleshooting attempt, I was trying to see if the warning messages were stored anywhere in the fitted `{tidyclust}` model object?\nIt seemed like maybe they would have been in `ifault`, but those values were all 0, even when I had model convergence warnings.\nIn that case I tried indexing into the fitted model objects with `map(km_fit, ~pluck(.x, 'result', 'fit', 'fit', 'fit', 'ifault' ))`\n\n### View messages & warnings\n\nWe can look at any warnings or messages from the modeling process:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm_fit_df %>% \n  select(n_clust, warn, msg, n_iter)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"n_clust\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"warn\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"msg\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"n_iter\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"1\"},{\"1\":\"3\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"4\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"3\"},{\"1\":\"5\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"6\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"7\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"8\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"},{\"1\":\"9\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"},{\"1\":\"10\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"},{\"1\":\"11\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"12\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"13\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"8\"},{\"1\":\"14\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"7\"},{\"1\":\"15\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"16\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"},{\"1\":\"17\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"18\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"},{\"1\":\"19\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"7\"},{\"1\":\"20\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n### Look at one fit object\n\nAs an example, these are what the fitted objects look like.\n\n**NOTE** the clustering vector here is using the cluster numbers directly from `kmeans()`.\n`tidyclust` assigns names like \"Cluster_1\", \"Cluster_2\" etc. , but the numbers do NOT necessarily match with what `kmeans()` returns.\nThe CLUSTERINGS are the same, but the numbers are not necessarily so.\nSo 2 in this \"Clustering Vector\" below is NOT necessarily equal to tidyclust \"Cluster_2\" that you might get by using the `extract_cluster_assignment` function.\nTo keep things consistent, I'm always using the cluster names assigned by `tidyclust`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexamp_fit <- km_fit_df$km_result[[4]][['result']]\n\nexamp_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_mutate()\n• step_sqrt()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-means clustering with 5 clusters of sizes 479, 2019, 1832, 1065, 1477\n\nCluster means:\n   claytotal         om       cec7 dbthirdbar         ec  ph1to1h2o      caco3\n1  0.5174494  0.3745308  0.3208957 -0.6376723  3.3283137  1.1660203  1.0228662\n2 -0.3814723 -0.3465904 -0.2722727  0.3236714 -0.2571456 -0.5186223 -0.5279703\n3  0.7722551  0.6366879  0.8147342 -0.5735785 -0.2463628 -0.2631583 -0.5750294\n4 -1.5118222 -1.1990889 -1.6374004  1.0687407 -0.2271428 -0.7273531 -0.5026188\n5  0.4858861  0.4272060  0.4382179 -0.2948278 -0.2585251  1.1816599  1.4656479\n         lep       ksat        awc\n1  0.4284214 -0.5251382  0.2167607\n2 -0.4841584  0.2713584  0.0553920\n3  0.6939805 -0.7350181  0.5806865\n4 -0.9544845  1.5123302 -1.7551062\n5  0.3503424 -0.3794245  0.3992591\n\nClustering vector:\n   [1] 4 4 4 2 2 4 4 4 4 4 2 4 4 2 4 3 2 2 4 4 2 2 4 4 2 4 4 4 4 4 2 2 4 4 4 4 4\n  [38] 2 2 2 4 4 4 4 4 2 4 4 4 4 4 2 2 2 3 5 5 2 3 3 3 3 3 1 4 4 4 4 5 5 3 3 4 4\n  [75] 5 4 5 3 2 2 3 3 3 1 5 4 4 4 4 4 4 4 3 4 4 5 2 4 4 4 3 3 5 5 5 5 4 4 2 2 2\n [112] 4 4 4 3 5 4 4 4 4 5 3 3 3 3 4 4 4 4 2 2 4 2 4 4 2 4 2 5 2 4 4 4 4 4 4 5 4\n [149] 4 4 5 2 2 3 1 2 1 4 2 2 2 4 5 5 5 5 2 2 4 4 4 2 2 4 4 2 2 5 5 5 2 3 2 2 3\n [186] 5 5 3 3 3 2 2 5 3 3 2 2 2 4 4 4 4 4 4 4 2 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 5\n [223] 5 3 4 4 3 3 3 5 4 4 4 3 3 4 5 2 3 3 4 2 5 5 2 3 4 3 3 1 4 4 4 2 1 2 4 4 5\n [260] 4 5 4 4 4 4 2 4 4 2 4 4 4 4 4 4 4 4 2 2 2 2 4 4 2 2 4 4 4 2 4 4 4 4 2 2 2\n [297] 2 2 4 4 4 4 2 4 2 4 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4\n [334] 2 2 2 1 3 5 5 1 1 1 1 5 1 5 5 3 3 5 5 3 1 5 1 3 5 5 1 2 5 3 5 5 2 1 5 5 3\n [371] 2 2 3 2 2 3 3 1 2 3 4 4 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 5 5 3 3 3 5 5 5\n [408] 5 5 3 2 2 3 3 3 3 2 2 2 2 3 3 3 2 2 5 5 5 3 2 3 2 2 2 3 3 3 4 4 4 3 3 4 2\n [445] 2 3 3 3 3 3 5 3 3 3 3 5 4 3 4 4 2 3 3 3 3 2 3 3 3 5 5 3 3 3 3 3 5 3 3 3 3\n [482] 5 3 2 2 2 5 2 2 3 3 2 3 3 3 5 2 3 3 3 5 3 3 5 5 5 3 3 3 5 3 3 3 3 3 3 3 5\n [519] 2 2 5 5 5 3 3 3 3 3 4 4 2 2 3 3 3 2 2 3 3 2 2 5 3 2 5 5 2 4 2 5 5 1 2 4 2\n [556] 2 5 2 5 3 5 5 5 5 2 2 5 5 5 5 5 3 5 2 5 2 3 3 5 2 5 5 5 3 5 3 5 5 5 2 2 2\n [593] 3 3 5 5 5 3 3 5 2 4 5 3 3 2 3 3 5 3 4 2 2 3 5 2 5 2 3 3 3 3 3 3 5 5 5 5 5\n [630] 2 3 3 3 3 2 2 2 2 3 2 2 2 5 3 3 3 2 2 2 2 3 3 3 3 2 5 5 5 4 4 4 4 2 2 2 2\n [667] 3 5 2 2 2 2 2 2 2 2 2 2 3 2 2 2 4 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 2 2 2 2 4\n [704] 4 2 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 2 4 4 4 5 3 2 4 4 2 4 4 2 4\n [741] 4 4 4 4 2 4 4 2 4 4 5 5 5 5 5 2 5 2 2 4 5 3 2 3 2 5 3 5 5 3 2 3 3 2 3 3 5\n [778] 5 1 5 5 5 5 5 5 2 2 2 2 5 3 3 5 3 3 2 3 5 3 3 5 5 5 5 5 5 5 5 2 3 5 5 5 5\n [815] 3 5 5 5 5 5 5 5 5 2 2 2 3 3 2 2 3 3 3 5 3 3 5 5 2 2 2 2 2 4 5 5 5 3 1 3 3\n [852] 3 1 5 5 1 5 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 4 4 1 3 1 1 4 4 4 1 2 2 4 2\n [889] 2 5 1 3 3 5 3 5 3 5 1 5 5 3 3 1 4 1 2 5 5 1 3 4 5 5 5 5 5 1 1 1 2 1 1 2 1\n [926] 1 1 1 1 1 3 2 2 3 4 2 1 1 1 3 3 5 3 4 4 4 4 3 3 1 5 5 1 1 1 4 4 5 4 4 2 2\n [963] 2 3 5 3 5 4 4 4 3 3 5 5 2 2 4 4 5 2 3 4 2 4 4 4 2 5 5 2 4 4 4 4 3 3 3 1 4\n[1000] 4 2 3 1 2 1 2 4 5 4 4 4 4 4 5 2 2 5 5 2 2 2 2 5 3 3 3 5 5 5 2 5 2 2 3 5 5\n[1037] 5 5 3 3 5 3 5 5 5 2 2 5 3 3 5 3 4 4 5 3 3 3 3 3 3 3 5 3 2 5 5 3 3 3 5 2 2\n[1074] 5 3 3 2 3 5 3 3 5 5 5 2 2 5 5 5 5 3 3 3 3 3 3 3 3 5 5 5 3 5 5 3 3 5 5 5 2\n[1111] 2 2 3 2 3 3 5 5 5 3 3 5 3 3 3 4 5 3 3 3 4 4 4 4 4 5 4 2 4 4 4 4 4 4 4 4 4\n[1148] 4 4 4 3 4 4 4 4 2 4 4 4 2 3 2 4 4 4 4 4 4 4 4 3 2 5 2 4 3 2 3 2 2 4 4 4 4\n[1185] 4 4 4 2 2 2 2 2 4 2 4 4 4 3 3 2 2 2 2 2 2 4 4 4 3 3 4 4 4 2 2 3 3 3 3 2 2\n\n...\nand 162 more lines.\n```\n:::\n:::\n\n\nA nicer way to look at the results is by accessing specific parts of the fitted model object, as below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# some basic model metrics\nglance(examp_fit)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"totss\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tot.withinss\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"betweenss\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"iter\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"68710\",\"2\":\"29211.84\",\"3\":\"39498.16\",\"4\":\"4\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# centroid data (transformed/standardized scale)\ncentroids <- tidyclust::extract_centroids(examp_fit)\ncentroids\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".cluster\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"claytotal\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"om\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"cec7\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"dbthirdbar\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ec\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ph1to1h2o\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"caco3\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lep\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ksat\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"awc\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Cluster_1\",\"2\":\"0.4858861\",\"3\":\"0.4272060\",\"4\":\"0.4382179\",\"5\":\"-0.2948278\",\"6\":\"-0.2585251\",\"7\":\"1.1816599\",\"8\":\"1.4656479\",\"9\":\"0.3503424\",\"10\":\"-0.3794245\",\"11\":\"0.3992591\"},{\"1\":\"Cluster_2\",\"2\":\"-0.3814723\",\"3\":\"-0.3465904\",\"4\":\"-0.2722727\",\"5\":\"0.3236714\",\"6\":\"-0.2571456\",\"7\":\"-0.5186223\",\"8\":\"-0.5279703\",\"9\":\"-0.4841584\",\"10\":\"0.2713584\",\"11\":\"0.0553920\"},{\"1\":\"Cluster_3\",\"2\":\"0.7722551\",\"3\":\"0.6366879\",\"4\":\"0.8147342\",\"5\":\"-0.5735785\",\"6\":\"-0.2463628\",\"7\":\"-0.2631583\",\"8\":\"-0.5750294\",\"9\":\"0.6939805\",\"10\":\"-0.7350181\",\"11\":\"0.5806865\"},{\"1\":\"Cluster_4\",\"2\":\"0.5174494\",\"3\":\"0.3745308\",\"4\":\"0.3208957\",\"5\":\"-0.6376723\",\"6\":\"3.3283137\",\"7\":\"1.1660203\",\"8\":\"1.0228662\",\"9\":\"0.4284214\",\"10\":\"-0.5251382\",\"11\":\"0.2167607\"},{\"1\":\"Cluster_5\",\"2\":\"-1.5118222\",\"3\":\"-1.1990889\",\"4\":\"-1.6374004\",\"5\":\"1.0687407\",\"6\":\"-0.2271428\",\"7\":\"-0.7273531\",\"8\":\"-0.5026188\",\"9\":\"-0.9544845\",\"10\":\"1.5123302\",\"11\":\"-1.7551062\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# helpful to add to future plots for examining indiv. clusters\nclust_stat <- tidyclust::sse_within(examp_fit)\nclust_stat\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".cluster\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"wss\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n_members\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Cluster_1\",\"2\":\"5962.365\",\"3\":\"1477\"},{\"1\":\"Cluster_2\",\"2\":\"6669.121\",\"3\":\"2019\"},{\"1\":\"Cluster_3\",\"2\":\"7930.378\",\"3\":\"1832\"},{\"1\":\"Cluster_4\",\"2\":\"4204.982\",\"3\":\"479\"},{\"1\":\"Cluster_5\",\"2\":\"4444.992\",\"3\":\"1065\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n## Model metrics {#sec-mod-metrics}\n\nSee also section 7.5 in the Chapter by Tan et al. for more about cluster evaluation.\n\n### Extract metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics_df <- km_fit_df %>%\n  mutate(\n    # tot_sse = total sum of squared error\n    tot_sse = map_dbl(km_fit, ~ sse_total_vec(.x)),\n    # tot_wss = sum of within-cluster sse\n    tot_wss = map_dbl(km_fit, ~sse_within_total_vec(.x)),\n    # sse ratio = wss / total sse, \n    sse_ratio = map_dbl(km_fit, ~sse_ratio_vec(.x))\n    )\n\nrm(km_fit_df)\n\nmetrics_simple <- metrics_df %>% \n  select(n_clust, tot_sse, tot_wss, sse_ratio)\n\nmetrics_simple\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"n_clust\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"tot_sse\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tot_wss\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sse_ratio\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"68710\",\"3\":\"45880.29\",\"4\":\"0.6677381\"},{\"1\":\"3\",\"2\":\"68710\",\"3\":\"37880.57\",\"4\":\"0.5513109\"},{\"1\":\"4\",\"2\":\"68710\",\"3\":\"33293.29\",\"4\":\"0.4845480\"},{\"1\":\"5\",\"2\":\"68710\",\"3\":\"29211.84\",\"4\":\"0.4251468\"},{\"1\":\"6\",\"2\":\"68710\",\"3\":\"25898.12\",\"4\":\"0.3769192\"},{\"1\":\"7\",\"2\":\"68710\",\"3\":\"23301.32\",\"4\":\"0.3391256\"},{\"1\":\"8\",\"2\":\"68710\",\"3\":\"21657.03\",\"4\":\"0.3151948\"},{\"1\":\"9\",\"2\":\"68710\",\"3\":\"20461.49\",\"4\":\"0.2977949\"},{\"1\":\"10\",\"2\":\"68710\",\"3\":\"19494.50\",\"4\":\"0.2837215\"},{\"1\":\"11\",\"2\":\"68710\",\"3\":\"18319.31\",\"4\":\"0.2666179\"},{\"1\":\"12\",\"2\":\"68710\",\"3\":\"17905.45\",\"4\":\"0.2605945\"},{\"1\":\"13\",\"2\":\"68710\",\"3\":\"17095.40\",\"4\":\"0.2488051\"},{\"1\":\"14\",\"2\":\"68710\",\"3\":\"16511.05\",\"4\":\"0.2403005\"},{\"1\":\"15\",\"2\":\"68710\",\"3\":\"15973.99\",\"4\":\"0.2324842\"},{\"1\":\"16\",\"2\":\"68710\",\"3\":\"15361.98\",\"4\":\"0.2235771\"},{\"1\":\"17\",\"2\":\"68710\",\"3\":\"14928.80\",\"4\":\"0.2172726\"},{\"1\":\"18\",\"2\":\"68710\",\"3\":\"14611.63\",\"4\":\"0.2126565\"},{\"1\":\"19\",\"2\":\"68710\",\"3\":\"14153.53\",\"4\":\"0.2059894\"},{\"1\":\"20\",\"2\":\"68710\",\"3\":\"13589.21\",\"4\":\"0.1977763\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n### Plot Total WSS\n\nNot a clear \"elbow\" here, although by the time we get to 10-11 it does seem to be leveling off.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics_simple %>% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Compare values of k: looking for elbow\")\n```\n\n::: {.cell-output-display}\n![](11-implement-kmeans_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmetrics_simple %>% \n  filter(n_clust %in% c(2:12)) %>% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:12)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Zoom in a bit: looking for elbow\")\n```\n\n::: {.cell-output-display}\n![](11-implement-kmeans_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n### Average Silhouette\n\nFrom the `{tidyclust}` documentation:\n\n> Another common measure of cluster structure is called the **silhouette**.\n> The silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\\\n>\n> In principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\nSee also pg.\n581 in Tan2018 Chap 7 Cluster Analysis: Basic Concepts and Algorithms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprepped_rec <- prep(rec_spec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \nbaked_df <- bake(prepped_rec, new_data = NULL) %>% \n  select(-mukey) \n\ndists <- baked_df %>% as.matrix() %>% dist(method = \"euclidean\")\n\nsilh_df <- metrics_df %>% \n  mutate(avg_sil = map_dbl(km_fit, \n                       tidyclust::silhouette_avg_vec,\n                       dists = dists),\n         indiv_sil = map(km_fit, \n                         tidyclust::silhouette,\n                         dists = dists))\n\nindiv_sil_df <- silh_df %>% select(n_clust, indiv_sil) %>% \n  unnest(indiv_sil) %>% \n  mutate(across(.cols = c(cluster, neighbor),\n                .fns = as.character))\n\nwrite_csv(indiv_sil_df, \"data/kmeans_points_silhouettes.csv\")\n\nrm(metrics_df)  \nrm(dists)\nrm(prepped_rec)\n```\n:::\n\n\nHigher silhouette is better (means observations are closer to their centroids than to other observations).\nSeems to suggest that 4, 6, 10-12 would be OK (those are local maxima), but not greater than 12.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsilh_df %>% \n  ggplot(aes(x = n_clust, y = avg_sil)) +\n  geom_point() + \n  geom_line() + \n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  ggtitle(\"Overall Average Silhouette\") +\n  labs(subtitle = \"Higher is better, possible values [-1,1]\")\n```\n\n::: {.cell-output-display}\n![](11-implement-kmeans_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nCan also plot the individual silhouettes.\nFor each clustering (model version), we have a silhouette value per observation in the dataset (n=6872).\nWe also have the closest \"neighbor\" cluster, or the cluster that specific observation would belong to if its home cluster didn't exist.\n\nHere's an example of this data for the k=6 clustering.\nThe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneighbor_counts <- indiv_sil_df %>% \n  group_by(n_clust, cluster, neighbor) %>% \n  count()  %>% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c\"),\n         neighbor = str_replace(neighbor, \"Cluster_\", \"c\"))\n\nk6_neighbor_counts <- neighbor_counts %>% \n  filter(n_clust == 6)\n\n\nindiv_sil_df %>% \n  mutate(across(.cols = c(cluster, neighbor),\n         ~str_replace(.x, \"Cluster_\", \"c\"))) %>% \n  filter(n_clust == 6) %>% \n  ggplot() + \n  geom_boxplot(aes(x = neighbor, y = sil_width)) +\n    geom_point(aes(x = neighbor, y = sil_width),\n             position = position_jitter(width = 0.1),\n             alpha = 0.2,\n             color = \"pink\") + \n  geom_text(data = k6_neighbor_counts,\n            aes(x = neighbor, y = 0.7, label = n),\n            color = \"blue\") +\n  facet_wrap(vars(cluster), scales = \"free_x\") +\n  theme_bw() +\n  ggtitle(\"k=6 silhouettes\")\n```\n\n::: {.cell-output-display}\n![](11-implement-kmeans_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclust_sil_avgs <- indiv_sil_df %>% \n  group_by(cluster,\n           n_clust) %>% \n  summarise(mean_sil = mean(sil_width),\n            sd_sil = sd(sil_width), \n            .groups = \"drop\")\n\nclust_sil_avgs %>% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c0\"),\n         cluster = case_when(\n           cluster %in% c(\"c010\", \"c011\", \"c012\", \"c013\", \"c014\", \"c015\", \"c016\",\n                          \"c017\", \"c018\", \"c019\", \"c020\") ~ str_replace(cluster, \"c0\", \"c\"),\n           TRUE ~ cluster\n         )) %>% \n  filter(n_clust %in% c(6:12)) %>% \n  ggplot() +\n  geom_col(aes(y = cluster, x = mean_sil)) +\n  facet_wrap(vars(n_clust), scales = \"free_y\") + \n  ggtitle(\"Average silhouette width per cluster for k=6-12\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](11-implement-kmeans_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### Not used: Gap statistic\n\nFor `fviz_nbclust()`, first couple times running this, got `Warning: Quick-TRANSFER stage steps exceeded maximum…` Looking online, this seems to be a problem with the model not converging.\nI added some arguments here that are passed on to `kmeans()`, to make sure that the algorithm settings here match what I run above, including `set.seed()`\n\nContinued to get warnings, even though I'm using all the same settings as I use for kmeans up above.\nNot sure why this is, but I'm not going to spend any more time on it right now.\nMaybe see if getting the gap statistic through `NbClust` works better?\n(Later note: NbClust won't be a good option either, I can't alter important kmeans() settings in NbClust).\nExpect it will take a long time either way, consider running this in a separate script and pulling in the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nfviz_gap_stat(x = baked_df, \n             FUNcluster = kmeans,\n             method = c(\"gap_stat\"),\n             k.max = 10, # only considering 2-10 clusters\n             nboot = 50, # default is 100\n             verbose = TRUE, \n             iter.max = 20, # passed to kmeans\n             nstart = 10 # passed to kmeans\n             )\n```\n:::\n\n\n### Calinski-Harabasz index\n\n**Not used: `{NbClust}`** **, using `{fpc}` instead.**\n\n-   For Calinski-Harabasz index, higher values are better\n-   Realized after setting this up with `NbClust` that I don't have the option to pass additional arguments to the `kmeans` function here. So I can't make the algorithm settings exactly match my main clustering pipeline above (where I implement k-means using `tidyclust` and the `tidymodels` framework, and where I save the results for further analysis). This is a problem because I know from my original tests that I need to change the iter.max value to avoid non-convergence issues, and I also want to change nstart because nstart \\>1 is typically known to be best practice (find citation for this).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# keeping this here as a record, but I\"m NOT USING this function for the C-H index. \n\nnbc_indices <- NbClust::NbClust(data = baked_df,\n                 distance = \"euclidean\",\n                 method = \"kmeans\",\n                 min.nc = 2,\n                 max.nc = 20,\n                 index = \"ch\") # Calinski and Harabasz\n\n# enframe turns a named vector into a dataframe\nch_index_vals <- enframe(nbc_indices$All.index) %>% \n  mutate(name = as.integer(name)) %>% \n  rename(n_clust = name)\n```\n:::\n\n\nTrying a different implementation of the Calinski-Harabasz index from the `{fpc}` package.\nThis is preferred to the above approach, where I originally used the `NbClust` function from `{NbClust}` package because I can give this function my clustering generated above (NbClust does its own run of kmeans but I can't customize it to keep it consistent with\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calinhara wants an observations/variables matrix\n# as first argument (as opposed to a distance matrix)\nobsvar_mx <- as.matrix(baked_df)\n\n# function to extract and modify tidyclust clusters\n# into a integer vector, which I will pass to calinhara()\ncreate_clust_vec <- function(fit_obj){\n  \n  extract_cluster_assignment(fit_obj) %>% \n    pull(.cluster) %>% \n    str_replace(., \"Cluster_\", \"\") %>% \n    as.integer()\n  \n}\n\n# apply function to extract clusterings as integer vectors\n# map to get a c-h index value for every value of k (2-20)\nch_metrics <- silh_df %>%\n  select(n_clust, km_fit) %>%\n  mutate(\n    clustering_vec = map(km_fit, create_clust_vec),\n    ch_index = map_dbl(clustering_vec,\n                       ~ fpc::calinhara(x = obsvar_mx,\n                                        clustering = .x)\n    ))\n\nch_metrics %>% \n  ggplot(aes(x = n_clust, y = ch_index)) + \n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  ylab(\"Calinski-Harabasz index\") + \n  ggtitle(\"Calinski-Harabasz\") +\n  labs(subtitle = \"Higher is better\") +\n  scale_x_continuous(breaks = c(2:20))\n```\n\n::: {.cell-output-display}\n![](11-implement-kmeans_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n### Hopkins Statistic\n\nUsing the [`{hopkins}`](https://kwstat.github.io/hopkins/reference/hopkins.html) package for this.\nCitations included in the package documentation (also cite Tan et al., 2019 who give an example of using this for evaluating kmeans clusters).\n\n-   Hopkins, B. and Skellam, J.G., 1954. A new method for determining the type of distribution of plant individuals. Annals of Botany, 18(2), pp.213-227.\n-   Cross, G. R., and A. K. Jain. (1982). Measurement of clustering tendency. Theory and Application of Digital Control. Pergamon, 1982. 315-320.\n\nAnd a third citation, helpful illustrations:\n\n-   Lawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of Chemical Information and Computer Sciences, 30(1), 36--41. https://doi.org/10.1021/ci00065a010\n\nApparently `{factoextra}` also has a Hopkins statistic, try that here too.\n(It takes a very long time to run, but returns 0.93, similar to 0.99 returned by `hopkins::hopkins()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nhstat <- hopkins(X = baked_df,\n                 # default, number of rows to sample from the df\n                 m = ceiling(nrow(baked_df)/10), \n                 # default, dimension of the data\n                 d = ncol(baked_df),\n                 # default, kth nearest neighbor to find\n                 k = 1) \nhstat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9999999\n```\n:::\n\n```{.r .cell-code}\nhopkins.pval(x = hstat,\n             # this is the default for hopkins() above\n             n = ceiling(nrow(baked_df)/10)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# commenting out because it takes a very long time to run\n# factoextra::get_clust_tendency(data = baked_df,\n#                                n = 687, \n#                                graph = FALSE)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n### WSS and Silhouette metrics on one plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| echo: false\n\nsil_totwss <- silh_df %>% \n  select(n_clust, avg_sil, tot_wss, tot_sse, sse_ratio)\n\nch <- ch_metrics %>% \n  select(n_clust, ch_index)\n\nmet_combined <- left_join(sil_totwss, ch, by = \"n_clust\")\n\nwrite_csv(met_combined, \"data/kmeans_cluster_metrics.csv\")\n\nmet2 <- met_combined %>% \n  pivot_longer(cols = -c('n_clust'), names_to = \"metric\",\n               values_to = \"value\")\n\nmet2 %>% \n  ggplot(aes(x = n_clust, y = value)) + \n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(2:20)) + \n  facet_wrap(vars(metric), ncol = 1, scales = \"free\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](11-implement-kmeans_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Save model fits\n\nWill save these as Rdata so I can call them up and investigate the cluster centroids more closely in the next chapter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmods <- silh_df %>% \n  select(n_clust, km_fit)\n\nsave(mods, file = \"data/fitted_kmeans_mods.RData\")\n```\n:::\n\n\n## Save cluster assignments\n\nFor each version of the model (each value of k, different numbers of clusters), a MUKEY is assigned to a specific cluster.\nHere, I'm pulling that data, shaping it into one dataframe (one row per MUKEY, cluster assingments in separate columns).\nI'm also adding back in the soil property data so we can use this in the next step when evaluating different cluster sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclust_assign_df <- mods %>% \n  mutate(clust_assign = map(km_fit, ~augment(.x, new_data = d)),\n         mukey_clust = map(clust_assign, ~select(.x, mukey, .pred_cluster)))\n\n\nassign_mukey_df <- clust_assign_df %>% \n  select(n_clust, mukey_clust) %>% \n  unnest(mukey_clust) %>% pivot_wider(names_from = n_clust, values_from = .pred_cluster, names_prefix = \"k_\")\n\n\nclust_props <- full_join(d, assign_mukey_df, by = \"mukey\")\n\nwrite_csv(clust_props, \"data/mukey_cluster_assignments_and_soilprops.csv\")\n```\n:::\n",
    "supporting": [
      "11-implement-kmeans_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}