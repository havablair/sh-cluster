{
  "hash": "eb284f14344906e448dc406532da5f38",
  "result": {
    "markdown": "# Do PCA before k-means\n\n## Overview\n\nAfter lots of reading during the drafting phase of methods & results for my original k-means analysis, I found many authors in soil science, climate/atomspheric science, and geochemistry who use principal components analysis (PCA) for data reduction prior to doing k-means.\nThe argument for doing this is that when you have highly correlated variables (which we do, see @sec-corr ), including them all basically gives more weight to the correlated variables.\nIf they are highly correlated, they contain most of the same \"information\", and by including both variables you are giving that \"information\" more weight in the calculation of (dis)similarity matrix (Euclidean distances) that we ultimately use for clustering.\n\nMy plan is to do PCA, run k-means again, and compare the results with my original k-means analysis.\nI think this would be a valuable thing to add to my paper, as we can make an argument for whether one method or another might be more generalizable for others who want to use this technique for creating similar conceptual clusters from a regional set of soil data (presumably with a slightly different set of variables given the specific context - think about depth to restrictive horizon in Devine vs. carbonates in my analysis).\n\n## Implementation\n\nThere is an easy way to do PCA as a pre-processing step in the `{tidymodels}` framework I've been doing using the `step_pca()` function.\nUnder the hood, this uses `stats::prcomp()` to do the PCA.\n\nMuch of this code will be similar to @sec-run-km, but with some additional exploratory plots as I think about how to incorporate this information in my manuscript.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glue)\nlibrary(tidyclust)\nlibrary(factoextra) # trying fviz_nbclust(), which gives elbow, silhouette, and gap statistic\nlibrary(hopkins)\nlibrary(fpc)\nlibrary(ggforce)\nlibrary(gt)\n\n\nd <- readr::read_csv(\"./data/clean_mu_weighted_soil_props.csv\") %>% \n  select(-contains(\"comp_pct\"))\n\nold_names <- colnames(d)\n\nnew_names <- stringr::str_replace_all(old_names, \"_r_value\", \"\")\n\ncolnames(d) <- new_names\n\n# this is a dataframe of the results from my first k-means\n# run using same methods as Devine et al. Loading it so I \n# can use the colors in my PCA plots to get a qualitative \n# sense of whether we \"see\" similar clusters after PCA\nclust_membership <- readr::read_csv(\"data/mukey_cluster_assignments_and_soilprops.csv\") %>% \n  select(mukey, k_6)\n\nd <- dplyr::left_join(d, clust_membership, by = \"mukey\")\n```\n:::\n\n\nReminder of what the data look like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(d)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mukey\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"claytotal\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"om\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"cec7\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"dbthirdbar\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ec\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ph1to1h2o\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"caco3\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"lep\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ksat\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"awc\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"k_6\"],\"name\":[12],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"395973\",\"2\":\"3.000000\",\"3\":\"1.50\",\"4\":\"4.50000\",\"5\":\"1.600\",\"6\":\"0\",\"7\":\"6.000000\",\"8\":\"0\",\"9\":\"0.3\",\"10\":\"282.00000\",\"11\":\"0.0800000\",\"12\":\"Cluster_1\"},{\"1\":\"395975\",\"2\":\"3.000000\",\"3\":\"1.00\",\"4\":\"4.50000\",\"5\":\"1.600\",\"6\":\"0\",\"7\":\"6.000000\",\"8\":\"0\",\"9\":\"0.3\",\"10\":\"282.00000\",\"11\":\"0.0800000\",\"12\":\"Cluster_1\"},{\"1\":\"395900\",\"2\":\"5.000000\",\"3\":\"4.50\",\"4\":\"11.37500\",\"5\":\"1.435\",\"6\":\"0\",\"7\":\"7.250000\",\"8\":\"0\",\"9\":\"1.5\",\"10\":\"74.75000\",\"11\":\"0.0975000\",\"12\":\"Cluster_1\"},{\"1\":\"395948\",\"2\":\"19.500000\",\"3\":\"5.00\",\"4\":\"21.00000\",\"5\":\"1.400\",\"6\":\"0\",\"7\":\"6.500000\",\"8\":\"0\",\"9\":\"1.5\",\"10\":\"9.00000\",\"11\":\"0.2200000\",\"12\":\"Cluster_2\"},{\"1\":\"885885\",\"2\":\"7.155556\",\"3\":\"4.95\",\"4\":\"17.81667\",\"5\":\"1.437\",\"6\":\"0\",\"7\":\"6.416667\",\"8\":\"0\",\"9\":\"1.5\",\"10\":\"38.27778\",\"11\":\"0.1396111\",\"12\":\"Cluster_1\"},{\"1\":\"395974\",\"2\":\"3.000000\",\"3\":\"1.00\",\"4\":\"4.50000\",\"5\":\"1.600\",\"6\":\"0\",\"7\":\"6.000000\",\"8\":\"0\",\"9\":\"0.3\",\"10\":\"282.00000\",\"11\":\"0.0800000\",\"12\":\"Cluster_1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nReminder of the transformations I chose to improve variable distributions and make them normal-ish.\n\n-   Square root: clay, carbonates\n-   Log10: organic matter, cec, lep, ksat, awc\n-   Cube (\\^3): bulk density\n-   None: ec, ph\n\n## Pre-process data (build recipe)\n\n-   apply some transformations to achieve more normal distributions,\n-   then standardize (`step_normalize`) by subtracting the mean and dividing by 1 sd\n-   then PCA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_spec <- recipe(~., data = d) %>% \n    update_role(mukey, new_role = \"ID\") %>% \n    update_role(k_6, new_role = \"cluster_k6\") %>% \n  # note this is log10 (the default is ln)\n    step_log(om, cec7, ksat, awc, lep, base = 10) %>% \n    step_mutate(dbthirdbar = dbthirdbar^3) %>% \n    step_sqrt(claytotal, caco3) %>% \n    step_normalize(all_numeric_predictors()) %>% \n  step_pca(all_numeric_predictors(), num_comp = 10)\n\nrec_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n       role #variables\n cluster_k6          1\n         ID          1\n  predictor         10\n\nOperations:\n\nLog transformation on om, cec7, ksat, awc, lep\nVariable mutation for dbthirdbar^3\nSquare root transformation on claytotal, caco3\nCentering and scaling for all_numeric_predictors()\nPCA extraction with all_numeric_predictors()\n```\n:::\n:::\n\n\n## Functions to run PCA, visualize results\n\nThis is modified from the \"Tidy models with R\" book, [section 16.5 \"Feature Extraction Techniques\"](https://www.tmwr.org/dimensionality.html#feature-extraction-techniques).\n\nI removed the \"dat\" argument because I'm using the dataset that is already in my recipe.\nI also set `new_data = NULL` in bake as a reminder of this.\nIn `prep`, it is default to have `retain=TRUE`, but again I'm explicitly typing it as a reminder to myself of what the defaults / where the data is coming from.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrun_pca <- function(recipe){\n  \n  recipe %>% \n    # here, prep estimates the added PCA step\n    prep(retain = TRUE) %>%\n    # Process the data (new_data=NULL means use data in recipe)\n    bake(new_data = NULL) \n  \n  \n}\n\n# Create the scatterplot matrix\nplot_validation_results <- function(pca_df) {\n  \n  pca_df %>%\n    ggplot(aes(\n      x = .panel_x,\n      y = .panel_y,\n      color = k_6,\n      fill = k_6\n    )) +\n    geom_point(alpha = 0.4, size = 0.5) +\n    geom_autodensity(alpha = .3) +\n    facet_matrix(vars(-c(mukey, k_6)), layer.diag = 2) +\n    scale_color_brewer(palette = \"Dark2\") +\n    scale_fill_brewer(palette = \"Dark2\")\n  \n}\n```\n:::\n\n\n## Principal Components Analysis (PCA)\n\n### Visualize PCA results\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_results <- run_pca(rec_spec)\n\nwrite_csv(pca_results, \"data/pca_scores.csv\")\n\n# just plotting the first 5 PCs so we can actually seem them\n\npca_results %>% \nselect(mukey, k_6, PC01, PC02, PC03, PC04, PC05, PC06) %>% \nplot_validation_results() +\n  ggtitle(\"PCA pairwise plots: colors indicate k_6 clusters from original k-means\")\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n### Extract loadings\n\nTook me FOREVER to figure out why I wasn't able to extract my loadings with the example code from the `step_pca` [documentation here](https://recipes.tidymodels.org/reference/step_pca.html#ref-examples) using `tidy(prepped_rec, number = 2, type = \"coef\")`.\nAfter much frustration, I figured out that the \"number\" argument needs to correspond to the PCA step in the tidied dataframe of my prepped recipe... in all the documentation this number is 2, so I kept trying that and getting my bulk density mutation step.\nIn my case, the number is 5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_prep <- prep(rec_spec)\n\n# 'type = \"coef\"' here gets variables loadings per component\npca_loadings <- tidy(pca_prep, 5, type = \"coef\")\n\nwrite_csv(pca_loadings, \"data/pca_loadings.csv\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nltab <- loadings_dat %>% \n  select(terms, value, component) %>% \n  pivot_wider(names_from = component, \n              values_from = value,\n              names_prefix = \"PC\") %>% \n  gt(rowname_col = \"terms\") %>%\n  tab_stubhead(label = \"term\") %>% \n  fmt_number(\n    columns  = contains(\"PC\"),\n    decimals = 2\n  ) %>% \n  sub_missing() %>% \n  tab_header(title = \"PC Loadings\")\n\ngtsave(ltab, filename = \"figs/pc_loadings_table.docx\")\n```\n:::\n\n\n### Extract variance\n\n**Note** the number of rows in the `pca_var` dataframe: 40.\nWe have 4 different terms calculated for each component:\n\n-   variance\n-   cumulative variance\n-   percent variance\n-   cumulative percent variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_var <- tidy(pca_prep, 5, type = \"variance\")\n\nwrite_csv(pca_var, \"data/pca_variance.csv\")\n```\n:::\n\n\nFirst we can look at % variance explained by each component:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_var %>% \n  filter(terms == \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) +\n  geom_col() + \n  geom_text(aes(x = component, y = value+2, label = round(value, 1)), color = \"red\") +\n  theme_bw() +\n  ylab(\"Percent variation explained\") +\n  scale_x_continuous(breaks = c(1:10))\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nNext, we can look at cumulative % explained\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_var %>% \n  filter(terms == \"cumulative percent variance\") %>% \n  ggplot(aes(x = component, y = value)) +\n  geom_col() + \n  geom_text(aes(x = component, y = value+2, label = round(value, 1)), color = \"red\") +\n  theme_bw() +\n  ylab(\"Cumulative percent variation explained\") +\n  scale_x_continuous(breaks = c(1:10))\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nAnd now a more traditional scree plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_var %>% \n  filter(terms == \"variance\") %>% \n  ggplot(aes(x = component, y = value)) +\n  geom_point() + \n  geom_line() + \n  scale_x_continuous(breaks = c(1:10)) +\n  theme_minimal() +\n  ylab(\"Variance (Eigenvalue?)\")\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Select PCs to keep\n\nFor this round, I am keeping the first 5 PCs.\nThis will always be a somewhat subjective decision, something noted by both Jolliffe & Cadima (2016) and the helpfully detailed climate zone paper by Fovell & Fovell 1993 that walks through their process and comparison of 3 vs. 5 PCs for their modeling scenario.\n\nKeeping the first 5 PCs accounts for 90% of the variation in my dataset.\nIt also happens to be the number at which all of my original variables have been loaded on at least one PC (PC5 is the first time we see AWC loaded at all).\n\nMight be interesting, like Fovell & Fovell (1993), to do an additional version with 7 PCs.\nThat gets us at \\>95% variation accounted for.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_for_kmeans <- pca_results %>% \n  select(mukey, PC01, PC02, PC03, PC04, PC05)\n```\n:::\n\n\n## Set model options {#sec-mod-opt}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# writing a custom function here so I can be explicit \n# about the options I'm choosing, and also use within the \n# list-col framework I set up with map() below. \nkm_spec <- function(nclust){\n  \n  tidyclust::k_means(num_clusters = nclust) %>%\n    parsnip::set_engine(engine = \"stats\",\n               nstart = 10, # 1 is default, >1 recommended\n               algorithm = \"Hartigan-Wong\", # H-W is default\n               iter.max = 20) # default is 10, wasn't always enough\n  \n}\n```\n:::\n\n\n## Set up data structure and kmeans recipe\n\nHere I set up a dataframe that will catch my modeling results in list columns of the different model objects and return values.\nThe first column I define specifies the range of different cluster sizes (k) that we will try.\n\nI also set up my recipe here, which is much simpler compared to the original version because all of my data has been processed already before running it through k-means.\nPer the reading I did on 2023-01-11, especially Green & Krieger (1995) and Schaffer & Green (1998), I'm not further standardizing my component scores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntry_clusts <- c(2:20)\n\nkm_df <- data.frame(n_clust = try_clusts)\n\nkm_rec <- recipe(~., data = dat_for_kmeans) %>% \n  update_role(mukey, new_role = \"ID\")\n\nkm_rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n predictor          5\n```\n:::\n:::\n\n\n## Specify model (for each value of k)\n\nFor each unique value of k (2-20), this returns a model specification object (in the `kmeans_spec` column) based on the custom function I wrote above.\nThe model specification has all the options set about how we want the algorithm to run (methods, number of starts, etc.).\nWe need a different one for each value of k.\n\nThe `kmeans_wflow` column here holds our workflow objects.\nThese objects combine our model specification (from `kmeans_spec`) with the data recipe (preprocessor) we made above (`rec_spec`, is same for all models).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for each unique value of clusters (2:20), returns a model\n# specification (kmeans_spec) and a workflow (kmeans_wflow) \n# note that the workflow \nkm_df <- km_df %>%\n  mutate(\n    kmeans_spec = map(n_clust, ~ km_spec(nclust = .x)),\n    kmeans_wflow = map(kmeans_spec,\n                       ~ workflow(\n                         preprocessor = km_rec, spec = .x\n                       ))\n  )\n\n# our current data structure\nhead(km_df, n=3L )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"n_clust\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"kmeans_spec\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"kmeans_wflow\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"_rn_\":\"1\"},{\"1\":\"3\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"_rn_\":\"2\"},{\"1\":\"4\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"_rn_\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# take a look at an example workflow\nkm_df$kmeans_wflow[3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = nclust\n\nEngine-Specific Arguments:\n  nstart = 10\n  algorithm = Hartigan-Wong\n  iter.max = 20\n\nComputational engine: stats \n```\n:::\n:::\n\n\n## Fit the models\n\nAll the steps above were related to specifying different aspects of this model.\nNow we can actually fit the models.\n\nSome troubleshooting here:\n\n-   Started by specifying `tidyclust::fit()` but something weird was happening where my `step_normalize()` wasn't included in the pre-processor recipe when I looked at the fitted model object.\n-   If I specify `parsnip::fit()` , then `step_normalize()` is included and the values of the cluster centroids are in the expected ranges (centered, scaled).\n-   I also tried this without explicitly specifying the package (so just `fit()` ) and it worked as expected.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a quiet version of fit(), so we can capture results \n# and any warning messages from the models \n# see troubleshooting notes below\nquiet_fit <- purrr::quietly(.f = parsnip::fit)\n\nset.seed(4) # for reproducibility \nkm_fit_df <- km_df %>%\n  mutate(km_result = map(.x = kmeans_wflow,\n                      .f = quiet_fit,\n       # data comes after .f b/c not vectorized over            \n                      data = dat_for_kmeans),\n       km_fit = map(km_result, ~pluck(.x, 'result')),\n       warn = map(km_fit, ~pluck(.x, 'warnings')),\n       msg = map(km_fit, ~pluck(.x, 'messages')),\n       n_iter = map_dbl(km_fit, \n                      ~pluck(.x, 'fit', 'fit', 'fit', 'iter' ))) \n         \n\n# check out current data structure\nhead(km_fit_df, n = 3L)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"n_clust\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"kmeans_spec\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"kmeans_wflow\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"km_result\"],\"name\":[4],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"km_fit\"],\"name\":[5],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"warn\"],\"name\":[6],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"msg\"],\"name\":[7],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"n_iter\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"4\":\"<named list [4]>\",\"5\":\"<S3: workflow>\",\"6\":\"<NULL>\",\"7\":\"<NULL>\",\"8\":\"1\",\"_rn_\":\"1\"},{\"1\":\"3\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"4\":\"<named list [4]>\",\"5\":\"<S3: workflow>\",\"6\":\"<NULL>\",\"7\":\"<NULL>\",\"8\":\"3\",\"_rn_\":\"2\"},{\"1\":\"4\",\"2\":\"<S3: k_means>\",\"3\":\"<S3: workflow>\",\"4\":\"<named list [4]>\",\"5\":\"<S3: workflow>\",\"6\":\"<NULL>\",\"7\":\"<NULL>\",\"8\":\"3\",\"_rn_\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# don't need anymore, cleaning up\nrm(km_df)\n```\n:::\n\n\n### View messages & warnings\n\nWe can look at any warnings or messages from the modeling process:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm_fit_df %>% \n  select(n_clust, warn, msg, n_iter)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"n_clust\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"warn\"],\"name\":[2],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"msg\"],\"name\":[3],\"type\":[\"list\"],\"align\":[\"right\"]},{\"label\":[\"n_iter\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"1\"},{\"1\":\"3\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"3\"},{\"1\":\"4\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"3\"},{\"1\":\"5\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"6\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"7\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"8\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"9\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"10\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"11\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"12\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"13\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"},{\"1\":\"14\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"7\"},{\"1\":\"15\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"},{\"1\":\"16\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"5\"},{\"1\":\"17\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"},{\"1\":\"18\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"19\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"4\"},{\"1\":\"20\",\"2\":\"<NULL>\",\"3\":\"<NULL>\",\"4\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n### Look at one fit object\n\nAs an example, these are what the fitted objects look like.\n\n**NOTE** the clustering vector here is using the cluster numbers directly from `kmeans()`.\n`tidyclust` assigns names like \"Cluster_1\", \"Cluster_2\" etc. , but the numbers do NOT necessarily match with what `kmeans()` returns.\nThe CLUSTERINGS are the same, but the numbers are not necessarily so.\nSo 2 in this \"Clustering Vector\" below is NOT necessarily equal to tidyclust \"Cluster_2\" that you might get by using the `extract_cluster_assignment` function.\nTo keep things consistent, I'm always using the cluster names assigned by `tidyclust`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexamp_fit <- km_fit_df$km_result[[4]][['result']]\n\nexamp_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-means clustering with 5 clusters of sizes 1463, 431, 1809, 1096, 2073\n\nCluster means:\n        PC01       PC02        PC03         PC04       PC05\n1  1.5136139 -1.1675498 -0.04287291  1.011715005 -0.1041855\n2  1.7443425 -2.6232190 -0.74284903 -2.312509274 -0.1599810\n3  1.6349352  1.1528420 -0.02833620 -0.175076730  0.2152876\n4 -3.7357281 -0.3166354 -0.15275027  0.001684125  0.4859401\n5 -0.8825222  0.5307642  0.29019078 -0.081321539 -0.3379982\n\nClustering vector:\n   [1] 4 4 4 5 5 4 4 4 4 4 5 4 4 5 4 3 5 5 4 4 5 5 4 4 5 4 4 4 4 4 5 5 4 4 4 4 4\n  [38] 5 5 5 4 4 4 4 4 5 4 4 4 4 4 5 5 5 3 1 1 5 3 3 3 3 3 2 4 4 4 4 1 1 3 3 4 4\n  [75] 1 4 1 5 5 5 3 3 3 2 1 4 4 4 4 4 4 4 3 4 4 1 5 4 4 4 3 3 1 1 1 1 4 4 5 5 5\n [112] 4 4 4 3 1 4 4 4 4 1 3 3 3 3 4 4 4 4 5 5 4 4 4 4 5 4 5 1 5 4 4 4 4 4 4 1 4\n [149] 4 4 1 5 5 3 2 5 2 4 5 5 5 4 1 1 1 1 5 5 4 4 4 5 5 4 4 5 5 1 1 1 5 3 5 5 3\n [186] 1 1 3 3 3 5 5 1 3 5 5 5 5 4 4 4 4 4 4 4 5 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1\n [223] 1 3 4 4 3 3 3 1 4 4 4 3 3 4 1 5 3 3 4 5 1 1 5 3 4 3 3 2 4 4 4 5 2 5 4 4 1\n [260] 4 1 4 4 4 4 5 4 4 5 4 4 4 4 4 4 4 4 5 5 5 5 4 4 5 5 4 4 4 5 4 4 4 4 5 5 5\n [297] 5 5 4 4 4 4 5 4 5 4 4 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4\n [334] 5 5 5 2 3 1 1 2 2 2 2 1 2 1 1 3 3 1 1 3 2 1 3 3 1 1 2 5 1 3 1 1 5 2 1 1 3\n [371] 5 5 3 5 5 3 3 2 5 3 4 4 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 1 3 3 3 1 1 1\n [408] 1 1 3 5 5 3 3 3 3 5 5 5 5 3 5 3 5 5 1 1 1 3 5 3 5 5 5 3 3 3 4 4 4 3 3 4 5\n [445] 5 3 3 3 3 3 1 3 3 3 3 1 4 3 4 4 5 3 3 3 3 5 3 3 3 1 1 3 3 3 3 3 1 3 3 3 3\n [482] 1 3 5 5 5 1 5 5 3 3 5 3 3 3 1 5 3 3 3 1 3 3 1 1 1 3 3 3 1 3 3 3 3 3 3 3 1\n [519] 5 5 1 1 1 3 3 3 3 3 4 4 5 5 3 3 3 5 5 3 3 5 5 1 3 5 1 1 5 4 5 1 1 2 5 4 5\n [556] 5 1 5 1 5 1 1 1 1 5 5 1 1 1 1 1 3 1 5 1 5 3 3 1 5 1 1 1 3 1 3 1 1 1 5 5 5\n [593] 3 3 1 1 1 3 3 1 5 4 1 3 3 5 3 3 1 3 4 5 5 5 1 5 1 5 3 3 3 3 3 3 1 1 1 1 1\n [630] 5 3 3 3 3 5 5 5 5 3 5 5 5 1 3 3 3 5 5 5 5 3 3 3 3 5 1 1 1 4 4 4 4 5 5 5 5\n [667] 5 1 5 5 5 5 5 5 5 5 5 5 3 5 5 5 4 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 5 5 5 5 4\n [704] 4 5 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4 4 5 4 4 4 1 5 5 4 4 5 4 4 5 4\n [741] 4 4 4 4 5 4 4 5 4 4 1 1 1 1 1 5 1 5 5 4 1 3 5 3 5 1 3 1 1 3 5 3 3 5 3 3 1\n [778] 1 2 1 1 1 1 1 1 5 5 4 4 1 3 3 1 3 3 5 3 1 3 3 1 1 1 1 5 1 1 1 5 3 1 1 1 1\n [815] 3 1 1 1 1 1 1 1 1 5 5 5 3 3 5 5 3 3 3 1 3 3 1 1 5 5 5 5 5 4 1 1 1 3 2 3 3\n [852] 3 2 1 1 2 1 2 2 3 3 3 2 2 2 2 5 2 2 2 2 2 2 5 4 4 2 5 2 2 4 4 4 2 5 5 4 5\n [889] 5 1 3 3 3 1 3 1 3 1 2 1 1 3 3 2 4 2 5 1 1 3 3 4 1 1 1 1 1 2 2 2 5 2 2 5 2\n [926] 2 2 2 2 2 3 5 5 3 4 5 2 2 2 3 3 1 3 4 4 4 4 3 3 2 1 1 2 2 2 4 4 1 4 4 5 5\n [963] 5 3 1 3 1 4 4 4 3 3 1 1 5 5 4 4 1 5 3 4 5 4 4 4 5 1 1 5 4 4 4 4 3 3 3 2 4\n[1000] 4 5 3 2 5 2 5 4 1 4 4 4 4 4 1 5 5 1 1 5 5 5 5 1 3 3 3 1 1 1 5 1 5 5 3 1 1\n[1037] 1 1 3 3 1 5 1 1 1 5 5 1 3 3 3 3 4 4 1 3 3 3 3 3 3 3 1 5 5 1 1 3 3 3 1 5 5\n[1074] 1 3 3 5 3 1 3 3 1 1 1 5 5 1 1 1 1 3 3 3 3 3 3 3 3 1 1 1 3 1 1 3 3 1 1 1 5\n[1111] 5 5 3 5 3 3 1 1 1 3 3 1 3 3 3 4 1 3 3 3 4 4 4 4 4 1 4 5 4 4 4 4 4 4 4 4 4\n[1148] 4 4 4 3 4 4 4 4 5 4 4 4 5 3 5 4 4 4 4 4 4 4 4 3 5 1 5 4 3 5 3 5 5 4 4 4 4\n[1185] 4 4 4 5 5 5 5 5 4 5 4 4 4 5 3 5 5 5 5 4 4 4 4 4 3 3 4 4 4 5 5 3 3 5 3 5 5\n[1222] 3 3 1 5 5 4 4 4 5 5 3 5 5 5 5 1 1 5 5 5 5 5 5 3 3 3 3 1 5 5 5 5 5 5 4 4 4\n[1259] 1 5 5 5 5 5 5 5 5 4 4 1 1 1 4 3 5 3 5 5 5 4 5 5 5 5 3 3 3 5 5 5 3 5 5 5 5\n[1296] 4 4 4 4 1 3 3 3 3 3 5 5 5 1 5 5 5 3 5 3 1 1 4 4 4 5 5 4 5 2 2 5 3 1 3 5 1\n[1333] 1 3 3 1 3 1 3 1 1 1 3 2 1 5 5 1 1 1 1 1 1 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5\n[1370] 3 3 1 3 1 3 3 4 4 4 4 5 5 5 5 5 3 1 3 3 5 3 3 3 3 4 5 5 5 5 5 5 5 1 5 5 3\n[1407] 3 3 3 3 3 1 1 1 1 3 1 3 3 1 5 1 1 3 1 1 3 1 1 5 5 3 3 3 1 3 3 3 3 3 3 1 1\n\n...\nand 156 more lines.\n```\n:::\n:::\n\n\nA nicer way to look at the results is by accessing specific parts of the fitted model object, as below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# some basic model metrics\nglance(examp_fit)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"totss\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tot.withinss\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"betweenss\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"iter\"],\"name\":[4],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"61861.2\",\"2\":\"22474.31\",\"3\":\"39386.89\",\"4\":\"4\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# centroid data (transformed/standardized scale)\ncentroids <- tidyclust::extract_centroids(examp_fit)\ncentroids\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".cluster\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"PC01\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PC02\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PC03\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PC04\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PC05\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Cluster_1\",\"2\":\"-3.7357281\",\"3\":\"-0.3166354\",\"4\":\"-0.15275027\",\"5\":\"0.001684125\",\"6\":\"0.4859401\"},{\"1\":\"Cluster_2\",\"2\":\"-0.8825222\",\"3\":\"0.5307642\",\"4\":\"0.29019078\",\"5\":\"-0.081321539\",\"6\":\"-0.3379982\"},{\"1\":\"Cluster_3\",\"2\":\"1.6349352\",\"3\":\"1.1528420\",\"4\":\"-0.02833620\",\"5\":\"-0.175076730\",\"6\":\"0.2152876\"},{\"1\":\"Cluster_4\",\"2\":\"1.5136139\",\"3\":\"-1.1675498\",\"4\":\"-0.04287291\",\"5\":\"1.011715005\",\"6\":\"-0.1041855\"},{\"1\":\"Cluster_5\",\"2\":\"1.7443425\",\"3\":\"-2.6232190\",\"4\":\"-0.74284903\",\"5\":\"-2.312509274\",\"6\":\"-0.1599810\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\n# helpful to add to future plots for examining indiv. clusters\nclust_stat <- tidyclust::sse_within(examp_fit)\nclust_stat\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\".cluster\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"wss\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n_members\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Cluster_1\",\"2\":\"3150.346\",\"3\":\"1096\"},{\"1\":\"Cluster_2\",\"2\":\"4606.289\",\"3\":\"2073\"},{\"1\":\"Cluster_3\",\"2\":\"6763.020\",\"3\":\"1809\"},{\"1\":\"Cluster_4\",\"2\":\"4748.878\",\"3\":\"1463\"},{\"1\":\"Cluster_5\",\"2\":\"3205.773\",\"3\":\"431\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n## Model metrics {#sec-mod-metrics}\n\nSee also section 7.5 in the Chapter by Tan et al. for more about cluster evaluation.\n\n### Extract metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics_df <- km_fit_df %>%\n  mutate(\n    # tot_sse = total sum of squared error\n    tot_sse = map_dbl(km_fit, ~ sse_total_vec(.x)),\n    # tot_wss = sum of within-cluster sse\n    tot_wss = map_dbl(km_fit, ~sse_within_total_vec(.x)),\n    # sse ratio = wss / total sse, \n    sse_ratio = map_dbl(km_fit, ~sse_ratio_vec(.x))\n    )\n\nrm(km_fit_df)\n\nmetrics_simple <- metrics_df %>% \n  select(n_clust, tot_sse, tot_wss, sse_ratio)\n\nmetrics_simple\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"n_clust\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"tot_sse\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"tot_wss\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"sse_ratio\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2\",\"2\":\"61861.2\",\"3\":\"39055.772\",\"4\":\"0.6313452\"},{\"1\":\"3\",\"2\":\"61861.2\",\"3\":\"31115.600\",\"4\":\"0.5029906\"},{\"1\":\"4\",\"2\":\"61861.2\",\"3\":\"26545.763\",\"4\":\"0.4291181\"},{\"1\":\"5\",\"2\":\"61861.2\",\"3\":\"22474.306\",\"4\":\"0.3633021\"},{\"1\":\"6\",\"2\":\"61861.2\",\"3\":\"19465.419\",\"4\":\"0.3146628\"},{\"1\":\"7\",\"2\":\"61861.2\",\"3\":\"17060.585\",\"4\":\"0.2757881\"},{\"1\":\"8\",\"2\":\"61861.2\",\"3\":\"15432.422\",\"4\":\"0.2494685\"},{\"1\":\"9\",\"2\":\"61861.2\",\"3\":\"14516.457\",\"4\":\"0.2346617\"},{\"1\":\"10\",\"2\":\"61861.2\",\"3\":\"13348.700\",\"4\":\"0.2157847\"},{\"1\":\"11\",\"2\":\"61861.2\",\"3\":\"12567.155\",\"4\":\"0.2031508\"},{\"1\":\"12\",\"2\":\"61861.2\",\"3\":\"12114.468\",\"4\":\"0.1958331\"},{\"1\":\"13\",\"2\":\"61861.2\",\"3\":\"11319.202\",\"4\":\"0.1829774\"},{\"1\":\"14\",\"2\":\"61861.2\",\"3\":\"10785.308\",\"4\":\"0.1743469\"},{\"1\":\"15\",\"2\":\"61861.2\",\"3\":\"10509.792\",\"4\":\"0.1698931\"},{\"1\":\"16\",\"2\":\"61861.2\",\"3\":\"10075.480\",\"4\":\"0.1628724\"},{\"1\":\"17\",\"2\":\"61861.2\",\"3\":\"9537.115\",\"4\":\"0.1541696\"},{\"1\":\"18\",\"2\":\"61861.2\",\"3\":\"9228.785\",\"4\":\"0.1491853\"},{\"1\":\"19\",\"2\":\"61861.2\",\"3\":\"8843.186\",\"4\":\"0.1429521\"},{\"1\":\"20\",\"2\":\"61861.2\",\"3\":\"8534.301\",\"4\":\"0.1379589\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\nrm(pca_prep)\nrm(rec_spec)\n```\n:::\n\n\n### Plot Total WSS\n\nNot a clear \"elbow\" here, although by the time we get to 10-11 it does seem to be leveling off.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics_simple %>% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Compare values of k: looking for elbow\")\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmetrics_simple %>% \n  filter(n_clust %in% c(2:12)) %>% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:12)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Zoom in a bit: looking for elbow\")\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n\n### Average Silhouette\n\nFrom the `{tidyclust}` documentation:\n\n> Another common measure of cluster structure is called the **silhouette**.\n> The silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\\\n>\n> In principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\nSee also pg.\n581 in Tan2018 Chap 7 Cluster Analysis: Basic Concepts and Algorithms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprepped_rec <- prep(km_rec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \nbaked_df <- bake(prepped_rec, new_data = NULL) %>% \n  select(-mukey) \n\ndists <- baked_df %>% as.matrix() %>% dist(method = \"euclidean\")\n\nsilh_df <- metrics_df %>% \n  mutate(avg_sil = map_dbl(km_fit, \n                       tidyclust::silhouette_avg_vec,\n                       dists = dists),\n         indiv_sil = map(km_fit, \n                         tidyclust::silhouette,\n                         dists = dists))\n\nindiv_sil_df <- silh_df %>% select(n_clust, indiv_sil) %>% \n  unnest(indiv_sil) %>% \n  mutate(across(.cols = c(cluster, neighbor),\n                .fns = as.character))\n\nwrite_csv(indiv_sil_df, \"data/pca_kmeans_point_silhouettes.csv\")\n\nrm(metrics_df)  \nrm(dists)\nrm(prepped_rec)\n```\n:::\n\n\nHigher silhouette is better (means observations are closer to their centroids than to other observations).\nSeems to suggest that 4, 6, 8, 11 would be OK\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsilh_df %>% \n  ggplot(aes(x = n_clust, y = avg_sil)) +\n  geom_point() + \n  geom_line() + \n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  ggtitle(\"Overall Average Silhouette\") +\n  labs(subtitle = \"Higher is better, possible values [-1,1]\")\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nCan also plot the individual silhouettes.\nFor each clustering (model version), we have a silhouette value per observation in the dataset (n=6872).\nWe also have the closest \"neighbor\" cluster, or the cluster that specific observation would belong to if its home cluster didn't exist.\n\nHere's an example of this data for the k=6 clustering.\nThe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneighbor_counts <- indiv_sil_df %>% \n  group_by(n_clust, cluster, neighbor) %>% \n  count()  %>% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c\"),\n         neighbor = str_replace(neighbor, \"Cluster_\", \"c\"))\n\nk6_neighbor_counts <- neighbor_counts %>% \n  filter(n_clust == 6)\n\n\nindiv_sil_df %>% \n  mutate(across(.cols = c(cluster, neighbor),\n         ~str_replace(.x, \"Cluster_\", \"c\"))) %>% \n  filter(n_clust == 6) %>% \n  ggplot() + \n  geom_boxplot(aes(x = neighbor, y = sil_width)) +\n    geom_point(aes(x = neighbor, y = sil_width),\n             position = position_jitter(width = 0.1),\n             alpha = 0.2,\n             color = \"pink\") + \n  geom_text(data = k6_neighbor_counts,\n            aes(x = neighbor, y = 0.7, label = n),\n            color = \"blue\") +\n  facet_wrap(vars(cluster), scales = \"free_x\") +\n  theme_bw() +\n  ggtitle(\"k=6 silhouettes\")\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclust_sil_avgs <- indiv_sil_df %>% \n  group_by(cluster,\n           n_clust) %>% \n  summarise(mean_sil = mean(sil_width),\n            sd_sil = sd(sil_width), \n            .groups = \"drop\")\n\nclust_sil_avgs %>% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c0\"),\n         cluster = case_when(\n           cluster %in% c(\"c010\", \"c011\", \"c012\", \"c013\", \"c014\", \"c015\", \"c016\",\n                          \"c017\", \"c018\", \"c019\", \"c020\") ~ str_replace(cluster, \"c0\", \"c\"),\n           TRUE ~ cluster\n         )) %>% \n  filter(n_clust %in% c(6:12)) %>% \n  ggplot() +\n  geom_col(aes(y = cluster, x = mean_sil)) +\n  facet_wrap(vars(n_clust), scales = \"free_y\") + \n  ggtitle(\"Average silhouette width per cluster for k=6-12\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n### Not used: Gap statistic\n\nFor `fviz_nbclust()`, first couple times running this, got `Warning: Quick-TRANSFER stage steps exceeded maximum…` Looking online, this seems to be a problem with the model not converging.\nI added some arguments here that are passed on to `kmeans()`, to make sure that the algorithm settings here match what I run above, including `set.seed()`\n\nContinued to get warnings, even though I'm using all the same settings as I use for kmeans up above.\nNot sure why this is, but I'm not going to spend any more time on it right now.\nMaybe see if getting the gap statistic through `NbClust` works better?\n(Later note: NbClust won't be a good option either, I can't alter important kmeans() settings in NbClust).\nExpect it will take a long time either way, consider running this in a separate script and pulling in the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nfviz_gap_stat(x = baked_df, \n             FUNcluster = kmeans,\n             method = c(\"gap_stat\"),\n             k.max = 10, # only considering 2-10 clusters\n             nboot = 50, # default is 100\n             verbose = TRUE, \n             iter.max = 20, # passed to kmeans\n             nstart = 10 # passed to kmeans\n             )\n```\n:::\n\n\n### Calinski-Harabasz index\n\n**Not used: `{NbClust}`** **, using `{fpc}` instead.**\n\n-   For Calinski-Harabasz index, higher values are better\n-   Realized after setting this up with `NbClust` that I don't have the option to pass additional arguments to the `kmeans` function here. So I can't make the algorithm settings exactly match my main clustering pipeline above (where I implement k-means using `tidyclust` and the `tidymodels` framework, and where I save the results for further analysis). This is a problem because I know from my original tests that I need to change the iter.max value to avoid non-convergence issues, and I also want to change nstart because nstart \\>1 is typically known to be best practice (find citation for this).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# keeping this here as a record, but I\"m NOT USING this function for the C-H index. \n\nnbc_indices <- NbClust::NbClust(data = baked_df,\n                 distance = \"euclidean\",\n                 method = \"kmeans\",\n                 min.nc = 2,\n                 max.nc = 20,\n                 index = \"ch\") # Calinski and Harabasz\n\n# enframe turns a named vector into a dataframe\nch_index_vals <- enframe(nbc_indices$All.index) %>% \n  mutate(name = as.integer(name)) %>% \n  rename(n_clust = name)\n```\n:::\n\n\nTrying a different implementation of the Calinski-Harabasz index from the `{fpc}` package.\nThis is preferred to the above approach, where I originally used the `NbClust` function from `{NbClust}` package because I can give this function my clustering generated above (NbClust does its own run of kmeans but I can't customize it to keep it consistent with\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calinhara wants an observations/variables matrix\n# as first argument (as opposed to a distance matrix)\nobsvar_mx <- as.matrix(baked_df)\n\n# function to extract and modify tidyclust clusters\n# into a integer vector, which I will pass to calinhara()\ncreate_clust_vec <- function(fit_obj){\n  \n  extract_cluster_assignment(fit_obj) %>% \n    pull(.cluster) %>% \n    str_replace(., \"Cluster_\", \"\") %>% \n    as.integer()\n  \n}\n\n# apply function to extract clusterings as integer vectors\n# map to get a c-h index value for every value of k (2-20)\nch_metrics <- silh_df %>%\n  select(n_clust, km_fit) %>%\n  mutate(\n    clustering_vec = map(km_fit, create_clust_vec),\n    ch_index = map_dbl(clustering_vec,\n                       ~ fpc::calinhara(x = obsvar_mx,\n                                        clustering = .x)\n    ))\n\nch_metrics %>% \n  ggplot(aes(x = n_clust, y = ch_index)) + \n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  ylab(\"Calinski-Harabasz index\") + \n  ggtitle(\"Calinski-Harabasz\") +\n  labs(subtitle = \"Higher is better\") +\n  scale_x_continuous(breaks = c(2:20))\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n### Hopkins Statistic\n\nUsing the [`{hopkins}`](https://kwstat.github.io/hopkins/reference/hopkins.html) package for this.\nCitations included in the package documentation (also cite Tan et al., 2019 who give an example of using this for evaluating kmeans clusters).\n\n-   Hopkins, B. and Skellam, J.G., 1954. A new method for determining the type of distribution of plant individuals. Annals of Botany, 18(2), pp.213-227.\n-   Cross, G. R., and A. K. Jain. (1982). Measurement of clustering tendency. Theory and Application of Digital Control. Pergamon, 1982. 315-320.\n\nAnd a third citation, helpful illustrations:\n\n-   Lawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of Chemical Information and Computer Sciences, 30(1), 36--41. https://doi.org/10.1021/ci00065a010\n\nApparently `{factoextra}` also has a Hopkins statistic, try that here too.\n(It takes a very long time to run, but returns 0.93, similar to 0.99 returned by `hopkins::hopkins()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nhstat <- hopkins(X = baked_df,\n                 # default, number of rows to sample from the df\n                 m = ceiling(nrow(baked_df)/10), \n                 # default, dimension of the data\n                 d = ncol(baked_df),\n                 # default, kth nearest neighbor to find\n                 k = 1) \nhstat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9999704\n```\n:::\n\n```{.r .cell-code}\nhopkins.pval(x = hstat,\n             # this is the default for hopkins() above\n             n = ceiling(nrow(baked_df)/10)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# below gives 0.9331899 as the result\n# which agrees with above\n# commenting out because it takes a very long time to run\n# factoextra::get_clust_tendency(data = baked_df,\n#                                n = 687, \n#                                graph = FALSE)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n### WSS and Silhouette metrics on one plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| echo: false\n\nsil_totwss <- silh_df %>% \n  select(n_clust, avg_sil, tot_wss, tot_sse, sse_ratio)\n\nch <- ch_metrics %>% \n  select(n_clust, ch_index)\n\nmet_combined <- left_join(sil_totwss, ch, by = \"n_clust\")\n\nwrite_csv(met_combined, \"data/pca_kmeans_cluster_metrics.csv\")\n\nmet2 <- met_combined %>% \n  pivot_longer(cols = -c('n_clust'), names_to = \"metric\",\n               values_to = \"value\")\n\nmet2 %>% \n  ggplot(aes(x = n_clust, y = value)) + \n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(2:20)) + \n  facet_wrap(vars(metric), ncol = 1, scales = \"free\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](22-pca-kmeans_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n## Save model fits\n\nWill save these as Rdata so I can call them up and investigate the cluster centroids more closely in the next chapter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_mods <- silh_df %>% \n  select(n_clust, km_fit)\n\nsave(pca_mods, file = \"data/fitted_pca_kmeans_mods.RData\")\n```\n:::\n\n\n## Save cluster assignments\n\nFor each version of the model (each value of k, different numbers of clusters), a MUKEY is assigned to a specific cluster.\nHere, I'm pulling that data, shaping it into one dataframe (one row per MUKEY, cluster assingments in separate columns).\nI'm also adding back in the soil property data so we can use this in the next step when evaluating different cluster sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclust_assign_df <- pca_mods %>% \n  mutate(clust_assign = map(km_fit, ~augment(.x, new_data = dat_for_kmeans)),\n         mukey_clust = map(clust_assign, ~select(.x, mukey, .pred_cluster)))\n\n\nassign_mukey_df <- clust_assign_df %>% \n  select(n_clust, mukey_clust) %>% \n  unnest(mukey_clust) %>% pivot_wider(names_from = n_clust, values_from = .pred_cluster, names_prefix = \"k_\")\n\n# drop k_6 column from d (it was from orig k-means clusters,\n# used above for viz purposes only)\n\nsoil_props <- d %>% select(-k_6) \n\nclust_props <- full_join(soil_props, assign_mukey_df, by = \"mukey\")\n\nwrite_csv(clust_props, \"data/pca_mukey_cluster_assignments_and_soilprops.csv\")\n```\n:::\n",
    "supporting": [
      "22-pca-kmeans_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}