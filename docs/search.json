[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Minnesota Topsoil Classification for Soil Health Assessment",
    "section": "",
    "text": "Overview\nThis is an analysis notebook with code and notes on the analysis presented in Blair et al. (2024) manuscript “A data-driven topsoil classification to support soil health assessment in Minnesota, USA”\nManuscript authors: Hava K Blair, Jessica M. Gutknecht, Anna M. Cates, A. Marcelle Lewandowski, Nicolas A. Jelinski"
  },
  {
    "objectID": "02-data_sources.html#define-area-of-interest",
    "href": "02-data_sources.html#define-area-of-interest",
    "title": "1  Data Sources",
    "section": "1.1 Define Area of Interest",
    "text": "1.1 Define Area of Interest\n\nUnion of MN county boundaries and MLRA boundaries\n\nAn idea for subsetting later - bring in Cropland Data Layer. Exclude pixels with impervious &gt;50%, or some other %?"
  },
  {
    "objectID": "02-data_sources.html#major-land-resource-areas",
    "href": "02-data_sources.html#major-land-resource-areas",
    "title": "1  Data Sources",
    "section": "1.2 Major Land Resource Areas",
    "text": "1.2 Major Land Resource Areas\n\nAccess the USDA’s Geospatial Data Gateway.\n\nClick “Order by State” link, select Minnesota\n\nSelect Major Land Resource Areas map layers\nFormat: ESRI Shape (not sure about this, other options are ESRI File GeoDatabase, separate ESRI shapefiles, separate ESRI GeoDatabase Feature Classes)\n\nThe code below is reproduced from create_aoi_shapefile.R.\n\n# USA MLRAs\nmlras &lt;- st_read(\"data/mlra/mlra_v42.shp\")\n\nReading layer `mlra_v42' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\data\\mlra\\mlra_v42.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5306 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -179.1334 ymin: -14.38165 xmax: 179.7882 ymax: 71.39805\nGeodetic CRS:  NAD83\n\n# State boundary\nmn &lt;- st_read(\"_qgis_files/shp_mn_state_boundary/mn_state_boundary.shp\")\n\nReading layer `mn_state_boundary' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\_qgis_files\\shp_mn_state_boundary\\mn_state_boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -97.23921 ymin: 43.49936 xmax: -89.49174 ymax: 49.38436\nGeodetic CRS:  NAD83\n\n# confirm CRS the same\nst_crs(mn) == st_crs(mlras)\n\n[1] TRUE\n\n# keep the intersection of MLRAs and MN boundary\nmn_mlras &lt;- st_intersection(mn, mlras)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n# check out the result\nplot(mn_mlras[\"MLRARSYM\"])\n\n\n\n\nNow that I have the MLRA-MN intersection, need to subset to only the target MLRAs (selected b/c we know we will have validation data broadly in these regions, and because they are domianted by agriculture, unlike the NE part of the state)\n\n# keep only target MLRAs (agricultural regions of the state)\n# clipped to MN boundary\n\nkeep_mlrarsym &lt;- c(\"56\",# Red River Valley of the North\n                \"102A\", # Rolling Till Prairie\n                \"91A\", # Central MN Sandy Outwash\n                \"57\", # Northern Minnesota Gray Drift\n                \"103\", # Central IA and MN Till Prairies\n                \"104\", # Eastern IA and MN Till Prairies\n                \"105\") # Northern Mississippi Valley Loess Hills \n\nmn_targets &lt;- mn_mlras %&gt;% \n  rownames_to_column(var = \"rowid\") %&gt;% \n  filter(MLRARSYM %in% keep_mlrarsym,\n         # drops the northern portion of N MN Gray Drift \n         # which was excluded b/c lack of validation pts \n         rowid != \"1.4\") \n\nmn_targets %&gt;% ggplot() + \n  geom_sf(aes(fill = MLRARSYM))\n\n\n\n# might want to make a map that shows the full extent of the MLRAs in MN, \n# extending out to neighborhing states. \nmn_mlras_extend &lt;- mlras %&gt;% \n  filter(MLRARSYM %in% keep_mlrarsym) %&gt;% \n  # drops the northern portion of N MN Gray Drift \n  # which was excluded b/c lack of validation pts \n  slice(-2)\n\nggplot(data = mn_mlras_extend) +\n  geom_sf(aes(fill = MLRARSYM)) +\n  theme_bw()"
  },
  {
    "objectID": "02-data_sources.html#minnesota-county-boundaries",
    "href": "02-data_sources.html#minnesota-county-boundaries",
    "title": "1  Data Sources",
    "section": "1.3 Minnesota County Boundaries",
    "text": "1.3 Minnesota County Boundaries\nDownloaded the MDNR’s version of county boundaries from MN Geospatial"
  },
  {
    "objectID": "02-data_sources.html#minnesota-state-boundary",
    "href": "02-data_sources.html#minnesota-state-boundary",
    "title": "1  Data Sources",
    "section": "1.4 Minnesota State Boundary",
    "text": "1.4 Minnesota State Boundary\nMN State Boundary downloaded from US Census Cartographic Boundary Files\nhttps://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html\nFile is the States 2018 500K shapefile"
  },
  {
    "objectID": "02-data_sources.html#national-cultivated-layer",
    "href": "02-data_sources.html#national-cultivated-layer",
    "title": "1  Data Sources",
    "section": "1.5 2020 National Cultivated Layer",
    "text": "1.5 2020 National Cultivated Layer\n\nAccess the data on the Cropland “Research & Science” page\n\nDownload zip file of 2020 National Cultivated Layer\n\nI think we want the “Cultivated Layer”, which is based on most recent 5 years of data, updated annually. Pixels are included in this layer if it was identified as cultivated in at least 2 of the last 5 years."
  },
  {
    "objectID": "02-data_sources.html#soils-data",
    "href": "02-data_sources.html#soils-data",
    "title": "1  Data Sources",
    "section": "1.6 Soils Data",
    "text": "1.6 Soils Data\n\n1.6.1 gSSURGO data\n\nNavigate to the Geospatial Data Gateway’s “Direct DAta/NAIP Download” list\n\nClick link for Soils Geographic Databases\nOctober 2021 gSSURGO by State\n\nDownload ZIP file for MN\n\nCitation for gSSURGO data:\nSoil Survey Staff. Gridded Soil Survey Geographic (gSSURGO) Database for Minnesota. United States Department of Agriculture, Natural Resources Conservation Service. Available online at https://gdg.sc.egov.usda.gov/. November, 22, 2021 (2021 official release).\n\n# inspect layers in .gdb\nsf::st_layers(\"./data/gSSURGO_MN/gSSURGO_MN.gdb\")\n\nDriver: OpenFileGDB \nAvailable layers:\n                   layer_name     geometry_type features fields\n1                    chaashto                NA   223330      4\n2               chconsistence                NA     2794     10\n3               chdesgnsuffix                NA    62846      3\n4                     chfrags                NA   321265     12\n5                    chorizon                NA   130493    171\n6                     chpores                NA       16      9\n7                    chstruct                NA   110021      7\n8                 chstructgrp                NA    95901      4\n9                      chtext                NA        0      7\n10                  chtexture                NA   318704      4\n11               chtexturegrp                NA   313739      6\n12               chtexturemod                NA    50000      3\n13                  chunified                NA   240027      4\n14              cocanopycover                NA       50      6\n15                  cocropyld                NA    34146     12\n16             codiagfeatures                NA    31760     12\n17                 coecoclass                NA    52483      6\n18                  coeplants                NA    25544      7\n19               coerosionacc                NA     5743      4\n20                  coforprod                NA     2726     12\n21                 coforprodo                NA        0     10\n22               cogeomordesc                NA    83667      8\n23           cohydriccriteria                NA    25091      3\n24                   cointerp                NA  5698886     13\n25                    comonth                NA   421728     17\n26                  component                NA    42529    109\n27                       copm                NA    49990      7\n28                    copmgrp                NA    34883      4\n29               copwindbreak                NA   161238      8\n30             corestrictions                NA     4369     13\n31                cosoilmoist                NA   666791      9\n32                 cosoiltemp                NA       96      9\n33                cosurffrags                NA    10955     15\n34              cosurfmorphgc                NA    17984      6\n35             cosurfmorphhpp                NA    28248      3\n36              cosurfmorphmr                NA      460      3\n37              cosurfmorphss                NA    42228      4\n38                 cotaxfmmin                NA    33654      3\n39               cotaxmoistcl                NA    20332      3\n40                     cotext                NA    42529      7\n41               cotreestomng                NA    32192      5\n42                cotxfmother                NA    31091      3\n43               distinterpmd                NA    11868      8\n44               distlegendmd                NA       92     11\n45                     distmd                NA       92      4\n46                   featdesc                NA      801      6\n47                  laoverlap                NA      512      6\n48                     legend                NA       92     14\n49                 legendtext                NA        0      7\n50                    mapunit                NA    10688     24\n51                      month                NA       12      2\n52                   muaggatt                NA    10688     40\n53                 muaoverlap                NA    35224      4\n54                  mucropyld                NA    23355     10\n55                     mutext                NA    12230      7\n56                  sacatalog                NA       92     11\n57                   sainterp                NA    11868      9\n58               sdvalgorithm                NA        8      4\n59               sdvattribute                NA      211     53\n60                  sdvfolder                NA       20      6\n61         sdvfolderattribute                NA      213      2\n62               mdstatdomdet                NA     6930      5\n63               mdstatdommas                NA      123      2\n64               mdstatidxdet                NA      172      4\n65               mdstatidxmas                NA      149      3\n66             mdstatrshipdet                NA       66      5\n67             mdstatrshipmas                NA       63      5\n68              mdstattabcols                NA      865     14\n69                 mdstattabs                NA       75      5\n70                   FEATLINE Multi Line String    60902      5\n71                  FEATPOINT             Point   254435      4\n72                     MULINE Multi Line String        0      5\n73                    MUPOINT             Point        0      4\n74                  SAPOLYGON     Multi Polygon       96      5\n75                  MUPOLYGON     Multi Polygon  2123552      6\n76                      Valu1                NA    10688     58\n77          MapunitRaster_10m     Multi Polygon        1      3\n78      VAT_MapunitRaster_10m                NA    10688      3\n79 fras_aux_MapunitRaster_10m                NA        1      3\n80 fras_blk_MapunitRaster_10m                NA   180371      6\n81 fras_bnd_MapunitRaster_10m                NA        1     18\n82 fras_ras_MapunitRaster_10m                NA        1      3\n               crs_name\n1                  &lt;NA&gt;\n2                  &lt;NA&gt;\n3                  &lt;NA&gt;\n4                  &lt;NA&gt;\n5                  &lt;NA&gt;\n6                  &lt;NA&gt;\n7                  &lt;NA&gt;\n8                  &lt;NA&gt;\n9                  &lt;NA&gt;\n10                 &lt;NA&gt;\n11                 &lt;NA&gt;\n12                 &lt;NA&gt;\n13                 &lt;NA&gt;\n14                 &lt;NA&gt;\n15                 &lt;NA&gt;\n16                 &lt;NA&gt;\n17                 &lt;NA&gt;\n18                 &lt;NA&gt;\n19                 &lt;NA&gt;\n20                 &lt;NA&gt;\n21                 &lt;NA&gt;\n22                 &lt;NA&gt;\n23                 &lt;NA&gt;\n24                 &lt;NA&gt;\n25                 &lt;NA&gt;\n26                 &lt;NA&gt;\n27                 &lt;NA&gt;\n28                 &lt;NA&gt;\n29                 &lt;NA&gt;\n30                 &lt;NA&gt;\n31                 &lt;NA&gt;\n32                 &lt;NA&gt;\n33                 &lt;NA&gt;\n34                 &lt;NA&gt;\n35                 &lt;NA&gt;\n36                 &lt;NA&gt;\n37                 &lt;NA&gt;\n38                 &lt;NA&gt;\n39                 &lt;NA&gt;\n40                 &lt;NA&gt;\n41                 &lt;NA&gt;\n42                 &lt;NA&gt;\n43                 &lt;NA&gt;\n44                 &lt;NA&gt;\n45                 &lt;NA&gt;\n46                 &lt;NA&gt;\n47                 &lt;NA&gt;\n48                 &lt;NA&gt;\n49                 &lt;NA&gt;\n50                 &lt;NA&gt;\n51                 &lt;NA&gt;\n52                 &lt;NA&gt;\n53                 &lt;NA&gt;\n54                 &lt;NA&gt;\n55                 &lt;NA&gt;\n56                 &lt;NA&gt;\n57                 &lt;NA&gt;\n58                 &lt;NA&gt;\n59                 &lt;NA&gt;\n60                 &lt;NA&gt;\n61                 &lt;NA&gt;\n62                 &lt;NA&gt;\n63                 &lt;NA&gt;\n64                 &lt;NA&gt;\n65                 &lt;NA&gt;\n66                 &lt;NA&gt;\n67                 &lt;NA&gt;\n68                 &lt;NA&gt;\n69                 &lt;NA&gt;\n70 NAD83 / Conus Albers\n71 NAD83 / Conus Albers\n72 NAD83 / Conus Albers\n73 NAD83 / Conus Albers\n74 NAD83 / Conus Albers\n75 NAD83 / Conus Albers\n76                 &lt;NA&gt;\n77 NAD83 / Conus Albers\n78                 &lt;NA&gt;\n79                 &lt;NA&gt;\n80                 &lt;NA&gt;\n81                 &lt;NA&gt;\n82                 &lt;NA&gt;\n\n\n\n\n1.6.2 Process for .gdb &gt; .tif\nWhen you unzip gSSURGO, the data is in ESRI geodatabase (.gdb) format. I had thought this was a proprietary format that you had to open in an ESRI program like ArcMap, but this 2015 blog post from UCLA suggests otherwise (and helpfully explains some GDAL drivers for opening .gdbs). Also this blog post from 2021 walks through opening a .gdb in QGIS, but doesn’t mention a maximum file size (which I thought was an issue?). Anyway, I think QGIS froze when I tried to open the .gdb, so I used ArcMap instead to save it is a .tif, as describe below.\nFor future, gis.stackexchange post suggests that maybe using a database like spatialite or postgis would be helpful? Learn more about this, starting with the QGIS docs here (esp module 16 and 18). Also for future,some breadcrumbs that maybe .gdb IS in fact proprietary and must be opened using ESRI software is necessary. Distinction that there is no GDAL raster driver for ESRI GeoDatabase files (for vector I believe there is a driver, stumbled on some posts about this).\nOther posts on this topic, both of which support the majority opinion that for rasters you really do need to open in ArcMap and export to get free of the .gdb :\nhttps://gis.stackexchange.com/questions/385255/how-to-extract-raster-from-gdb-instead-of-empty-polygons\nhttps://stackoverflow.com/questions/27821571/working-with-rasters-in-file-geodatabase-gdb-with-gdal\n(Side note, it is possible to access the tabular data in R without changing the .gdb format as demonstrated in the code chunk above, but for the spatial data I still needed to convert from .gdb to .tif).\nArcMap documentation explains that there are two ways to export a dataset (which is when you would have the option to change the file type). These are:\n\nexport raster data dialog box. I get this by right clicking the data layer listing in the side panel of ArcMap and selecting “export…”\ncopy raster tool. this tool can be found in Data management toolbox &gt; raster toolset. The documentation is helpful. I used this to scale my pixels to a new bit depth (32bit -&gt; 16bit) after replacing the values with my shorter MUKEYs using a reclass (see below). This reduced the overall file size of the tif significantly.\n\nMore about the export raster data dialog box (in ESRI’s words, from documentation linked above:\n\nThe dialog box allows you to export a raster dataset or a portion of a raster dataset. Unlike other raster import or export tools, the Export Raster Data dialog box gives you additional capabilities such as clipping via the current data frame area, clipping via a selected graphic, choosing the spatial reference of the data frame, using the current renderer, choosing the output cell size, or specifying the NoData value. In addition, you will be able to choose the output format for the raster dataset: BMP, ENVI, Esri BIL, Esri BIP, Esri BSQ, GIF, GRID, IMG, JPEG, JPEG 2000, PNG, TIFF, or exporting to a geodatabase.\n\nI learned through trial and error about what happens when you lower the pixel depth even though you have values beyond the acceptable range: “If the pixel type is demoted (lowered), the raster values outside the valid range for that pixel depth will be truncated and lost”\n\n\n1.6.3 Process for raster reclass (new MUKEYs)\nOpen .tif version of gSSURGO in ArcMap version XXX.\nFirst need to create a table of new (shorter) MUKEY values. The table I provided to the raster reclass tool below was a text file of our cross-walk between the original map unit keys and our new, shorter map unit keys that allowed us to reduce the file size by going down to 16bit pixel depth. I created this original crosswalk table by:\n\nBuild pyramids\nBuild attribute table in Data Management Toolbox &gt; Raster &gt; Raster Properties &gt; Build Raster Attribute Table\nOpen attribute table, add field called “new_mu” with “short integer type” (b/c only 2 bits to store short integers and we want to reduce size). Field properties “precision” leaving at 0. Now new_mu is the same as OID, just integers starting at 0 and going all the way to 7861\nSaved attribute table as a .txt file for future reference.\n\nThen, I performed a raster reclass by table. Had to turn on the 3D analyst tools before I could access them (they were grayed out). Using Customize menu &gt; Extensions, turned on the 3D analyst tools and spatial analyst tools\nNow in the ArcToolbox, going to 3D analyst tools &gt; Raster Reclass &gt; Reclass by Table\nInput raster: MapunitRaster_10m_Clip1\nInput remap table: mukey_new_crosswalk.txt\nFrom value field: MUKEY\nTo value field: MUKEY\nOutput value fuield: MUKEY_new\nChange missing values to NoData: DATA?\nSUCCESS with raster reclass above. To save, I used “Export Raster Data”\nDialog box shows that the uncompressed size is 5.42 GB (!!), and the pixel depth is 16bit. I am assigning the NoData value as 7999 (our max MU value is 7861)\nSaving to C:\\Users\\blair304\\Desktop\\MapunitRaster_10m_Clip1, name Reclass_tif1.tif, compression type NONE, compression quality set at default 75 (can’t see a way to change it?)"
  },
  {
    "objectID": "02-data_sources.html#ncss-kellogg-lab-data",
    "href": "02-data_sources.html#ncss-kellogg-lab-data",
    "title": "1  Data Sources",
    "section": "1.7 NCSS (Kellogg Lab) Data",
    "text": "1.7 NCSS (Kellogg Lab) Data\nNavigate to this link: https://ncsslabdatamart.sc.egov.usda.gov/\nClick on “Advanced Query”\nSpecify:\n\nCountry: United States\nState: Minnesota\nSubmission Date Jan 1, 2000 - Oct 17 2022\n\nThis returns 145 records.\nHad to do the downloads separated by “data tiers” because I got a network timeout / server error when I tried highlighting everything. Here are the batches:\n\nCarbon and extractions\nPSDA & Rock frags, WAter content\nBulk density & moisture, water content\nCEC and bases, salt, organic\nPhosphorus\nTaxonomy tier 1 & 2\npH & carbonates\n\n\n1.7.1 Accessing data\n\nFor each chunk of data I downloaded (see bullet points above), have a folder with CSV files. Each folder contains a data dictionary, a “site” csv, and a “pedon” csv\nlatitude & longitude data are in the “site” files. Not clear if there is usually 1 pedon per site, and so this lat/lon are probably right? Or would there be multiple pedons at a given site, so the site location won’t be quite right?\nwith code in R/unzip_merge_ncss_data.R, created “NCSS_validation_point_key_site_pedon.csv”, which is all pedons with location data (56 total)\n\nNeed to match up the right lab methods from the NCSS dataset so they correspond with the data I pulled from gSSURGO. Looking up more info in the gSSURGO metadata about these to see if methods are listed\n\n\n\n\n\n\n\n\nvariable\ngSSURGO info\nSpreadsheet\n\n\n\n\nx clay\n\n\n\n\nx SOC (for OM)\nOM (LOI), pull SOC data from NCSS and convert\n\n\n\nx CEC\npH 7\n\n\n\nx bulk density\n1/3 bar\nDb1/3_4A1d_Caj_g/cc_0_CMS_0_0 AND/OR Db13b_3B1b_Caj_0_SSL_0_0 AND/OR Db13b_DbWR1_Caj_g/cc_0_SSL_0_0\n\n\nx EC\nsaturated paste\n\n\n\nx pH\n1:1 H2O\n\n\n\nx carbonates\nweight percent CaCO3 equivalent\n\n\n\nx LEP\n\nCOLE?\n\n\nO KSAT\n\nSee notes below on ksat, don’t have this data for now, think it’s not the most important for validation\n\n\nx AWC\nvolume fraction, diff b/t water contents at 1/10 or 1/3 and 15 bar\n0 or 1/3 bar and 15 bars tension\n\n\nx fragvol_r\nvolume percentage of horizon occupied by 2mm or larger fraction (20mm or larger for wood fragments) on a whole soil basis\n? I found weight fractions but not volume fractions in the PSDA and rock fragments table.\n\n\n\nI couldn’t find ksat data in any of the NCSS tables. Suspect it might be because it is estimated using some kind of pedotransfer function. I found a Masters thesis from 2017 by Joshua Randall Weaver that seems to confirm this based on references in their literature review. The thesis is “Comparison of Saturated Hydraulic Conductivity using Geospatial Analysis of Field and SSURGO data for septic tank suitability assessment”, Clemson University. On pg 2 (introduction), Weaver states\n\n“Currently, SSURGO-reported saturated hydraulic conductivity (Ksat) data are often estimated from particle-size analysis (PSA) data from specific locations and then extrapolated across large areas based on soil map units (O’Neal, 1952; Rawls and Brakensiek, 1983; Williamson et al., 2014).” Rare comparisons of SSURGO recorded PSA-derived Ksat values are often different from site-specific field Ksat measurements (Hart et al. 2008). The freely available PSA-derived Ksat data from SSURGO is frequently used for regional and national modelling for the purposes of environmental management, but spatial variability associated with using SSURGO data instead of site-specific data is largely unknown (Hoos and McMahon, 2009). ”\n\nFor now I’m not going to worry too much about Ksat data, as it doesn’t seem like the most important thing to validate (very few SH studies collect it right now, it’s highly variable, it will be related to particle size). So I’m leaving this aside for now, but wanted to document in case I come back in the future."
  },
  {
    "objectID": "02-data_sources.html#noaa-ncei-u.s.-climate-normals",
    "href": "02-data_sources.html#noaa-ncei-u.s.-climate-normals",
    "title": "1  Data Sources",
    "section": "1.8 NOAA NCEI U.S. Climate Normals",
    "text": "1.8 NOAA NCEI U.S. Climate Normals\n\nOn January 9, 2023 Access the map interface for US Climate Normals Data at https://www.ncei.noaa.gov/maps/normals\nSelect 1991-2020 Climate Normals in the side bar, Annual Normals (hourly, daily, and monthly are also available)\nSelect the wrench icon next to the selected dataset in the sidebar to open the “Tools” for that dataset. Use the “identify” (i) tool to pick a station and learn it’s name and ID number.\nI picked two stations:\n\nOne in the far northwest (Hallock, MN USC00213455) and one in the far southeast (Caledonia MN, USC00211198)\nNW: Hallock, MN MAT is 39.1 F and MAP is 22.31 inches\nSE: Caledonia, MN MAT is 45.5 F and MAP is 38.32 in\n\nRather than download the full dataset, I used the station names I had identified to see a quick summary with the “U.S. Climate Normals Quick Access” Tool.\n\nSelected the “Annual/Seasonal Tab” and 1991-2020 tab\n\n\nRecommended dataset citation, per metadata page, is Arguez, A., I. Durre, S. Applequist, R. Vose, M. Squires, X. Yin, R. Heim, and T. Owen, 2012: NOAA’s 1981-2010 climate normals: An overview. Bull. Amer. Meteor. Soc., 93, 1687-1697. (this is saved in my Zotero library already)\n\n\n1.8.1 Convert MAP and MAT values\n\n# MAT convert from F to C: subtract 32 and multiply by 5/9\n(39.1 - 32)*(5/9) # Hallock (NW)\n\n[1] 3.944444\n\n(45.5- 32)*(5/9) # Caledonia (SE)\n\n[1] 7.5\n\n# MAP convert inches to mm (1in=2.54cm, 10mm = 1cm)\n22.31*2.54*10 # Hallock (NW)\n\n[1] 566.674\n\n38.32*2.54*10 # Caledonia (SE)\n\n[1] 973.328"
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#overview",
    "href": "03-mus_comp_in_aoi.html#overview",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nI have a list of map unit keys from my clipped area of interest. In order to reduce the file size of the TIF I was working with, I created new map unit key IDs. The cross-walk table is in data/gSSURGO_MN/mukey_new_crosswalk.txt. There are 7,862 unique map unit keys in my area of interest (AOI).\nThis is what the cross-walk table looks like:"
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#map-units",
    "href": "03-mus_comp_in_aoi.html#map-units",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.2 Map Units",
    "text": "2.2 Map Units\nI can use the map unit keys (MUKEYs) from my AOI to get more information on my target map units by calling up different tables from gSSURGO and subsetting based on the MUKEYs, and identifying the components within each mapunit in my AOI.\nFor more info about SSURGO tables and columns, refer to descriptions in the SSURGO metadata PDFs in data/gSSURGO_MN/\n\n# which layers (tables) are available to me?\n# can inspect layers in .gdb\nsf::st_layers(\"./data/gSSURGO_MN/gSSURGO_MN.gdb\")\n\nDriver: OpenFileGDB \nAvailable layers:\n           layer_name     geometry_type features fields\n1            chaashto                NA   223330      4\n2       chconsistence                NA     2794     10\n3       chdesgnsuffix                NA    62846      3\n4             chfrags                NA   321265     12\n5            chorizon                NA   130493    171\n6             chpores                NA       16      9\n7            chstruct                NA   110021      7\n8         chstructgrp                NA    95901      4\n9              chtext                NA        0      7\n10          chtexture                NA   318704      4\n11       chtexturegrp                NA   313739      6\n12       chtexturemod                NA    50000      3\n13          chunified                NA   240027      4\n14      cocanopycover                NA       50      6\n15          cocropyld                NA    34146     12\n16     codiagfeatures                NA    31760     12\n17         coecoclass                NA    52483      6\n18          coeplants                NA    25544      7\n19       coerosionacc                NA     5743      4\n20          coforprod                NA     2726     12\n21         coforprodo                NA        0     10\n22       cogeomordesc                NA    83667      8\n23   cohydriccriteria                NA    25091      3\n24           cointerp                NA  5698886     13\n25            comonth                NA   421728     17\n26          component                NA    42529    109\n27               copm                NA    49990      7\n28            copmgrp                NA    34883      4\n29       copwindbreak                NA   161238      8\n30     corestrictions                NA     4369     13\n31        cosoilmoist                NA   666791      9\n32         cosoiltemp                NA       96      9\n33        cosurffrags                NA    10955     15\n34      cosurfmorphgc                NA    17984      6\n35     cosurfmorphhpp                NA    28248      3\n36      cosurfmorphmr                NA      460      3\n37      cosurfmorphss                NA    42228      4\n38         cotaxfmmin                NA    33654      3\n39       cotaxmoistcl                NA    20332      3\n40             cotext                NA    42529      7\n41       cotreestomng                NA    32192      5\n42        cotxfmother                NA    31091      3\n43       distinterpmd                NA    11868      8\n44       distlegendmd                NA       92     11\n45             distmd                NA       92      4\n46           featdesc                NA      801      6\n47          laoverlap                NA      512      6\n48             legend                NA       92     14\n49         legendtext                NA        0      7\n50            mapunit                NA    10688     24\n51              month                NA       12      2\n52           muaggatt                NA    10688     40\n53         muaoverlap                NA    35224      4\n54          mucropyld                NA    23355     10\n55             mutext                NA    12230      7\n56          sacatalog                NA       92     11\n57           sainterp                NA    11868      9\n58       sdvalgorithm                NA        8      4\n59       sdvattribute                NA      211     53\n60          sdvfolder                NA       20      6\n61 sdvfolderattribute                NA      213      2\n62       mdstatdomdet                NA     6930      5\n63       mdstatdommas                NA      123      2\n64       mdstatidxdet                NA      172      4\n65       mdstatidxmas                NA      149      3\n66     mdstatrshipdet                NA       66      5\n67     mdstatrshipmas                NA       63      5\n68      mdstattabcols                NA      865     14\n69         mdstattabs                NA       75      5\n70           FEATLINE Multi Line String    60902      5\n71          FEATPOINT             Point   254435      4\n72             MULINE Multi Line String        0      5\n73            MUPOINT             Point        0      4\n74          SAPOLYGON     Multi Polygon       96      5\n75          MUPOLYGON     Multi Polygon  2123552      6\n76              Valu1                NA    10688     58\n\nmn_gdb &lt;- \"data/gSSURGO_MN/gSSURGO_MN.gdb\" \n\n# read only mapunit table, as dataframe\nmn_mapunits &lt;- sf::st_read(dsn = mn_gdb, layer = \"mapunit\")\n\nReading layer `mapunit' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n\nThese are the columns in the mapunit table:\n\ncolnames(mn_mapunits)\n\n [1] \"musym\"         \"muname\"        \"mukind\"        \"mustatus\"     \n [5] \"muacres\"       \"mapunitlfw_l\"  \"mapunitlfw_r\"  \"mapunitlfw_h\" \n [9] \"mapunitpfa_l\"  \"mapunitpfa_r\"  \"mapunitpfa_h\"  \"farmlndcl\"    \n[13] \"muhelcl\"       \"muwathelcl\"    \"muwndhelcl\"    \"interpfocus\"  \n[17] \"invesintens\"   \"iacornsr\"      \"nhiforsoigrp\"  \"nhspiagr\"     \n[21] \"vtsepticsyscl\" \"mucertstat\"    \"lkey\"          \"mukey\"        \n\n\nWe will subset to only include MUKEYs in our AOI.\n\n# keep only the map units in my AOI (n=7862)\ntarget_mapunits &lt;- mn_mapunits %&gt;% \n  filter(mukey %in% aoi_mu$MUKEY)\n\n# summary of map unit types\ntarget_mapunits %&gt;% \n  dplyr::group_by(mukind) %&gt;% \n  dplyr::summarise(n = n()) \n\n\n\n  \n\n\n# when is mukind undefined?\ntarget_mapunits %&gt;% \n  filter(is.na(mukind))\n\n\n\n  \n\n\n# what's going on with W = water?\nwater_mukeys &lt;- target_mapunits %&gt;% \n  filter(musym == \"W\") %&gt;% \n  pull(mukey)\n\ntarget_mapunits %&gt;% \n  filter(mukey %in% water_mukeys) %&gt;% \n  head()\n\n\n\n  \n\n\n# saving CSV of target map unit info\nwrite_csv(target_mapunits, \"data/target_mapunit_table.csv\")"
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#components",
    "href": "03-mus_comp_in_aoi.html#components",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.3 Components",
    "text": "2.3 Components\nFirst a reminder of the columns in the component table:\n\nmn_components &lt;- sf::st_read(dsn = mn_gdb, layer = \"component\")\n\nReading layer `component' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\ncolnames(mn_components)\n\n  [1] \"comppct_l\"        \"comppct_r\"        \"comppct_h\"       \n  [4] \"compname\"         \"compkind\"         \"majcompflag\"     \n  [7] \"otherph\"          \"localphase\"       \"slope_l\"         \n [10] \"slope_r\"          \"slope_h\"          \"slopelenusle_l\"  \n [13] \"slopelenusle_r\"   \"slopelenusle_h\"   \"runoff\"          \n [16] \"tfact\"            \"wei\"              \"weg\"             \n [19] \"erocl\"            \"earthcovkind1\"    \"earthcovkind2\"   \n [22] \"hydricon\"         \"hydricrating\"     \"drainagecl\"      \n [25] \"elev_l\"           \"elev_r\"           \"elev_h\"          \n [28] \"aspectccwise\"     \"aspectrep\"        \"aspectcwise\"     \n [31] \"geomdesc\"         \"albedodry_l\"      \"albedodry_r\"     \n [34] \"albedodry_h\"      \"airtempa_l\"       \"airtempa_r\"      \n [37] \"airtempa_h\"       \"map_l\"            \"map_r\"           \n [40] \"map_h\"            \"reannualprecip_l\" \"reannualprecip_r\"\n [43] \"reannualprecip_h\" \"ffd_l\"            \"ffd_r\"           \n [46] \"ffd_h\"            \"nirrcapcl\"        \"nirrcapscl\"      \n [49] \"nirrcapunit\"      \"irrcapcl\"         \"irrcapscl\"       \n [52] \"irrcapunit\"       \"cropprodindex\"    \"constreeshrubgrp\"\n [55] \"wndbrksuitgrp\"    \"rsprod_l\"         \"rsprod_r\"        \n [58] \"rsprod_h\"         \"foragesuitgrpid\"  \"wlgrain\"         \n [61] \"wlgrass\"          \"wlherbaceous\"     \"wlshrub\"         \n [64] \"wlconiferous\"     \"wlhardwood\"       \"wlwetplant\"      \n [67] \"wlshallowwat\"     \"wlrangeland\"      \"wlopenland\"      \n [70] \"wlwoodland\"       \"wlwetland\"        \"soilslippot\"     \n [73] \"frostact\"         \"initsub_l\"        \"initsub_r\"       \n [76] \"initsub_h\"        \"totalsub_l\"       \"totalsub_r\"      \n [79] \"totalsub_h\"       \"hydgrp\"           \"corcon\"          \n [82] \"corsteel\"         \"taxclname\"        \"taxorder\"        \n [85] \"taxsuborder\"      \"taxgrtgroup\"      \"taxsubgrp\"       \n [88] \"taxpartsize\"      \"taxpartsizemod\"   \"taxceactcl\"      \n [91] \"taxreaction\"      \"taxtempcl\"        \"taxmoistscl\"     \n [94] \"taxtempregime\"    \"soiltaxedition\"   \"castorieindex\"   \n [97] \"flecolcomnum\"     \"flhe\"             \"flphe\"           \n[100] \"flsoilleachpot\"   \"flsoirunoffpot\"   \"fltemik2use\"     \n[103] \"fltriumph2use\"    \"indraingrp\"       \"innitrateleachi\" \n[106] \"misoimgmtgrp\"     \"vasoimgtgrp\"      \"mukey\"           \n[109] \"cokey\"           \n\n\nSummarizing some info about number of components, major components\n\n# keep the components in my target map units\ntarget_comp &lt;- mn_components %&gt;% \n  filter(mukey %in% target_mapunits$mukey) \n\n# looking at this, it's probabaly safe to exclude \n# compname == \"Water\" when the comppct_r is also 100%\n# there's one here where water is only 5%, \"Riverwash\", rest are 100%\nwater_comp &lt;- target_comp %&gt;% filter(mukey %in% water_mukeys)\n\n# how many components?\nnrow(target_comp)\n\n[1] 31065\n\n# how many major components? (defined by \"majcompflag\")\ntarget_comp %&gt;% \n  filter(majcompflag == \"Yes\") %&gt;% \n  nrow() \n\n[1] 10588\n\n\nFor our unique MUKEYs, how many components in each?\n\n# how many components per mukey? \ncomp_nest &lt;- target_comp %&gt;%\n  dplyr::group_by(mukey) %&gt;% \n  nest()\n\ncomp_nest_n &lt;- comp_nest %&gt;% \n  dplyr::mutate(n_comp = map_dbl(data, nrow))\n\ncomp_nest_n %&gt;% \n  dplyr::group_by(n_comp) %&gt;% \n  count(name = \"n_mapunits\")\n\n\n\n  \n\n\n\n\n# just the major components from target MUs\nmaj_comp &lt;- target_comp %&gt;% \n  filter(majcompflag == \"Yes\")\n\nnrow(maj_comp)\n\n[1] 10588"
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#taxonomy-summaries",
    "href": "03-mus_comp_in_aoi.html#taxonomy-summaries",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.4 Taxonomy Summaries",
    "text": "2.4 Taxonomy Summaries\n\n2.4.1 Order\n\ntarget_comp %&gt;% \n  dplyr::group_by(taxorder) %&gt;% \n  dplyr::summarise(n = n()) %&gt;% \n  arrange(n)\n\n\n\n  \n\n\n# what's up with taxonomic order = NA?\n# not really farmable stuff.\nmaj_comp %&gt;% \n  filter(is.na(taxorder)) %&gt;% \n  pull(compname) %&gt;% \n  unique()\n\n [1] \"Water\"                  \"Pits\"                   \"Dumps\"                 \n [4] \"Urban land\"             \"Riverwash\"              \"Highway\"               \n [7] \"Pits, gravel\"           \"Rock outcrop\"           \"Beaches\"               \n[10] \"Pits, limestone quarry\" \"Dune land\"              \"Terrace escarpments\"   \n[13] \"Alluvial\"              \n\n\n\n\n2.4.2 Suborder\n\nsuborders &lt;- target_comp %&gt;% \n  dplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(n = n()) \n\n  \nsuborders %&gt;% \n  ggplot() +\n  geom_col(aes(y = reorder(taxsuborder, -n), x = n)) +\n  ylab(\"Suborder\") + \n  xlab(\"Count\") + \n  theme_bw() +\n  ggtitle(\"Number of components by suborder\")\n\n\n\n# what's up with taxonomic suborder = NA?\n# much of this likely dropped if we use CDL to subset \ncomps_no_taxsuborder &lt;- target_comp %&gt;% \n  filter(is.na(taxsuborder)) %&gt;% \n  pull(compname) %&gt;% \n  unique()\n\nlength(comps_no_taxsuborder)\n\n[1] 829\n\nhead(comps_no_taxsuborder, n = 20)\n\n [1] \"Water\"                                  \n [2] \"Pits\"                                   \n [3] \"Dumps\"                                  \n [4] \"Cormant\"                                \n [5] \"Urban land\"                             \n [6] \"Mavie\"                                  \n [7] \"Kratka\"                                 \n [8] \"Radium\"                                 \n [9] \"Sahkahtay\"                              \n[10] \"Deerwood\"                               \n[11] \"Flaming\"                                \n[12] \"Northwood\"                              \n[13] \"Rosewood\"                               \n[14] \"Roliss\"                                 \n[15] \"Hamre\"                                  \n[16] \"Percy\"                                  \n[17] \"Berner\"                                 \n[18] \"Soils that have a mineral surface layer\"\n[19] \"Strathcona\"                             \n[20] \"Huot\"                                   \n\ntail(comps_no_taxsuborder, n = 20)\n\n [1] \"Sawmill\"                             \n [2] \"Marshan\"                             \n [3] \"Tripoli\"                             \n [4] \"Poorly drained sandy alluvial soils\" \n [5] \"Poorly and very poorly drained soils\"\n [6] \"Newalbin\"                            \n [7] \"Clrippin\"                            \n [8] \"Collinwood\"                          \n [9] \"Lakefield\"                           \n[10] \"Ocheyedan\"                           \n[11] \"Fostoria\"                            \n[12] \"Crippen\"                             \n[13] \"Farrer\"                              \n[14] \"Ocheydan\"                            \n[15] \"Grogan\"                              \n[16] \"Farrar\"                              \n[17] \"PD sandy soils\"                      \n[18] \"Moundprairie\"                        \n[19] \"PD soils\"                            \n[20] \"Walford Variant\"                     \n\n\n\n\n2.4.3 Subgroup & Family (long name)\nThe taxclname is a concatenation of the taxonomy subgroup and family. On 11 August, 2022 Nic and I talked about how this could be another way to pull out mineralogy information. Could grab “smectitic” or “mixed” from these names if we wanted to do yes/no variable.\n\ntarget_comp %&gt;% \n  pull(taxclname) %&gt;% \n  unique() %&gt;% \n  head()\n\n[1] \"Sandy over clayey, mixed over smectitic, frigid Typic Calciaquolls\"\n[2] \"Sandy over clayey, mixed over smectitic, frigid Aquic Calciudolls\" \n[3] \"Mixed, frigid Aquic Udipsamments\"                                  \n[4] \"Fine, smectitic, frigid Typic Epiaquerts\"                          \n[5] \"Sandy, mixed, frigid Typic Endoaquolls\"                            \n[6] \"Sandy, mixed, frigid Aeric Calciaquolls\""
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#variables-of-interest",
    "href": "03-mus_comp_in_aoi.html#variables-of-interest",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.5 Variables of Interest",
    "text": "2.5 Variables of Interest\nI think my next step is to use the chorizon table and pull the variables I’m interested in for each of my major components. Devine et al. 2021 did 0-30cm depth-weighted averages. I am going to do 0-20cm because I believe this is more likely to be the available depth in our validation datasets (esp. CIG).\nThen I can turn my horizon table into an SPC object, with cokey as “site”?\n\nchorizon &lt;- mn_components &lt;- sf::st_read(dsn = mn_gdb, layer = \"chorizon\")\n\nReading layer `chorizon' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\ntarget_horiz &lt;- chorizon %&gt;% \n  filter(cokey %in% maj_comp$cokey)"
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#sec-diag-feat",
    "href": "03-mus_comp_in_aoi.html#sec-diag-feat",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.6 Diagnostic Features",
    "text": "2.6 Diagnostic Features\nHere, I’m pulling out the depth to restrictive horizon (if any) in the profile. Note that this may be &gt;20cm, for our other features we are summarizing soil properties in the 0-20cm depth range.\n\n# read only diag features table, as dataframe\ndiag &lt;- sf::st_read(dsn = mn_gdb, layer = \"codiagfeatures\")\n\nReading layer `codiagfeatures' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n# check out what kind of diagnostic features we have\nunique(diag$featkind) %&gt;% head(20)\n\n [1] \"Histic epipedon\"                         \n [2] \"Ochric epipedon\"                         \n [3] \"Albic materials\"                         \n [4] \"Albic horizon\"                           \n [5] \"Argillic horizon\"                        \n [6] \"Aquic conditions\"                        \n [7] \"Lamellae\"                                \n [8] \"Sapric soil materials\"                   \n [9] \"Hemic soil materials\"                    \n[10] \"Fibric soil materials\"                   \n[11] \"Cambic horizon\"                          \n[12] \"Mollic epipedon\"                         \n[13] \"Calcic horizon\"                          \n[14] \"Abrupt textural change\"                  \n[15] \"Slickensides\"                            \n[16] \"Strongly contrasting particle size class\"\n[17] \"Lithologic discontinuity\"                \n[18] NA                                        \n[19] \"Lithic contact\"                          \n[20] \"Spodic horizon\"                          \n\ntarget_diag &lt;- diag %&gt;% \n  filter(cokey %in% maj_comp$cokey)\n\nOk, so from above we can see that 10458 cokeys out of a total of 10588 have something entered in the featkind field as a diagnostic feature. Now let’s investigate how many of those might be considered restrictive to roots (relevant in agricultural context):\ntarget_restr\n\ntarget_restr &lt;- target_diag %&gt;% \n  filter(featkind %in% c(\"Lithic contact\", \n                         \"Densic contact\",\n                         \"Paralithic contact\"))\n\n# how many cokeys have a restrictive horizon? \nlength(unique(target_restr$cokey))\n\n[1] 69\n\ntarget_restr\n\n\n\n  \n\n\n# save this in case we want to investigate further\n#it's such a small number, seems unlikely \nwrite_csv(target_restr, \"./data/restr_horiz_data_cokey.csv\")"
  },
  {
    "objectID": "04-subset-component-data.html#setup",
    "href": "04-subset-component-data.html#setup",
    "title": "3  Identify Target Components",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nFirst, load the list of MUKEYs generated in mus_comp_in_aoi.qmd\nWe will use our MUKEYs to get the relevant components. I’ve created a list column with all the components for each MUKEY in it. There are 7862 unique mapunits in my area of interest (AOI).\n\nmn_gdb &lt;- \"data/gSSURGO_MN/gSSURGO_MN.gdb\" \n\n# read only component table, as dataframe\nmn_comp &lt;- sf::st_read(dsn = mn_gdb, layer = \"component\")\n\nReading layer `component' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\ntarget_comp &lt;- mn_comp %&gt;% \n  dplyr::filter(mukey %in% mus$mukey)\n\ncomp_nest &lt;- target_comp %&gt;% \n  dplyr::group_by(mukey) %&gt;% \n  nest() %&gt;% \n  dplyr::mutate(n_comp = map_dbl(data, nrow), \n                max_comp_pct = map_dbl(data,\n                                       ~max(.x[\"comppct_r\"])),\n                min_comp_pct = map_dbl(data, ~min(.x[\"comppct_r\"])))\n\nhead(comp_nest)\n\n\n\n  \n\n\n\nAfter some troubleshooting in later steps, I came back and decided it makes the most sense to save a key of the dominant component percent in each map unit here, before anything gets dropped in the conversion to an aqp object, see Section 4.5.4 for more on this.\nThis is relevant because we will use the dominant component percentage (which component has highest comppct_r as one of our “data sufficiency” checks to determine if a given MUKEY is included in the clustering analysis. In doing some exploratory work in Section 6.9, I noticed that we had some\n\ndom_cmp_key &lt;- target_comp %&gt;% \n  dplyr::select(cokey, mukey, comppct_r) %&gt;% \n  dplyr::group_by(mukey) %&gt;% \n  dplyr::summarise(dom_comppct = max(comppct_r))\n\nwrite_csv(dom_cmp_key, \"./data/key_dominant_component_percent.csv\")  \n  \n# remove b/c this is big, we are done with it\nrm(mn_comp)"
  },
  {
    "objectID": "04-subset-component-data.html#components-per-mapunit",
    "href": "04-subset-component-data.html#components-per-mapunit",
    "title": "3  Identify Target Components",
    "section": "3.2 Components per mapunit",
    "text": "3.2 Components per mapunit\nOut of curiosity, what does the distribution look like for number of components in a mapunit?"
  },
  {
    "objectID": "04-subset-component-data.html#subset-to-components-15",
    "href": "04-subset-component-data.html#subset-to-components-15",
    "title": "3  Identify Target Components",
    "section": "3.3 Subset to components >15%",
    "text": "3.3 Subset to components &gt;15%\n\ncomp_sub &lt;- comp_nest %&gt;%\n  dplyr::mutate(data_maj15 = map(data, ~ filter(.x, comppct_r &gt;= 15))) %&gt;%\n  dplyr::select(mukey, data_maj15) %&gt;%\n  dplyr::mutate(\n    n_comp_maj = map_dbl(data_maj15, nrow),\n    max_pct = map_dbl(data_maj15,\n                           ~ max(.x[\"comppct_r\"])),\n    min_pct = map_dbl(data_maj15, ~ min(.x[\"comppct_r\"]))\n  )\n  \n\nhead(comp_sub)\n\n\n\n  \n\n\n\nNow how many major components are we working with per map unit?\n\n\n\n\n\nI’m saving this simple list of map units and number of components, max component percent for a later step where we determine if there is enough data to include that map unit in our final analysis.\n\nmajor_mu_summary &lt;- comp_sub %&gt;% \n  dplyr::select(mukey, n_comp_maj, max_pct, min_pct)\n\nwrite_csv(major_mu_summary, \"./data/mu_summary_maj_only.csv\")"
  },
  {
    "objectID": "04-subset-component-data.html#identify-unique-components",
    "href": "04-subset-component-data.html#identify-unique-components",
    "title": "3  Identify Target Components",
    "section": "3.4 Identify unique components",
    "text": "3.4 Identify unique components\n\ncomp_unnest &lt;- comp_sub %&gt;% \n  dplyr::select(data_maj15, mukey) %&gt;% \n  tidyr::unnest(cols = c(data_maj15))\n\nwrite_csv(comp_unnest, \"./data/component_list.csv\")\n\nI have 7862 unique MUKEYs and 10785 and unique COKEYs. It appears that there are no repeated COKEYs shared between mapunits."
  },
  {
    "objectID": "04-subset-component-data.html#pull-horizon-data",
    "href": "04-subset-component-data.html#pull-horizon-data",
    "title": "3  Identify Target Components",
    "section": "3.5 Pull Horizon Data",
    "text": "3.5 Pull Horizon Data\n\n# most of the soil property data we want is in the\n# chorizon table, om, pH, clay, etc.\nchoriz &lt;- sf::st_read(dsn = mn_gdb, layer = \"chorizon\")\n\nReading layer `chorizon' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\ntarget_choriz &lt;- choriz %&gt;% \n  dplyr::filter(cokey %in% comp_unnest$cokey)"
  },
  {
    "objectID": "04-subset-component-data.html#coarse-fragments",
    "href": "04-subset-component-data.html#coarse-fragments",
    "title": "3  Identify Target Components",
    "section": "3.6 Coarse Fragments",
    "text": "3.6 Coarse Fragments\n\nNeed to pull fragvol_r from the chfrags table, this is the volume percentage of horizon occupied by 2mm or larger fraction (20mm or larger for wood fragments) on a whole soil basis\nTurns out I need to aggregate this by chkey, there can be multiple fragvol_r entries for a given component-horizon.\n\n\n# some coarse frag data in the chfrag table\nchfrag &lt;- sf::st_read(dsn = mn_gdb, layer = \"chfrags\")\n\nReading layer `chfrags' from data source \n  `C:\\Users\\Hava\\Documents\\R\\ch03-sh-cluster\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n# keep only the component horizons I'm interested in\ntarget_chfrag &lt;-  chfrag %&gt;% \n  dplyr::filter(chkey %in% target_choriz$chkey)\n\n# sum volume of coarse frags in a given horizon\nfrag_hz_summary &lt;- target_chfrag %&gt;% \n  dplyr::group_by(chkey) %&gt;% \n  dplyr::summarise(fragvol_r_sum = sum(fragvol_r, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% \n  dplyr::select(chkey, fragvol_r_sum) \n\n# add coarse frag col to my df\ntarget_choriz_frag &lt;- left_join(target_choriz, frag_hz_summary, by = \"chkey\")"
  },
  {
    "objectID": "04-subset-component-data.html#save-data",
    "href": "04-subset-component-data.html#save-data",
    "title": "3  Identify Target Components",
    "section": "3.7 Save Data",
    "text": "3.7 Save Data\n\nwrite_csv(target_choriz_frag, \"data/target_choriz_all.csv\")"
  },
  {
    "objectID": "05-horiz-data.html#overview",
    "href": "05-horiz-data.html#overview",
    "title": "4  Horizon Data Averages",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nGoal is to calculate 0-20cm depth weighted averages for all the soil properties of interest, for each component.\nI think I want to turn my horizon dataframe into an SPC object (aqp package), see this demo. I think I would use cokey as “site”?\nNote for available water capacity (AWC) and soil organic carbon (SOC) these will be sums. Need to convert OM to SOC stock following equation 1 in Devine et al. (2020), but consider if I want to use a different Van Bemmelen factor, per discussion with Nic on 2022-08-11.\nSOC (kg/m^-2) = OM % / 1.72 x BD x (1-CF) x Z/10"
  },
  {
    "objectID": "05-horiz-data.html#toy-example",
    "href": "05-horiz-data.html#toy-example",
    "title": "4  Horizon Data Averages",
    "section": "4.2 Toy Example",
    "text": "4.2 Toy Example\nTrying this process with just 4 components I selected at random, to make sure I understand what’s happening with a smaller dataset.\n\n4.2.1 Select some components\n\n# randomly selected cokeys\nex_cokeys &lt;- c(21760486, 21791957, 21760347, 21782338)\n\n# filter to example horizons\n# add some info from comps table for context\ntoy_hz &lt;- horiz %&gt;% \n  dplyr::filter(cokey %in% ex_cokeys) %&gt;% \n  left_join(x = ., y = comp_info, by = \"cokey\")\n\n# what kinds of soils are we working with?\ntoy_hz %&gt;% \n  dplyr::select(cokey, compname, taxsuborder, geomdesc, drainagecl) %&gt;% \n  unique()\n\n\n\n  \n\n\n\n\n\n4.2.2 Promote to SPC Object\nBefore we promote to SPC object, need to make sure all my horizons are pre-sorted by profile id cokey, and then by horizon top boundary hzdept_r. Following along with “Object Creation” in the Introduction to SPC Objects docs.\n\ntoy_sort &lt;- toy_hz %&gt;% \n  arrange(cokey, hzdept_r) \n\n# take a look\ntoy_sort %&gt;% \n  select(compname, hzname, hzdept_r)\n\n\n\n  \n\n\n\n\n# upgrade to SPC\ndepths(toy_sort) &lt;-  cokey ~ hzdept_r + hzdepb_r\n\n# specify horizon name col\nhzdesgnname(toy_sort) &lt;- 'hzname'\n\n# confirm it worked\nclass(toy_sort)\n\n[1] \"SoilProfileCollection\"\nattr(,\"package\")\n[1] \"aqp\"\n\n# check out the object\nprint(toy_sort)\n\nSoilProfileCollection with 4 profiles and 14 horizons\nprofile ID: cokey  |  horizon ID: hzID \nDepth range: 152 - 200 cm\n\n----- Horizons (6 / 14 rows  |  10 / 177 columns) -----\n# A tibble: 6 x 10\n  hzname    cokey hzID  hzdept_r hzdep~1 desgn~2 desgn~3 desgn~4 desgn~5 hzdep~6\n  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Ap     21760347 1            0      23      NA A       &lt;NA&gt;         NA       0\n2 A      21760347 2           23      41      NA A       &lt;NA&gt;         NA      18\n3 AB     21760347 3           41      51      NA AB      &lt;NA&gt;         NA      36\n4 Bkg    21760347 4           51      91      NA B       &lt;NA&gt;         NA      46\n5 Cg     21760347 5           91     200      NA C       &lt;NA&gt;         NA      74\n6 Ap     21760486 6            0      18      NA A       &lt;NA&gt;         NA       0\n# ... with abbreviated variable names 1: hzdepb_r, 2: desgndisc,\n#   3: desgnmaster, 4: desgnmasterprime, 5: desgnvert, 6: hzdept_l\n[... more horizons ...]\n\n----- Sites (4 / 4 rows  |  1 / 1 columns) -----\n# A tibble: 4 x 1\n  cokey   \n  &lt;chr&gt;   \n1 21760347\n2 21760486\n3 21782338\n4 21791957\n\nSpatial Data:\n[EMPTY]\n\n\n\n\n4.2.3 Aggregate Along Slabs\nWe want to work with just the 0-20cm depth, can use slab() to do this. Example in “Aggregating Soil Profile Collections Along Regular Slabs”\nHere I’m grouping by cokey (individual profiles) so that we get a depth-wise summary for each cokey, weighted by horizon.\nUsing slab.fun = mean, I double checked and this computes a depth-weighted mean. This is clear if you examine both the horizon data and the “slab” data below for the component 21760486. The top horizon (0-18cm) has OM 2.5%, and the second horizon (18-28cm) has OM 0.25. The depth weighted average (to 20cm depth) is 2.275\nI’m pretty sure I want to include na.rm = TRUE here for slab? Since I’m doing this for each profile, it seems unlikely that I’d have data for just one horizon, but not the next horizon. I suppose I could check this for our components\n\nslab_ex &lt;- slab(object = toy_sort, \n                fm = cokey ~ om_r + claytotal_r + cec7_r,\n                slab.structure = c(0,20),\n                slab.fun = mean,\n                na.rm = TRUE) \n\n# our example component, 0-20cm depth spans 2 horizons here\ntoy_hz %&gt;% \n  select(cokey, hzdept_r, hzdepb_r, om_r, claytotal_r) %&gt;% \n  arrange(cokey, hzdept_r) %&gt;% \n  filter(cokey == 21760486)\n\n\n\n  \n\n\n# slab results, note they are \"long\" format\n# om_r = 2.275\nslab_ex %&gt;% \n  filter(cokey == 21760486)\n\n\n\n  \n\n\n# do the weighted mean for om_r by hand \n# should match slab results above for cokey 21760486 \nweighted.mean(x = c(2.5, 0.25), w = c(18,2)) \n\n[1] 2.275"
  },
  {
    "objectID": "05-horiz-data.html#variables",
    "href": "05-horiz-data.html#variables",
    "title": "4  Horizon Data Averages",
    "section": "4.3 Variables",
    "text": "4.3 Variables\n\n4.3.1 List to include\nDevine et al. 2020 included SAR and gypsum, these seem less relevant to us (and in fact might not be populated as much over here, since there are relatively few spots on the western side of the state where this might be relevant?)\n\nvars_of_interest &lt;-\n  c(\n    'claytotal_r',\n    'silttotal_r',\n    'sandtotal_r',\n    'claysizedcarb_r', # not in Devine, I added \n    'om_r',\n    'cec7_r',\n    'dbthirdbar_r',\n    'fragvol_r_sum',\n    'kwfact', # erodibility factor, learn more?\n    'ec_r',\n    'ph1to1h2o_r',\n    'sar_r',\n    'caco3_r',\n    'lep_r',\n    'ksat_r',\n    'awc_r'\n    # 'freeiron_r', # not in Devine, also no data\n    # 'feoxalate_r' # not in Devine, also no data \n  )\n\n\n\n4.3.2 Number Missing\nHow many horizons are missing data for our variables of interest?"
  },
  {
    "objectID": "05-horiz-data.html#implicit-zeroes",
    "href": "05-horiz-data.html#implicit-zeroes",
    "title": "4  Horizon Data Averages",
    "section": "4.4 Implicit Zeroes",
    "text": "4.4 Implicit Zeroes\nNic suggested that I check on the cokeys where fragvol_r_sum is NA, it’s possible that these are actually implied zeroes. The main place we’ll use this info is for calculating SOC stocks.\n\nfrag_check &lt;- horiz %&gt;% \n  select(chkey,\n         cokey,\n         fragvol_r_sum) %&gt;% \n  left_join(comp_info, by = c('cokey')) %&gt;% \n  filter(is.na(fragvol_r_sum))\n\nSo out of 37552 chkeys, the sum of coarse fragment volume (fragvol_r_sum) is NA for 7414 of them.\n\n4.4.1 Geomorphic Description\nHere I’m trying to pull out big groups of similar soils based on geomorphic description, which is helpful as a starting point for whether we would expect there to be coarse fragments.\nLook into using the fetchOSD function http://ncss-tech.github.io/AQP/soilDB/soil-series-query-functions.html\nSoil Survey Manual online, helpful info about how this info is recorded: https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/ref/?c id=nrcs142p2_054252\n\n# how many are on lake plains? unlikely to have \n# many coarse frags here \nlake &lt;- frag_check %&gt;% \n  filter(str_detect(geomdesc, \"lake plain\"))\n\n# how many cokeys?\nnrow(lake)\n\n[1] 2818\n\n# a few examples\nhead(unique(lake$geomdesc))\n\n[1] \"depressions on lake plains\"                            \n[2] \"flats on lake plains\"                                  \n[3] \"till-floored lake plains on lake plains\"               \n[4] \"rises on lake plains\"                                  \n[5] \"depressions on till-floored lake plains on lake plains\"\n[6] \"lake plains\"                                           \n\nlake %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAlbolls\nGalchutt, Barbert\n\n\nAqualfs\nSpooner, Brickton\n\n\nAquents\nCormant, Grygla, Lallie, Aquents, Urness\n\n\nAquepts\nHaug, Deerwood, Wildwood, Northwood, Leafriver, Sago, Sax, Hamre\n\n\nAquerts\nFargo, Viking, Grano, Northcote, Hegne, Eaglepoint, Reis, Clearwater, Ludden, Beauford\n\n\nAquolls\nThiefriver, Garborg, Colvin, Rosewood, Ulen, Augsburg, Borup, Wheatville, Glyndon, Bearden, Noyes, Lamoure, Espelie, Wabanica, Woodslake, Boash, Mustinka, Hamar, Grimstad, Nielsville, Venlo, Arveson, Lindaas, Perella, Aquolls, Gunclub, Vallers, Antler, Flom, Kratka, Syrene, Wyndmere, Elmville, Winger, Parnell, Kittson, Bigstone, Quam, Spicer, Marna, Brownton, Waldorf, Madelia, Lura, Leen, Chetomba, Prinsburg, Nishna, Baroda\n\n\nHemists\nTacoosh, Mooselake, Uskabwanka, Rifle\n\n\nOrthents\nUdorthents, Orthents, Bold\n\n\nPsamments\nCorliss, Poppleton, Zimmerman, Barber, Guida\n\n\nSaprists\nMarkey, Cathro, Berner, Dora, Bullwinkle, Tawas, Seelyeville, Rondeau, Histosols, Haslie, Klossner\n\n\nUdalfs\nSkime, Moranville, Dalbo, Baudette, Debs, Rosy, Kilkenny, Shorewood\n\n\nUderts\nSinai, Hattie\n\n\nUdolls\nHecla, Huot, Foldahl, Croke, Hilaire, Glyndon, Zell, LaDelle, Flaming, Eckman, Overly, Bygland, Lizzie, Swenoda, Kittson, Wheatville, McIntosh, Wolverton, Doran, Towner, Lohnes, Gardena, Egeland, Byrne, Tara, Rondell, Shorewood, Truman, Gardencity, Collinwood, Kingston, Seaforth, Waubay, Rusklyn, Poinsett, Barrington, Ocheyedan, Lakefield, Good Thunder, Corwith, Grogan\n\n\n\n\n\n\n\n\n# how many are on flood plains? \nflood &lt;- frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(str_detect(geomdesc, \"flood plain\"))\n\n# how many cokeys?\nnrow(flood)\n\n[1] 684\n\n# a few examples\nhead(unique(flood$geomdesc))\n\n[1] \"natural levees on flood plains\"  \"flood plains\"                   \n[3] \"terraces on flood plains\"        \"flood plains on river valleys\"  \n[5] \"flood plains on alluvial plains\" \"flats on flood plains\"          \n\nflood  %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAquents\nLallie, Totagatic, Blue Earth, Chaska, Kalmarville\n\n\nAquerts\nLudden\n\n\nAquolls\nLamoure, Rauville, Cohoctah, Sedgeville, Suckercreek, Calco, Havelock, Nishna, Cedarrock, Faxon, Colo, Comfrey, Otter, Millington, Alluvial land, Southbrook, Riverston, Coland, Sawmill\n\n\nFluvents\nCashel, Fairdale, Alluvial land, Dorchester, McPaul, Dunnbot, Rawles, Dockery\n\n\nHemists\nLougee, Boots\n\n\nPsamments\nAlgansee, Scotah\n\n\nSaprists\nBowstring, Haslie, Seelyeville, Nidaros, Cathro, Muskego, Klossner, Houghton, Rondeau\n\n\nUderts\nWahpeton, Sinai\n\n\nUdolls\nLa Prairie, Hanlon, Lawson, Littleton, Kennebec, Ceresco, Alluvial land\n\n\nNA\nMuck and peat\n\n\n\n\n\n\n\n\n# how many are on sandhills? \nsandhill &lt;- frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(str_detect(geomdesc, \"sandhill\"))\n\n# how many cokeys?\nnrow(sandhill)\n\n[1] 20\n\n# a few examples\nhead(unique(sandhill$geomdesc))\n\n[1] \"sand sheets on sandhills\" \"dunes on sandhills\"      \n[3] \"depressions on sandhills\"\n\nsandhill %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAquents\nBantry\n\n\nHemists\nRifle\n\n\nPsamments\nSerden, Aylmer\n\n\n\n\n\n\n\n\n# how many are organic soils? \nhist &lt;- frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) \n\n# how many cokeys?\nnrow(hist)\n\n[1] 803\n\n# some examples\nhead(unique(hist$geomdesc))\n\n[1] \"-- Error in Exists On --\"                              \n[2] \"depressions on outwash plains\"                         \n[3] \"fens on beach ridges\"                                  \n[4] \"depressions on till plains\"                            \n[5] \"seeps on till plains\"                                  \n[6] \"depressions on moraines, depressions on outwash plains\"\n\nhist %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nFibrists\nBrophy\n\n\nHemists\nRifle, Mooselake, Greenwood, Tacoosh, Uskabwanka, Lougee, Carlos, Millerville, Caron\n\n\nSaprists\nSeelyeville, Markey, Lupton, Berner, Haslie, Cathro, Bullwinkle, Rondeau, Nidaros, Loxley, Histosols, Bowstring, Muck and peat, Muck, Muskego, Houghton, Klossner, Haplosaprists, Palms, Lena, Medo, Marsh, Muck soil\n\n\n\n\n\n\n\n\n# outwash plains?\noutwash &lt;- frag_check %&gt;% \n   filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(str_detect(geomdesc, \"outwash plain\"))\n\n# how many cokeys?\nnrow(outwash)\n\n[1] 470\n\n# some examples \nhead(unique(outwash$geomdesc))\n\n[1] \"depressions on outwash plains\"                       \n[2] \"lakeshores on outwash plains\"                        \n[3] \"flats on outwash plains\"                             \n[4] \"hillslopes on moraines, hillslopes on outwash plains\"\n[5] \"outwash plains\"                                      \n[6] \"hillslopes on outwash plains\"                        \n\noutwash %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAqualfs\nEpoufette\n\n\nAquents\nUrness\n\n\nAquepts\nLeafriver, Madaus, Minocqua, Wabuse\n\n\nAquolls\nIsanti, Warman, Colvin, Hamar, Rockwell, Fossum, Shakopee, Fieldon, Granby, Darfur, Trosky, Talcot\n\n\nBoralfs\nSoderville\n\n\nPsamments\nPsamments, Sartell, Zimmerman, Cantlin, Chelsea\n\n\nUdalfs\nAnoka, Dalbo, Soderville, Menomin\n\n\nUdolls\nEstelline, Maddock, Malachy, Flandreau, Litchfield, Langola, Kost, Glendorado, Athelwold, Embden, Torning, Dickinson, Gardencity, Tomall, Grogan, Kennebec, Waukegan, Strayhoss, Everts, Brandt, Sandberg, Allendorf, Lasa\n\n\nUstolls\nDempster, Flandreau, Graceville\n\n\n\n\n\n\n\n\n# stream terraces? \nterrace &lt;- frag_check %&gt;% \n   filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(!str_detect(geomdesc, \"outwash plain\")) %&gt;% \n  filter(str_detect(geomdesc, \"stream terrace\")) \n\n# how many cokeys?\nnrow(terrace)\n\n[1] 135\n\n# some examples\nhead(unique(terrace$geomdesc))\n\n[1] \"natural levees on alluvial plains, stream terraces on alluvial plains\"\n[2] \"stream terraces on river valleys\"                                     \n[3] \"stream terraces\"                                                      \n[4] \"hills on stream terraces\"                                             \n[5] \"escarpments on stream terraces\"                                       \n[6] \"swales, depressions on stream terraces\"                               \n\nterrace %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAquolls\nTalcot, Duelm, Faxon, Isanti, Tilfer, Joliet\n\n\nOrthents\nUdorthents\n\n\nPsamments\nSartell\n\n\nUdalfs\nSoderville, Anoka, Meridian, Eleva\n\n\nUderts\nWahpeton\n\n\nUdolls\nLaDelle, Hubbard, Langola, Dorset, Richwood, Copaston, Lasa, Dickinson, Ridgeport, Grogan, Allendorf\n\n\n\n\n\n\n\n\n# loess hills? \nloess &lt;- frag_check %&gt;%\n    filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(!str_detect(geomdesc, \"outwash plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"stream terrace\")) %&gt;% \n  filter(str_detect(geomdesc, \"loess\"))\n\nnrow(loess)  \n\n[1] 260\n\n# some examples\nhead(unique(loess$geomdesc))\n\n[1] \"loess hills\"                  \"knolls on loess hills\"       \n[3] \"valley sides on loess bluffs\" \"valley sides on loess hills\" \n[5] \"loess bluffs\"                \n\nloess %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nPsamments\nBoone\n\n\nUdalfs\nWhalan, Oak Center, Gale, Eleva, Hersey, Fayette, Downs, Massbach, Brinkman, Frankville, Nasset, Mt. Carroll\n\n\nUdolls\nJoy, Dinsmore, Tama, Shullsburg, Schapville, Port Byron, Elizabeth\n\n\n\n\n\n\n\n\n# drainageways? \ndrain &lt;- frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(!str_detect(geomdesc, \"outwash plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"stream terrace\")) %&gt;% \n  filter(!str_detect(geomdesc, \"loess\")) %&gt;% \n  filter(str_detect(geomdesc, \"drain\"))\n\n# how many cokeys?\nnrow(drain)\n\n[1] 131\n\n# some examples \nhead(unique(drain$geomdesc)) \n\n[1] \"drainageways on river valleys\"              \n[2] \"drainageways on moraines, flats on moraines\"\n[3] \"drainageways on till plains\"                \n[4] \"drainageways on terraces\"                   \n[5] \"flats on moraines, drainageways on moraines\"\n[6] \"drainageways\"                               \n\ndrain %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAqualfs\nBlomford\n\n\nAquents\nFluvaquents\n\n\nAquolls\nFaxon, Fulda, Parnell, Maxfield, Colo, Ossian, Otter, Badger, Hidewood, Little Cottonwood\n\n\nUdolls\nLaDelle, Zell, Joy, Barremills\n\n\n\n\n\n\n\n\nother &lt;-  frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(!str_detect(geomdesc, \"outwash plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"stream terrace\")) %&gt;% \n  filter(!str_detect(geomdesc, \"loess\")) %&gt;% \n  filter(!str_detect(geomdesc, \"drain\"))\n\n# other terrace related \nother %&gt;% filter(str_detect(geomdesc, \"terrace\")) %&gt;% pull(geomdesc) %&gt;% unique()\n\n [1] \"terraces\"                               \n [2] \"terraces, hills\"                        \n [3] \"hills, terraces\"                        \n [4] \"escarpments on terraces, hills\"         \n [5] \"escarpments on terraces\"                \n [6] \"outwash terraces\"                       \n [7] \"strath terraces\"                        \n [8] \"terraces on river valleys\"              \n [9] \"outwash terraces on till plains\"        \n[10] \"hills on terraces\"                      \n[11] \"escarpments, terraces\"                  \n[12] \"swales on outwash terraces, till plains\"\n[13] \"terraces on uplands\"                    \n\n# drumlin or moraine related\nother %&gt;% filter(str_detect(geomdesc, \"drumlin\")|\n                   str_detect(geomdesc, \"moraine\")) %&gt;% pull(geomdesc) %&gt;% unique()\n\n [1] \"flats on moraines, rises on moraines\"               \n [2] \"rises on moraines\"                                  \n [3] \"depressions on moraines\"                            \n [4] \"hillslopes on moraines, rises on moraines\"          \n [5] \"moraines on till plains\"                            \n [6] \"moraines\"                                           \n [7] \"flats on moraines\"                                  \n [8] \"swales on moraines\"                                 \n [9] \"hillslopes on moraines\"                             \n[10] \"flats on moraines, swales on moraines\"              \n[11] \"hillslopes on glacial lakes on moraines\"            \n[12] \"depressions on interdrumlins\"                       \n[13] \"hillslopes on drumlins\"                             \n[14] \"glacial lakes on hillslopes on moraines\"            \n[15] \"rims on depressions on moraines, flats on moraines\" \n[16] \"rises on moraines, flats on moraines\"               \n[17] \"swales on moraines, flats on moraines\"              \n[18] \"drumlins\"                                           \n[19] \"flats, moraines\"                                    \n[20] \"hills on moraines\"                                  \n[21] \"beaches on moraines\"                                \n[22] \"shores on beaches on moraines\"                      \n[23] \"marshes on moraines\"                                \n[24] \"rises on ground moraines\"                           \n[25] \"ground moraines\"                                    \n[26] \"flats on ground moraines, swales on ground moraines\"\n[27] \"ground moraines on till plains\"                     \n\n# hill related (some of these don't really make sense to me)\nother %&gt;% filter(str_detect(geomdesc, \"hill\")) %&gt;% pull(geomdesc) %&gt;% unique()\n\n [1] \"hillslopes on till plains, ridges on till plains\"\n [2] \"hillslopes on moraines, rises on moraines\"       \n [3] \"hillslopes on moraines\"                          \n [4] \"hillslopes on glacial lakes on moraines\"         \n [5] \"hillslopes on till plains\"                       \n [6] \"hillslopes on drumlins\"                          \n [7] \"glacial lakes on hillslopes on moraines\"         \n [8] \"hills on moraines\"                               \n [9] \"hills on till plains\"                            \n[10] \"terraces, hills\"                                 \n[11] \"hills, terraces\"                                 \n[12] \"escarpments on terraces, hills\"                  \n[13] \"hills\"                                           \n[14] \"hillslopes on uplands\"                           \n[15] \"hills on terraces\"                               \n[16] \"hills, valley sides\"                             \n[17] \"hills on outwash deltas\"                         \n[18] \"hillslopes on uplands, hillslopes on uplands\"    \n[19] \"hillslopes on interfluves on uplands\"            \n[20] \"swales on hillslopes on uplands\"                 \n\n# note that 'other' now contains only 208 unique compnames\nlength(unique(other$compname))\n\n[1] 208\n\nother %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAlbolls\nBarbert\n\n\nAqualfs\nWillosippi, Zwingle\n\n\nAquents\nUrness, Blue Earth, Tadkee, Aquents, Beaches, Beach\n\n\nAquepts\nHamre, Haug, Twig, Giese\n\n\nAquerts\nHegne\n\n\nAquolls\nFram, Roliss, Borup, Grimstad, Lindaas, Dovray, Colvin, Quam, Winger, Bearden, Glencoe, Darfur, Parnell, Bigstone, Lura, Okoboji, Granby, Glyndon, Perella, Spicer, Southam, Fulda, Calcousta, Maxcreek, Garwin, Maxfield, Tilfer, Soils that have more clay, Romnell, Trosky, Knoke, Faxon, Joliet, Fieldon, McIntosh, Rushmore, Whitewood, Mound Creek, Mayer, Prinsburg, Chetomba, Biscay, Haverhill, Marcus\n\n\nFluvents\nCashel, Arenzville, Dorchester\n\n\nOrthents\nUdorthents, Bold\n\n\nPsamments\nMahtomedi, Boone, Chelsea\n\n\nUdalfs\nWarba, Dalbo, Brainerd, Jewett, Dorerton, Blooming, Mt. Carroll, Winneshiek, Seaton, Hersey, Downs, Eleva, Oak Center, Meridian, Waucoma, Nasset, Waubeek, Massbach, Frankville, Gale, Dubuque, Fayette, Hixton, Medary, Festina, Plumcreek, Whalan, Renova, Newry, Backbone, Spinks, Churchtown, NewGlarus, Southridge, Palsgrove, Norden, Dunbarton, Donnan, Pepin, Nordness, Lamont, La Farge\n\n\nUdepts\nTimula\n\n\nUderts\nSinai, Nutley\n\n\nUdolls\nTowner, Maddock, Lohnes, Bygland, Dickey, Tara, Langola, Kost, Copaston, Doran, Byrne, Yellowbank, Doland, Embden, Waubay, Poinsett, Swenoda, Brodale, Ripon, Sparta, Torning, Fordville, Fedji, Tomall, Highpoint Lake, Darnen, Bechyn, Crooksford, Louris, Port Byron, Rockton, Lindstrom, Tallula, Marlean, Klinger, Joy, Kennebec, Merton, Bellechester, Grogan, Wadena, Dickinson, Channahon, Tama, Atkinson, Wagen Prairie, Wangs, Hesch, Richwood, Moland, Schapville, Rasset, Brookings, Vienna, Lismore, Kranzburg, Singsaas, Oak Lake, Lake Benton, Round Lake, Ridgeport, Dodgeville, Etter, Lasa, Litchfield, Sac, Everly, Ransom, Athelwold, Judson, Overly, Germantown, Augusta Lake, Okabena, Ocheyedan, Pilot Grove, Sylvester, Emeline, Hoopeston, Lawler, Flagler, Keltner, Waukee, Wilmonton, Ocheda, Lakefield, Fostoria, Farrar, Edmund\n\n\nUstolls\nIhlen, Alcester, Moody, Splitrock, Trent, Sogn, Bluemound\n\n\nNA\nRock outcrop"
  },
  {
    "objectID": "05-horiz-data.html#full-dataset-spc-workflow",
    "href": "05-horiz-data.html#full-dataset-spc-workflow",
    "title": "4  Horizon Data Averages",
    "section": "4.5 Full dataset SPC workflow",
    "text": "4.5 Full dataset SPC workflow\n\n4.5.1 Create SPC object\nHere I turn the entire horiz dataframe into an SPC object.\n\n# sort so all horizons are in order for each cokey\nhz_sort &lt;- horiz %&gt;% \n  arrange(cokey, hzdept_r) \n\n# upgrade to SPC\ndepths(hz_sort) &lt;-  cokey ~ hzdept_r + hzdepb_r\n\n# check it out \n# note only 10237 profiles, started with 10785 cokeys\n# figure out missing ones below\nhz_sort\n\nSoilProfileCollection with 10237 profiles and 37552 horizons\nprofile ID: cokey  |  horizon ID: hzID \nDepth range: 28 - 208 cm\n\n----- Horizons (6 / 37552 rows  |  10 / 173 columns) -----\n# A tibble: 6 x 10\n     cokey hzID  hzdept_r hzdep~1 hzname desgn~2 desgn~3 desgn~4 desgn~5 hzdep~6\n     &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 21694774 1            0      20 Ap          NA A       &lt;NA&gt;         NA       0\n2 21694774 2           20      56 Bw          NA B       &lt;NA&gt;         NA      13\n3 21694774 3           56     114 E           NA E       &lt;NA&gt;         NA      41\n4 21694774 4          114     116 Bt          NA B       &lt;NA&gt;         NA      61\n5 21694774 5          116     203 E and~      NA E and B &lt;NA&gt;         NA      63\n6 21694776 6            0      20 Ap          NA A       &lt;NA&gt;         NA       0\n# ... with abbreviated variable names 1: hzdepb_r, 2: desgndisc,\n#   3: desgnmaster, 4: desgnmasterprime, 5: desgnvert, 6: hzdept_l\n[... more horizons ...]\n\n----- Sites (6 / 10237 rows  |  1 / 1 columns) -----\n# A tibble: 6 x 1\n  cokey   \n  &lt;chr&gt;   \n1 21694774\n2 21694776\n3 21694781\n4 21694789\n5 21694790\n6 21694791\n[... more sites ...]\n\nSpatial Data:\n[EMPTY]\n\n# specify horizon name col\nhzdesgnname(hz_sort) &lt;- 'hzname'\n\n\n\n4.5.2 Custom slab function\nI will use this function along with the map function from {purrr} to calculate weighted means of the soil properties on our list.\n\nslab_fun &lt;- function(var, spc_obj){\n  \n  slab_formula &lt;- as.formula(paste(\"cokey ~ \", var))\n  \n  slab_df &lt;- slab(object = spc_obj, \n                fm = slab_formula,\n                slab.structure = c(0,20),\n                slab.fun = mean,\n                na.rm = TRUE) \n  \n  return(slab_df)\n  \n}\n\n\n\n4.5.3 Apply (map) slab function\nDebugging victory here! I was having trouble with my custom function (defined above) working with any of the variants of purrr::map() . Found the answer in the map2() documentation (but it seems to work here, even though I’m using just map() ). Under the Details section: “Note that arguments to be vectorised over come before .f , and arguments that are supplied to every call come after .f”\nSo in my case, the same spc object is used for all of these function calls; I needed to put that argument after .f .\n\n# # dataframe for results to land in\nr &lt;- data.frame(var_name = vars_of_interest)\n\n# map over my list of vars, using custom slab function\n# takes a minute or so\nrnest &lt;- r %&gt;% \n  dplyr::mutate(aggr_data = map(.x = var_name,\n                          .f = slab_fun, \n                          spc_obj = hz_sort))\n\n# result is a list col with soil prop data nested \nhead(rnest)\n\n\n\n  \n\n\n\n\n# unnest, save long version b/c nice for \n# facetted plots\nrlong &lt;- rnest %&gt;% \n  unnest(cols = c(aggr_data)) %&gt;% \n  select(-var_name)   # don't need, col also returned by slab\n\n  \n# save wider version b/c better for modeling\n# this matches the number of elements in our SPC (10237)\n# not sure why the difference between nrow of comps and n\n# elements in our SPC... look into this. \nrwide &lt;- rlong %&gt;% \n  pivot_wider(names_from = \"variable\",\n              values_from = c(\"value\", \"contributing_fraction\"),\n              names_glue = \"{variable}_{.value}\")\n\n\n\n4.5.4 Missing/Dropped Components?\nI started with 10785 that were identified in 04-subset-component-data.qmd. However, when I promoted this list to an SPC object in aqp we got only 10237 . Check out what’s missing.\n\ncomps_start &lt;- comps$cokey\n\ncomps_end &lt;- rwide$cokey\n\nmissing_cokeys &lt;- setdiff(comps_start, comps_end)\n\nmissing_comps &lt;- comps %&gt;% \n  filter(cokey %in% missing_cokeys) %&gt;% \n  select(compname,\n         compkind,\n         majcompflag,\n         taxclname,\n         taxorder,\n         taxsuborder,\n         taxgrtgroup,\n         taxsubgrp,\n         mukey)\n\nHere’s more info about the missing components, by component kind:\n\n\n\n\n\nCheck out component names, grouped by component kind.\n\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]"
  },
  {
    "objectID": "05-horiz-data.html#save-data",
    "href": "05-horiz-data.html#save-data",
    "title": "4  Horizon Data Averages",
    "section": "4.6 Save Data",
    "text": "4.6 Save Data\nBased on the summaries of missing/dropped components above, I think this is all stuff we would have wanted to exclude anyway (and data availability is likely low…). Saving long and wide versions of the 20cm slab-aggregated dataset for further analysis.\n\ndatestamp &lt;- lubridate::today() %&gt;% str_replace_all(\"-\", \"\")\n\nwrite_csv(rlong, glue(\"./data/long_slab_aggregated_soil_props_{datestamp}.csv\"))\n\nwrite_csv(rwide, glue(\"./data/wide_slab_aggregated_soil_props_{datestamp}.csv\"))"
  },
  {
    "objectID": "06-soil-prop-eda.html#soil-texture-classes",
    "href": "06-soil-prop-eda.html#soil-texture-classes",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.1 Soil Texture Classes",
    "text": "5.1 Soil Texture Classes"
  },
  {
    "objectID": "06-soil-prop-eda.html#clay-sand-and-silt",
    "href": "06-soil-prop-eda.html#clay-sand-and-silt",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.2 Clay, Sand, and Silt",
    "text": "5.2 Clay, Sand, and Silt\n\nplot_facet_hist(c(\"claytotal_r\", \"sandtotal_r\", \"silttotal_r\")) +\n  xlab(\"Weight %\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#carbonates",
    "href": "06-soil-prop-eda.html#carbonates",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.3 Carbonates",
    "text": "5.3 Carbonates\n\nplot_facet_hist(c(\"claysizedcarb_r\", \"caco3_r\")) +\n  xlab(\"Weight Percent\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#sec-ph-ec",
    "href": "06-soil-prop-eda.html#sec-ph-ec",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.4 pH and EC",
    "text": "5.4 pH and EC\n\nplot_facet_hist(c(\"ph1to1h2o_r\", \n                  \"ec_r\"), \n                nbins = 20) + \n  xlab(\"dS/m  ~~~~  unitless\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#organic-matter",
    "href": "06-soil-prop-eda.html#organic-matter",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.5 Organic Matter",
    "text": "5.5 Organic Matter\n\nplot_facet_hist(c(\"om_r\")) +\n  xlab(\"% LOI\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#bulk-density",
    "href": "06-soil-prop-eda.html#bulk-density",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.6 Bulk Density",
    "text": "5.6 Bulk Density\n\nplot_facet_hist(c(\"dbthirdbar_r\")) +\n  xlab(\"g/cm3\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#cec-ph-7",
    "href": "06-soil-prop-eda.html#cec-ph-7",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.7 CEC pH 7",
    "text": "5.7 CEC pH 7\n\nplot_facet_hist(c(\"cec7_r\")) +\n  xlab(\"meq/100g\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#ksat-and-awc",
    "href": "06-soil-prop-eda.html#ksat-and-awc",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.8 Ksat and AWC",
    "text": "5.8 Ksat and AWC\nIn Devine et al., they calculated AWC as a sum by soil component (so they multiplied the weighted average AWC by the depth to get a volume of water). Nic and I talked about this, and decided it’s not necessary. We can just use the AWC value from the database (the volume fraction), because we are always using the same depth (20cm). So multiplying by a constant wouldn’t change the distribution of values at all.\nAWC column description: The amount of water that an increment of soil depth, inclusive of fragments, can store that is available to plants. AWC is expressed as a volume fraction, and is commonly estimated as the difference between the water contents at 1/10 or 1/3 bar (field capacity) and 15 bars (permanent wilting point) tension and adjusted for salinity, and fragments.\n\nplot_facet_hist(c(\"ksat_r\", \n                  \"awc_r\")) + \n  xlab(\"um/s  ~~~~  cm/cm\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#linear-extensibility-and-sar",
    "href": "06-soil-prop-eda.html#linear-extensibility-and-sar",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.9 Linear Extensibility and SAR",
    "text": "5.9 Linear Extensibility and SAR\n\nplot_facet_hist(c(\"lep_r\", \n                  \"sar_r\")) + \n  xlab(\"%  ~~~~  unitless\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#volume-coarse-fragments",
    "href": "06-soil-prop-eda.html#volume-coarse-fragments",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.10 Volume Coarse Fragments",
    "text": "5.10 Volume Coarse Fragments\n\nplot_facet_hist(c(\"fragvol_r_sum\")) +\n  xlab(\"Volume %\")"
  },
  {
    "objectID": "06-soil-prop-eda.html#erodibility-factor",
    "href": "06-soil-prop-eda.html#erodibility-factor",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.11 Erodibility Factor",
    "text": "5.11 Erodibility Factor\n\nplot_facet_hist(c(\"kwfact\"),\n                nbins = 20) +\n  xlab(\"Unitless\")"
  },
  {
    "objectID": "07-map-unit-agg.html#clustering-variables",
    "href": "07-map-unit-agg.html#clustering-variables",
    "title": "6  Map unit aggregation",
    "section": "6.1 Clustering Variables",
    "text": "6.1 Clustering Variables\nUsing the same variables they did in Devine et al., but added calcium carbonate (caco3_r_value)\n\nclust_vars &lt;- c(\n  \"claytotal_r_value\",\n  # \"silttotal_r_value\",\n  # \"sandtotal_r_value\",\n  \"om_r_value\",\n  \"cec7_r_value\",\n  \"dbthirdbar_r_value\",\n  #\"fragvol_r_sum_value\",\n  #\"kwfact_value\",  \n  \"ec_r_value\",\n  \"ph1to1h2o_r_value\",\n  \"caco3_r_value\",\n  \"lep_r_value\",\n  \"ksat_r_value\",\n  \"awc_r_value\"\n)"
  },
  {
    "objectID": "07-map-unit-agg.html#drop-incomplete-cases",
    "href": "07-map-unit-agg.html#drop-incomplete-cases",
    "title": "6  Map unit aggregation",
    "section": "6.2 Drop incomplete cases",
    "text": "6.2 Drop incomplete cases\nI think we want to keep only complete cases for our COKEYs (all of the variables we want to cluster on should have a value). Otherwise that would mess with our clustering. Here I’m checking on how many NAs there are for the different variables, then I drop the incomplete cases.\n\ncmp %&gt;% \n  select(cokey, all_of(clust_vars)) %&gt;% \n  summarise(across(where(is.numeric), ~sum(is.na(.x)))) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"variable\",\n               values_to = \"n_missing\") %&gt;% \n  arrange(desc(n_missing))\n\n\n\n  \n\n\n\nAnd keep only the complete cases:\n\n# keep only cokey and clustering vars\ncmp_clust &lt;- cmp %&gt;% \n  select(cokey, all_of(clust_vars))\n\n# filter to include only complete cases \n# there are many ways to do this\n# another way would be filter(complete.cases(.))\ncmp_complete &lt;- cmp_clust %&gt;% drop_na()\n\nSo we went from 10237 to 9387 COKEYs. (We dropped 850)."
  },
  {
    "objectID": "07-map-unit-agg.html#create-table-for-map-unit-data",
    "href": "07-map-unit-agg.html#create-table-for-map-unit-data",
    "title": "6  Map unit aggregation",
    "section": "6.3 Create table for map unit data",
    "text": "6.3 Create table for map unit data\nHere, I take the (complete only) component data (0-20cm weighted averages for all our soil properties) and join it to my lookup table, which includes MUKEY. Then I nest COKEY data (multiple COKEYs can belong to one MUKEY). The result is a table with one row for each MUKEY.\nI also pull out some info about the number, min %, and max % of components in a given MUKEY for context.\nThe first time I did this, I forgot to deal with NAs in the cmp dataframe, and we had 7525 MUKEYs to work with. After filtering to include only complete cases based on the cluster variables defined above, we now have 7062 MUKEYs to work with. For reference, Devine et al. had 4,595 for inclusion in their model (after an additional drop step based on sufficient data representation, see below Section 6.5) .\n\n# nest by mukey\ncmp_nest &lt;- left_join(cmp_complete, cmp_lookup, by = c(\"cokey\")) %&gt;%\n  dplyr::select(cokey, mukey, everything()) %&gt;%\n  dplyr::group_by(mukey) %&gt;%\n  nest() %&gt;%\n  dplyr::mutate(\n    n_comp = map_dbl(data, nrow),\n    # max_comp_pct = map_dbl(data,\n    #                        ~ max(.[\"comppct_r\"])),\n    # min_comp_pct = map_dbl(data, ~ min(.[\"comppct_r\"])),\n    cokeys = map(data, ~pull(.data = ., var = cokey))\n  )"
  },
  {
    "objectID": "07-map-unit-agg.html#calculate-map-unit-weighted-means",
    "href": "07-map-unit-agg.html#calculate-map-unit-weighted-means",
    "title": "6  Map unit aggregation",
    "section": "6.4 Calculate map unit weighted means",
    "text": "6.4 Calculate map unit weighted means\nI use the custom function below to calculate the component percent-weighted means for all soil properties and MUKEYs. Recall that I have the representative component percent, comppct_r in the list-column “data” within the cmp_nest dataframe. This came from the cmp_lookup table I loaded at the beginning.\nConveniently, we don’t need to do any extra work to normalize the COKEY weights. The default behavior of the weighted.mean function is to take the numerical vector of weights, w, and normalize it to sum to one. So we can simply supply comppct_r for the weights.\n\n# decided that I don't need to add \"na.rm\" argument \n# to the weighted.mean function here, because I've already removed all the NAs up above\ncalc_mu_wtd &lt;- function(df, myvars){\n\n  wts &lt;- df %&gt;% pull(comppct_r)\n  \n  df_mu_wtd &lt;- df %&gt;%\n    summarise(across(.cols = all_of(myvars), \n                   .fns = ~weighted.mean(.x, w = wts)\n                   ))\n  \n  return(df_mu_wtd)\n\n}\n\n# test &lt;- as.data.frame(cmp_nest$data[[115]])\n# \n# calc_mu_wtd(df = test, myvars = clust_vars)\n\n\ndf_mu_sum &lt;- cmp_nest %&gt;% \n  mutate(mu_sum_data = map(.x = data, \n                           .f = calc_mu_wtd,\n                           myvars = clust_vars))"
  },
  {
    "objectID": "07-map-unit-agg.html#sec-dat-avail",
    "href": "07-map-unit-agg.html#sec-dat-avail",
    "title": "6  Map unit aggregation",
    "section": "6.5 Determine data availability",
    "text": "6.5 Determine data availability\nAnother thing to think about: what is our threshold for having “enough” data to appropriately represent a given MUKEY? For example, if we have a MUKEY represented by 1 component, but that component has a relatively small representative %? Like a comppct_r of &lt;50%? &lt;30%? Would we still consider that “representative” of that particular MUKEY?\nWhat Devine et al. did was use the following logic:\n\nData is available for at least 80% of the mapunit components OR\nData availability at least equal to the dominant component percentage (recall that this info is stored in the dom_cmp_key object for our analysis).\n\nFirst, a little context about the number of components in our remaining MUKEYs:\n\n# for context, tabulate mukeys by num comps\nncomp_counts &lt;- cmp_nest %&gt;% \n  group_by(n_comp) %&gt;% \n  count(name = \"n_mukeys\")\n\nhead(ncomp_counts)\n\n\n\n  \n\n\n\nNow, calculate the data availability for each MUKEY. Recall that since we dropped incomplete cases at the COKEY level above, we can get at data availability by summing the remaining comppct_r for each MUKEY.\nThe object dom_cmp_key I join in here was created in back in Chapter 3 . It specifies the comppct_r for the dominant component in a given map unit. It was important to pull this number early in our data aggregation process, before dropping any components due to missing data.\n\ndata_avail &lt;- df_mu_sum %&gt;%\n  mutate(avail_data_perc = map_dbl(data,  ~ sum(.[\"comppct_r\"]))) %&gt;%\n  select(mukey, cokeys, avail_data_perc) %&gt;% \n  left_join(., dom_cmp_key, by = \"mukey\")\n\n\nhead(data_avail)"
  },
  {
    "objectID": "07-map-unit-agg.html#exclude-based-on-data-availability",
    "href": "07-map-unit-agg.html#exclude-based-on-data-availability",
    "title": "6  Map unit aggregation",
    "section": "6.6 Exclude based on data availability",
    "text": "6.6 Exclude based on data availability\n\n# use same conditions as Devine et al. \n# to populate a column specifying whether \n# to include a given mukey.\n# the order of the conditions is important\n# must go from most specific to most general\n# see https://dplyr.tidyverse.org/reference/case_when.html\nexclude_key &lt;- data_avail %&gt;% \n  mutate(include = case_when(\n    avail_data_perc &gt;= 80 ~ \"yes\", # cond. 2 (n=5874)\n    avail_data_perc &gt;= dom_comppct ~ \"yes\", # cond. 1 (n=6912)\n    TRUE ~ \"no\" # doesn't meet either condition\n  )) %&gt;% \n  select(-cokeys)\n\n# add the info about include/excl to main dataset\ndf_mu_avail &lt;- full_join(exclude_key, df_mu_sum, by = \"mukey\")\n\n# keep only \"include\" mukeys\ndf_mu_incl &lt;- df_mu_avail %&gt;% \n  filter(include == \"yes\")\n\nAfter applying the criteria above for data availability, we went from 7062 to 6912 MUKEYs, a difference of 150."
  },
  {
    "objectID": "07-map-unit-agg.html#save-results",
    "href": "07-map-unit-agg.html#save-results",
    "title": "6  Map unit aggregation",
    "section": "6.7 Save Results",
    "text": "6.7 Save Results\n\n# the data we want, summarised at the MUKEY level, \n# is in a list-column. That's why I use unnest here \n# before saving the results. \nmu_unnest &lt;- df_mu_incl %&gt;%\n  select(mukey,\n         mu_sum_data) %&gt;%\n  unnest(mu_sum_data)\n\nwrite_csv(mu_unnest, \"./data/mu_weighted_soil_props.csv\")\n\nAlso want to save a key that relates my included cokeys with their mukey. This will be a slightly different list than the component_list.csv because we dropped COKEYs that had missing data in any of the clustering variables, and we dropped MUKEYs that didn’t have sufficient data.\n\ncmp_list_incl &lt;- df_mu_incl %&gt;% \n  select(mukey, cokeys) %&gt;% \n  unnest(cokeys) %&gt;% \n  rename(cokey = cokeys)\n\nwrite_csv(cmp_list_incl, \"data/key_cokey_mukey_complete_cases_include.csv\")"
  },
  {
    "objectID": "07-map-unit-agg.html#more-about-mukeys-we-are-excluding",
    "href": "07-map-unit-agg.html#more-about-mukeys-we-are-excluding",
    "title": "6  Map unit aggregation",
    "section": "6.8 More about MUKEYs we are excluding",
    "text": "6.8 More about MUKEYs we are excluding\nLearn a little more about what we are excluding:\n\n# how many MUKEYs are we excluding?\nexclude_key %&gt;% \n  group_by(include) %&gt;% \n  count(name = \"num_mukeys\")\n\n\n\n  \n\n\n# grab only mukeys we are excluding, join in more info\nmukeys_drop &lt;- exclude_key %&gt;% \n  filter(include == \"no\") \n\nmukeys_drop_details &lt;- mukeys_drop %&gt;% \n  left_join(., mukey_info, by = \"mukey\")\n\n# most of these are not prime farmland\nmukeys_drop_details %&gt;% \n  group_by(farmlndcl) %&gt;% \n  count()\n\n\n\n  \n\n\n# types of mus \nmukeys_drop_details %&gt;% \n  group_by(mukind) %&gt;% \n  count()\n\n\n\n  \n\n\n# how many acres are represented? \ndrop_acres &lt;- sum(mukeys_drop_details$muacres)\ndrop_acres\n\n[1] 512769\n\n# of the 150 MUKEYs, how many are urban?\nurban &lt;- mukeys_drop_details %&gt;% \n  filter(str_detect(muname, \"Urban\"))\n\nnrow(urban)\n\n[1] 54\n\n# and what percentage of the dropped acres are urban? \nround((sum(urban$muacres) / drop_acres) * 100, digits = 1)\n\n[1] 29.1\n\n# dropping urban stuff (don't care about it)\n# want to look more at the non-urban MUs\n\nmunames &lt;- mukeys_drop_details %&gt;% \n  filter(!str_detect(muname, \"Urban\")) %&gt;% \n  pull(muname)\n\n# splitting at the comma so I can drop slope info\nmuname_stripped &lt;- as.data.frame(str_split_fixed(munames, \",\", 2)) %&gt;% select(V1)\n\n# once we strip out the slope info\n# only 35 unique, non-urban munames\ndistinct(muname_stripped) %&gt;% \n  rename(abbrev_muname = V1) %&gt;% \n  arrange(abbrev_muname)"
  },
  {
    "objectID": "07-map-unit-agg.html#sec-dom-eda",
    "href": "07-map-unit-agg.html#sec-dom-eda",
    "title": "6  Map unit aggregation",
    "section": "6.9 (old) Explore dominant component percentages",
    "text": "6.9 (old) Explore dominant component percentages\nKeeping this here as an illustration/explanation, but the dominant component percentages we actually want to use come from Chapter 3 . This is some troubleshooting and exploration I did on the road to figuring that out.\n\n# this adds MUKEY to our df \ncmp_mukey_detail &lt;- left_join(cmp, cmp_lookup, by = c(\"cokey\"))\n\nold_dom_cmp_key &lt;- cmp_mukey_detail %&gt;% \n  select(cokey, mukey, comppct_r) %&gt;% \n  group_by(mukey) %&gt;% \n  summarise(dom_comppct = max(comppct_r))\n\nhead(old_dom_cmp_key)\n\n\n\n  \n\n\n# again, these % look weird, and have some \n# very low %, because of the issues described below\nold_dom_cmp_key %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = dom_comppct)) +\n  ggtitle(\"Distribution of dominant component %\") + \n  xlab(\"Dominant component %\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThoughts after troubleshooting this. The dominant component percentages shouldn’t be calculated here, they should be done earlier, in Chapter 3, before we promote to SPC object. Below is an example with two MUKEYs to illustrate.\nBoth of these MUKEYs have large areas in components that don’t have data in the database (“Pits” and “Urban Land”). This makes sense. These are the types of components that are dropped when we promote our chorizon data to an aqp object, see Section 4.5.4 . There is data available for lower % components “Dassel” and “Udipsamment” here, but those aren’t the true dominant components, they are simply the only components that made it through the filtering that happened when I created my aqp object.\nSo if I want to use the dominant component percentage as a criterion in including/excluding data, I need to grab it earlier.\nMore broadly, if these areas are primarily urban land or pits, they aren’t really the type of thing we’d want to include in this primarily agricultural analysis anyway. So it’s good that we have a way to filter them out, using similar criteria to Devine et al. (described above).\n\n# as an example\nold_dom_cmp_key %&gt;% \n  filter(dom_comppct &lt; 25) %&gt;% \n  left_join(., cmp_lookup, by = \"mukey\") %&gt;% \n  filter(mukey %in% c(394766, 428184))"
  },
  {
    "objectID": "08-var-transformations.html#overview",
    "href": "08-var-transformations.html#overview",
    "title": "7  Variable Transformations",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nSo it makes sense to me that we would want to apply transformations on our variables with skewed distributions, but it has been surprisingly hard to find more info about this in the textbooks and tutorials I’ve found on k-means. Might need to continue looking. Because we are going to standardize / scale the data by subtracting the mean and dividing by 1 standard deviation, I think it makes sense that we’d want something close to normal for the starting distribution (since the mean is not really a helpful summary statistic for data that is very skewed…).\nI made plots to review the best transformations for normalizing the soil property data I will be including in my k-means clustering. Here, I summarize my decisions based on the plots below:\n\nClay: square root\nOrganic matter: log10 or ln\nCEC: log10 or ln\nBulk density: cubed (dB ^ 3)\nEC: None\npH: None\nCarbonates: square root\nLEP: ln OR log10 (note that since we are setting LEP=0 values to 0.5, we don’t have to worry about adding an offset here before doing the log transformation).\nKsat: log10 or ln\nAWC: log10 or ln\n\n“In order to weight these properties equally in the analysis and de-emphasize extreme values, some data were log transformed and all data were rescaled to produce standardized distributions with means equal to 0 and standard deviations equal to 1.”\nDevine et al transformed:\n\nOM log(x+0)\nCEC log(x+5)\nEC log(x+2)\nLEP log(x+5)\nKs log(x+0)\n\n\nlibrary(tidyverse)\nlibrary(glue)\n\nm &lt;- read_csv(\"data/mu_weighted_soil_props.csv\")\n\nhead(m)\n\n\n\n  \n\n\n# relates mukeys to cokey(s), only complete cases\n# that have sufficient data availability (\"include\")\n# from 07-map-unit-agg.qmd\n\ncmp_lookup &lt;- read_csv(\"data/key_cokey_mukey_complete_cases_include.csv\")\n\ncmp_details &lt;- read_csv(\"data/component_list.csv\")\n\ncmp_slab &lt;- read_csv(\"data/wide_slab_aggregated_soil_props_20220912.csv\")\n\nmukey_details &lt;- read_csv(\"./data/target_mapunit_table.csv\")"
  },
  {
    "objectID": "08-var-transformations.html#zero-values",
    "href": "08-var-transformations.html#zero-values",
    "title": "7  Variable Transformations",
    "section": "7.2 Zero values",
    "text": "7.2 Zero values\nA key consideration for these transformations is whether we have zero values, and whether to set very low values to zero.\nIn cases where we have lots of zero values (carbonates would be a good example), think about whether we need to add a constant before applying a transformation in order to make the distribution more normal.\nIn terms of setting very low values to zero, need to think about this more in terms of the variables we have. I don’t have much experience with EC data, but Devine et al. did this for EC, setting anything &lt;1.5 to zero in order to disregard differences in this low range.\nLet’s count the zeroes, keeping only variables that have zero values:\nFor LEP, check textures of the samples with LEP = 0. If loamy sands or sands, this might be OK?\n\nm %&gt;% \n  summarise(across(where(is.numeric), ~sum(.x == 0))) %&gt;% \n  pivot_longer(cols = everything()) %&gt;% \n  filter(value &gt; 0)"
  },
  {
    "objectID": "08-var-transformations.html#function-for-transformation-plots",
    "href": "08-var-transformations.html#function-for-transformation-plots",
    "title": "7  Variable Transformations",
    "section": "7.3 Function for transformation plots",
    "text": "7.3 Function for transformation plots\nThis function allows us to compare multiple transformations on the same variable in one faceted plot. The arguments that end in _adjust are options to add a constant to deal with 0 values that would otherwise give us errors / infinite values.\n\nplot_transformations &lt;- function(var,\n                                 df,\n                                 log10_adjust = 0,\n                                 ln_adjust = 0,\n                                 sqrt_adjust = 0,\n                                 nbins = 30) {\n  trans_df &lt;- df %&gt;%\n    select(mukey, {{var}}) %&gt;%\n    mutate(\n      log10_trans = log10({{var}} + log10_adjust),\n      ln_trans = log({{var}} + ln_adjust),\n      sqrt_trans = sqrt({{var}} + sqrt_adjust)) %&gt;%\n  pivot_longer(cols = -c(mukey))\n\n\n  trans_df %&gt;%\n    ggplot(aes(x = value)) +\n    geom_histogram(bins = nbins) +\n    facet_wrap(vars(name), scales = \"free\") +\n    theme_bw()\n\n}"
  },
  {
    "objectID": "08-var-transformations.html#clay",
    "href": "08-var-transformations.html#clay",
    "title": "7  Variable Transformations",
    "section": "7.4 Clay",
    "text": "7.4 Clay\nHere, I think the square root transformation looks the best.\n\nplot_transformations(var = claytotal_r_value, df = m)"
  },
  {
    "objectID": "08-var-transformations.html#organic-matter",
    "href": "08-var-transformations.html#organic-matter",
    "title": "7  Variable Transformations",
    "section": "7.5 Organic Matter",
    "text": "7.5 Organic Matter\nAt first I thought the square root looked good here, but if you zoom in (second plot), it has a long tail (probably histosols). I would pick either the log10 or the ln.\n\nplot_transformations(var = om_r_value, df = m)\n\n\n\nm %&gt;% \n  mutate(sqrt_om = sqrt(om_r_value)) %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = sqrt_om)) + \n  theme_bw() +\n  ggtitle(\"Note long tail when zoomed in\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "08-var-transformations.html#cec",
    "href": "08-var-transformations.html#cec",
    "title": "7  Variable Transformations",
    "section": "7.6 CEC",
    "text": "7.6 CEC\nWould pick log10 or ln here.\n\nplot_transformations(var = cec7_r_value, df = m)"
  },
  {
    "objectID": "08-var-transformations.html#bulk-density",
    "href": "08-var-transformations.html#bulk-density",
    "title": "7  Variable Transformations",
    "section": "7.7 Bulk Density",
    "text": "7.7 Bulk Density\nLooking at the untransformed values, this is skewed left (others have been skewed right)\nNone of these transformations in the facet plot look great to me, I also tried squaring and cubing the values, I think cubing looks the best. However, this brings up the idea of interpretability, and what our transformations mean for interpreting the clusters. By squaring the bulk density values, I’m bringing low values closer together while spreading the higher values further apart. This deals with the skewness, but is this a reasonable thing to do from a interpretation perspective? It means that at the higher ranges of bulk density, values that are the same distance apart on the regular scale (say a 0.1 g/cm3 difference) are “more different” compared to values that are 0.1 g/cm3 different at the low end of our spectrum.\n\n# adding one here so we don't get any negative values \n# I'm not sure this is totally necessary? Think about\n# what happens when we rescale, \nplot_transformations(var = dbthirdbar_r_value,\n                     df = m,\n                     log10_adjust = 1,\n                     ln_adjust = 1,\n                     sqrt_adjust = 1)\n\n\n\n# I think this transformation looks the best\nm %&gt;% \n  mutate(cubed_db = dbthirdbar_r_value^3) %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = cubed_db)) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "08-var-transformations.html#sec-var-ec",
    "href": "08-var-transformations.html#sec-var-ec",
    "title": "7  Variable Transformations",
    "section": "7.8 EC",
    "text": "7.8 EC\nNo transformation for now, but need to think about if we have a threshold where we set everything below to zero. Consider setting our threshold at 1, but first look more closely at the MUKEYs with values =1, decide whether or not to include these. See Section 8.6.3 and the QGIS project in _qgis_files\nThe vast majority of our EC values are 0, approximately 89.6 . Of the values that are &gt;0, most are 1.5 dS/m or less, which Devine et al. set to zero. So it might be worth considering whether we even include EC? I looked up more info about the range of ECs that are a concern for crop production, perhaps that could inform our decision. See Section 7.8.2 .\n\nplot_transformations(var = ec_r_value,\n                     df = m,\n                     log10_adjust = 1,\n                     ln_adjust = 1,\n                     sqrt_adjust = 1)\n\n\n\n\n\n7.8.1 Explore EC values &gt;0\nLet’s take a look at the EC values that are greater than zero:\n\nm %&gt;% \n  filter(ec_r_value &gt; 0) %&gt;% \n  ggplot() + \n  geom_histogram(aes(x = ec_r_value), bins = 20) +\n  theme_bw() +\n  ggtitle(\"Distribution of EC values greater than 0\")\n\n\n\n\nWant to look more closely at the spatial distribution of these non-zero EC values, see Section 8.6.3 . To facilitate that, here I am making a simple lookup table of the MUKEYs and an “EC category” so I can make a simple map of where these non-zero values are.\n\nec_cat &lt;- m %&gt;% \n  select(mukey, ec_r_value) %&gt;% \n  mutate(ec_cat = case_when(\n    ec_r_value == 0 ~ 1,\n   (ec_r_value &gt; 0 & ec_r_value &lt; 1) ~ 2,\n   ec_r_value == 1 ~ 3,\n   ec_r_value &gt; 1 ~ 4\n  ))\n\nwrite_csv(ec_cat, \"data/ec_category_mukey.csv\")\n\n\nmukey_ec1 &lt;- m %&gt;% \n  filter(ec_r_value == 1) %&gt;% \n  select(mukey)\n\ndf_mukey_ec1 &lt;- left_join(mukey_ec1, mukey_details, by = \"mukey\")\n\nmukey_ecless1 &lt;- m %&gt;% \n  filter(ec_r_value &gt; 0 & ec_r_value &lt; 1) %&gt;% \n  select(mukey)\n\ndf_mukey_ecless1 &lt;- left_join(mukey_ecless1, mukey_details, by = \"mukey\")\n\n\n\n7.8.2 Crop tolerance ratings for EC\nThe line for saline or saline-sodic soils is 4 dS / m. A factsheet I found from NDSU online called “Corn Response to Soil Salinity” (saved in _refs) reports that corn yields started to decline above 1.96 dS/m (on sandy loam soils) and above 2.95 dS/M for silty clay loams. Crop response to salinity is related to soil texture; “coarser textured soils may not contain as much water as finer textured soils, making the salts more potent. So, if you have a sandy loam, your crop salt tolerances will be lower than if you have a silty clay loam”.\nThese tables are all taken from the excellent “Managing Saline Soils in North Dakota” fact sheet by David Franzen, published in 2003. Helpful context for interpreting these EC values. In general, it looks like vegetable crops are more sensitive at lower levels of EC compared to some of the major grain cash crops like corn, soybeans."
  },
  {
    "objectID": "08-var-transformations.html#ph",
    "href": "08-var-transformations.html#ph",
    "title": "7  Variable Transformations",
    "section": "7.9 pH",
    "text": "7.9 pH\nI think this one is fine without a transformation.\n\nplot_transformations(var = ph1to1h2o_r_value,\n                     df = m,\n                     nbins = 20)"
  },
  {
    "objectID": "08-var-transformations.html#carbonates",
    "href": "08-var-transformations.html#carbonates",
    "title": "7  Variable Transformations",
    "section": "7.10 Carbonates",
    "text": "7.10 Carbonates\nSquare root\n\nplot_transformations(var = caco3_r_value,\n                     df = m,\n                     nbins = 20, \n                     log10_adjust = 1, \n                     ln_adjust = 1,\n                     sqrt_adjust = 1)\n\n\n\nm %&gt;% \n  filter(caco3_r_value &gt; 0) %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = sqrt(caco3_r_value)))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "08-var-transformations.html#sec-lep-var",
    "href": "08-var-transformations.html#sec-lep-var",
    "title": "7  Variable Transformations",
    "section": "7.11 LEP",
    "text": "7.11 LEP\nWould pick log10 or ln here, recall that since we are setting LEP=0 to 0.5 (see Section 8.6.4 ), don’t need to worry about adding an offset before the log.\n\nplot_transformations(var = lep_r_value,\n                     df = m,\n                     nbins = 15, \n                     log10_adjust = 1, \n                     ln_adjust = 1,\n                     sqrt_adjust = 1)\n\n\n\n\nLEP values and corresponding shrink-swell classes:\n\nLow: &lt;3\nModerate: 3-6\nHigh: 6-9\nVery high: &gt;= 9\n\n\n# setting LEP = 0 as it's own class here\n# because I want to investigate where these are on a map\nlep_cat &lt;- m %&gt;% \n  select(mukey, lep_r_value) %&gt;% \n  mutate(lep_cat = case_when(\n    lep_r_value == 0 ~ 0,\n    lep_r_value &lt;3 ~ 1,\n   (lep_r_value &gt;= 3 & lep_r_value &lt; 6) ~ 2,\n   (lep_r_value &gt;= 6 & lep_r_value &lt; 9) ~ 3,\n   lep_r_value &gt;= 9 ~ 4\n  ))\n\nwrite_csv(lep_cat, \"data/lep_category_mukey.csv\")\n\n# quick tabulation of number mukeys in each group\nlep_cat_desc &lt;- c(\"Zero\", \"Low\", \"Moderate\", \"High\", \"Very High\")\n\nlep_cat %&gt;% \n  group_by(lep_cat) %&gt;% \n  summarise(n = n(), \n            .groups = \"drop\") %&gt;% \n  mutate(lep_desc = lep_cat_desc) %&gt;% \n  select(lep_desc, n, lep_cat)\n\n\n\n  \n\n\n\n\n7.11.1 Explore LEP = 0\n\nSee also Section 8.6.4\nThere are 117 MUKEYs with LEP = 0.\nThere are 75 unique MUNAMEs with LEP = 0\nAfter exploring maps of LEP on 2022-09-19, Nic and I decided to set anything with LEP = 0 to 0.5, which keeps it in the lowest part of the “low” range. When we looked at these values across the state, they were discontinuous across counties, and the total area with LEP=0 is small. Suspect these aren’t\n\n\nmukey_lep_zero &lt;- m %&gt;% \n  filter(lep_r_value == 0) %&gt;% \n  select(mukey)\n\n# how many MUKEYs have LEP = 0?\nnrow(mukey_lep_zero)\n\n[1] 117\n\n# are the MUNAMES a clue to why LEP=0?\ndf_mukey_lep_zero &lt;- left_join(mukey_lep_zero, mukey_details, by = \"mukey\") %&gt;% \n  select(mukey, muname, mukind)\n\ndf_mukey_lep_zero\n\n\n\n  \n\n\n# 22 of the 75 unique map unit NAMES contain \"muck\"\nunique(df_mukey_lep_zero$muname) %&gt;% str_detect(string = ., pattern = \"muck\") %&gt;% \n  sum()\n\n[1] 22\n\n# 30 of the 75 unique map unit NAMES contain \"sand\"\nunique(df_mukey_lep_zero$muname) %&gt;% str_detect(string = ., pattern = \"sand\") %&gt;% \n  sum()\n\n[1] 30\n\n# these munames don't contain \"sand\" or \"muck\"\ndf_mukey_lep_zero %&gt;% \n  filter(!str_detect(muname, \"muck\"),\n         !str_detect(muname, \"sand\"))\n\n\n\n  \n\n\n\nLet’s take a look at the texture classes:\n\n# rows are added because we have some MUKEYs with \n# multiple components \nlepzero_cokey &lt;- left_join(df_mukey_lep_zero, cmp_lookup, by = \"mukey\")\n\nlepzero_cmp &lt;- left_join(lepzero_cokey, cmp_slab, by = c(\"cokey\"))\n\ncmp_tax &lt;- cmp_details %&gt;% select(mukey, cokey, contains(\"tax\"))\n\n\nlz &lt;- left_join(lepzero_cmp, cmp_tax, by = \"cokey\")\n\nlepzero_ssc &lt;- lz %&gt;% \n  select(cokey, sandtotal_r_value, silttotal_r_value, claytotal_r_value) %&gt;% \n  mutate(texcl = aqp::ssc_to_texcl(sand = lz$sandtotal_r_value,\n                                   clay = lz$claytotal_r_value))\n\nlepzero_ssc %&gt;% \n  group_by(texcl) %&gt;% \n  count() %&gt;% \n  mutate(lab_ypos = n+2) %&gt;% \n  ggplot() + \n  geom_col(aes(x = reorder(texcl, n), y = n)) +\n  geom_text(aes(x = texcl, y = lab_ypos, label = n)) +\n  theme_bw() +\n  xlab(\"Texture Class\") + \n  ylab(\"Number MUKEYs\") + \n  ggtitle(\"MUKEYs with LEP=0\")\n\n\n\n\nAnd also look at the soil taxonomy:\n\n# great groups\nlz %&gt;% \n  group_by(taxgrtgroup) %&gt;% \n  count() %&gt;% \n  arrange(desc(n))\n\n\n\n  \n\n\n# subgroups\nlz %&gt;% \n  group_by(taxsubgrp) %&gt;% \n  count() %&gt;% \n  arrange(desc(n))"
  },
  {
    "objectID": "08-var-transformations.html#ksat",
    "href": "08-var-transformations.html#ksat",
    "title": "7  Variable Transformations",
    "section": "7.12 Ksat",
    "text": "7.12 Ksat\nWould pick log10 or ln here.\n\nplot_transformations(var = ksat_r_value,\n                     df = m,\n                     nbins = 15)"
  },
  {
    "objectID": "08-var-transformations.html#awc",
    "href": "08-var-transformations.html#awc",
    "title": "7  Variable Transformations",
    "section": "7.13 AWC",
    "text": "7.13 AWC\nLog10 or ln\n\nplot_transformations(var = awc_r_value,\n                     df = m,\n                     nbins = 20)"
  },
  {
    "objectID": "09-plot-ec-lep.html#overview",
    "href": "09-plot-ec-lep.html#overview",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.1 Overview",
    "text": "8.1 Overview\nThis chapter serves two main purposes:\n\nvisualize distributions of EC and LEP values, to help make a decision about how to deal with unlikely values (LEP = 0) or very low / agronomically unimportant values (EC between 0-1).\nDo a test run of the raster reclass process with {terra}"
  },
  {
    "objectID": "09-plot-ec-lep.html#load-packages-and-data",
    "href": "09-plot-ec-lep.html#load-packages-and-data",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.2 Load packages and data",
    "text": "8.2 Load packages and data\nI’ll be using the {terra} package, which is for working with raster data. When I started this project, I created the initial AOI raster in QGIS and modified in ArcGIS, but am hoping to do more in R so it can be documented as part of this workflow.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(terra)\n\n# gssurgo raster, clipped to AOI and simple MUKEYs to reduce size\nr &lt;- rast(\"data/gSSURGO_MN/MapunitRaster_10m_Clip1_and_Reclass/MapunitRaster_10m_Clip1_and_Reclass/Reclass_tif1.tif\")\n\n# allows us to translate our \"new\" (shorter) MUKEYs to the\n# originals that match up with the rest of the db\naoi_mu &lt;- read.delim(\"data/gSSURGO_MN/mukey_new_crosswalk.txt\", sep = \",\") %&gt;% \n  select(MUKEY, MUKEY_New, Count)\n\nI created these keys for assigned EC category and LEP category in Chapter 7 .\n\n# to make a thematic EC map\nec_cat &lt;- read_csv(\"data/ec_category_mukey.csv\") %&gt;%\n  full_join(., aoi_mu, by = c(\"mukey\" = \"MUKEY\")) %&gt;% \n  mutate(ec_cat = case_when(\n    is.na(ec_cat) ~ 5,\n    TRUE ~ ec_cat\n  ))\n\nRows: 6912 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): mukey, ec_r_value, ec_cat\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlep_cat &lt;- read_csv(\"data/lep_category_mukey.csv\")%&gt;% \n  full_join(., aoi_mu, by = c(\"mukey\" = \"MUKEY\")) %&gt;% \n  mutate(lep_cat = case_when(\n    is.na(lep_cat) ~ 5,\n    TRUE ~ lep_cat\n  ))\n\nRows: 6912 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): mukey, lep_r_value, lep_cat\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "09-plot-ec-lep.html#spatraster-object",
    "href": "09-plot-ec-lep.html#spatraster-object",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.3 SpatRaster object",
    "text": "8.3 SpatRaster object\nLook at how our raster object appears in R.\n\nr\n\nclass       : SpatRaster \ndimensions  : 61033, 47668, 1  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : -91855, 384825, 2278555, 2888885  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD_1983_Albers \nsource      : Reclass_tif1.tif \nname        : Reclass_tif1 \nmin value   :            0 \nmax value   :         7861"
  },
  {
    "objectID": "09-plot-ec-lep.html#tables-to-re-classify-cell-values",
    "href": "09-plot-ec-lep.html#tables-to-re-classify-cell-values",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.4 Tables to re-classify cell values",
    "text": "8.4 Tables to re-classify cell values\nFirst, try making a thematic map with the EC categories I loaded above. Will use terra::classify() to assign new values to my raster cells. Need to supply a two-column matrix for the reclass, with “from” (first column) and “to” (second column) values.\n\n# 7999 is the missing data value\nec_mx &lt;- ec_cat %&gt;% \n  select(MUKEY_New, ec_cat) %&gt;% \n  add_row(MUKEY_New = 7999, ec_cat = 5) %&gt;% \n  as.matrix()\n\nI’m going to do one for LEP too:\n\n# 7999 is the missing data value\nlep_mx &lt;- lep_cat %&gt;% \n  select(MUKEY_New, lep_cat) %&gt;% \n  add_row(MUKEY_New = 7999, lep_cat = 5) %&gt;% \n  as.matrix()"
  },
  {
    "objectID": "09-plot-ec-lep.html#raster-reclass-process",
    "href": "09-plot-ec-lep.html#raster-reclass-process",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.5 Raster reclass process",
    "text": "8.5 Raster reclass process\n!! Note that I have set this chunk option eval: false to make sure this doesn’t run again when I render the project (because it takes so long). In my _quarto.yml I have the execute option freeze: auto set, which typically would mean that code is only re-run if I have changed the source. I want to avoid that behavior here, because I’m fiddling around with the source, but don’t need this test to run again.\nAlso, it is important to supply the filename argument here to write the file. Otherwise, it will throw an error related to “insufficient disk space”, because terra is trying to save the reclassed raster as a temp file. For more info, read this issue terra’s github repo.\nRecall that 1 byte = 8 bits = 2^8 (256) unique numbers, so the datatype = \"INT2U\" below means “integer, 2 bytes, unsigned”. Back when I was dealing with the big raster size due to MUKEY values (see Section 1.6.3 ), I reclassed the raster in ArcGIS to allow us to use 16-bit encoding. 16-bit would be 2 bytes so I think I should be able to stick with that for saving this EC raster.\n\n###### RECLASS for EC -------------------------------\n\n# this took a LONG time (50 minutes??)\n\nstart_reclass &lt;- Sys.time()\n\nr_reclass &lt;- classify(x = r,\n                      rcl = ec_mx, \n                      filename = \"E:/big-files-backup/ch03-sh-groups/ec_reclass.tif\",\n                      datatype = \"INT2U\", \n                      overwrite = TRUE)\n\nend_reclass &lt;- Sys.time() \n\nstart_reclass - end_reclass\n\n###### RECLASS for LEP -------------------------------\n\nstart_reclass &lt;- Sys.time()\n\nr_reclass &lt;- classify(x = r,\n                      rcl = lep_mx, \n                      filename = \"E:/big-files-backup/ch03-sh-groups/lep_reclass.tif\",\n                      datatype = \"INT2U\", \n                      overwrite = TRUE)\n\nend_reclass &lt;- Sys.time() \n\nstart_reclass - end_reclass\n\n\n8.5.1 A note about file size\nAfter I saved this new raster, I checked the file size and it came to 0.1070863 GB (not bad!). I’d like to figure out how to make it go faster (if possible), but right now that’s not my first priority."
  },
  {
    "objectID": "09-plot-ec-lep.html#visualize-re-classed-rasters",
    "href": "09-plot-ec-lep.html#visualize-re-classed-rasters",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.6 Visualize re-classed rasters",
    "text": "8.6 Visualize re-classed rasters\n\n8.6.1 Read in .tifs\nReminder of what EC cell values mean:\n\n1: EC is 0\n2: EC is &gt;0, but &lt;1\n3: EC = 1\n4: EC &gt; 1\n5: NA value (meaning we are not including that area)\n\nNote the min and max values here (1 and 5). This means that I successfully reclassed all my raster values.\n\nec_rast &lt;- rast(\"E:/big-files-backup/ch03-sh-groups/ec_reclass.tif\")\n\nec_rast\n\nclass       : SpatRaster \ndimensions  : 61033, 47668, 1  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : -91855, 384825, 2278555, 2888885  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD_1983_Albers \nsource      : ec_reclass.tif \nname        : Reclass_tif1 \nmin value   :            1 \nmax value   :            5 \n\n\nReminder of what LEP cell values mean:\n\n0: LEP = 0\n1: Low LEP &lt;3\n2: Moderate LEP &gt;=3 & &lt;6\n3: High LEP&gt;= 6 & &lt;9\n4: Very high LEP &gt;= 9\n5: NA value (meaning we are not including that area)\n\n\nlep_rast &lt;- rast(\"E:/big-files-backup/ch03-sh-groups/lep_reclass.tif\")\n\nlep_rast\n\nclass       : SpatRaster \ndimensions  : 61033, 47668, 1  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : -91855, 384825, 2278555, 2888885  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD_1983_Albers \nsource      : lep_reclass.tif \nname        : Reclass_tif1 \nmin value   :            0 \nmax value   :            5 \n\n\n\n\n8.6.2 Plot troubleshooting\nHad issues with getting the colors to work in this plot, this gis.stackexchange thread was helpful.\nI had to update my version of terra, I had 1.4.XX, the method for doing colors described at the link above requires 1.5.50 or higher.\nWhen I run this interactively, I get error messages (below), but the plot still appears. This seems to be a known issue, potentially related to garbage collection. Can follow this issue for more details, last updated 2022-09-20 (very recent). This is the error:\nError in x$.self$finalize() : attempt to apply non-function\n\n\n8.6.3 EC Plot\nNic and I explored these rasters in QGIS on 2022-09-19, and decided that anything with EC &gt;0 but &lt;1 should be set to 0. As you can see on the plots below, there are some weird artefacts in SE Minnesota that are showing up with measured EC values, even though surrounding areas are all 0. Because there are parts of NW Minnesota that also have EC values in this range, it’s possible that this decision will create some weirdness in that part of the map. Check this out in the final clustering outputs.\n\nmycols &lt;- data.frame(values = c(1:5), cols = hcl.colors(5, palette = \"viridis\"))\n\ncoltab(ec_rast) &lt;- mycols\n\nplot(ec_rast, col = mycols, plg=list(legend = c(\"EC=0\", \"EC&gt;0 & &lt;1\", \"EC=1\", \"EC&gt;1\", \"Not Incl\")), main = \"EC Categories\")\n\n\n\n\nAnd a plot to highlight just the areas where EC will be set to zero:\n\nlowec_col_only &lt;- data.frame(values = c(1:5),\n                             cols = c(\"#4B0055\",\"#FDE333\", \"#4B0055\", \"#4B0055\", \"#4B0055\"))\n\nplot(ec_rast, col = lowec_col_only, plg=list(legend = c(\"EC=0\", \"EC&gt;0 & &lt;1\", \"EC=1\", \"EC&gt;1\", \"Not Incl\")), main = \"EC values to set to 0\")\n\n\n\n\n\n\n8.6.4 LEP Plot\n\nlepcols &lt;- data.frame(values = c(0:5), cols = hcl.colors(6, palette = \"Zissou 1\"))\n\ncoltab(lep_rast) &lt;- lepcols\n\n\nplot(lep_rast, col = lepcols, plg=list(legend = c(\"LEP=0\", \"LEP&lt;3\", \"LEP&gt;=3 & &lt;6\", \"LEP&gt;=6 & &lt;9\", \"LEP &gt;=9\", \"Not incl\")), main = \"LEP Categories\")\n\n\n\n\nAnd a plot to highlight only the areas with LEP = 0, these will be set to 0.5 (the low end of the lowest category). We made this decision because the LEP=0 areas are discontinuous across counties, and not very prevalent across the state. Suspect this is not a measured value.\n\nlepz_col_only &lt;- data.frame(values = c(0:5),\n                             cols = c(\"#FDE333\",\"#4B0055\", \"#4B0055\", \"#4B0055\", \"#4B0055\", \"#4B0055\"))\n\nplot(lep_rast, col = lepz_col_only, plg=list(legend = c(\"LEP=0\", \"LEP&lt;3\", \"LEP&gt;=3 & &lt;6\", \"LEP&gt;=6 & &lt;9\", \"LEP &gt;=9\", \"Not incl\")), main = \"LEP values to set to 0.5\")"
  },
  {
    "objectID": "10-additional-data-prep.html#anthroportic-udorthents",
    "href": "10-additional-data-prep.html#anthroportic-udorthents",
    "title": "9  Additional Data Preparation",
    "section": "9.1 Anthroportic Udorthents",
    "text": "9.1 Anthroportic Udorthents\n\n9.1.1 Identify MUKEYs to exclude\n\nExclude taxsubgrp == Anthroportic Udorthent\n\nThis covers some of the pits (see next bullet), and all of the MUNAME containing “dump”. It also covers everything with “landfill” in the MUNAME\n\nExclude MUNAMEs that contain “Pit” or “pit”, but keep “pitted”, as below\n\nThis covers pits that are not classified as Anthroportic Udorthents\n\n\n\n# look at anything that includes \"Anthro\" in the subgroup\nanthro &lt;- mu_detail %&gt;% \n  filter(str_detect(taxsubgrp, \"Anthro\")) %&gt;% \n  select(muname, taxsubgrp, muacres, mukey, cokey)\n\nanthro \n\n\n\n  \n\n\n# all landfills in our \"include\" dataset \n# are classified as Anthroportic Udorthents \nmu_detail %&gt;% filter(str_detect(muname, \"landfill\")) %&gt;% \n  select(muname, taxsubgrp, taxgrtgroup, muacres, mukey, cokey)\n\n\n\n  \n\n\n# not all munames that include \"pit\" (gravel or sand)\n# include classification info, some have taxsubgrp \n# entered as anthroportic udorthent and some don't \n# this means we will also want to exclude based on \n# some version of \"pit\" in the muname, but careful not to exclude # \"pitted\"\npits &lt;- mu_detail %&gt;% \n  filter(str_detect(muname, \"Pit\") |\n           str_detect(muname, \"pit\"),\n         !str_detect(muname, \"pitted\")) %&gt;% \n  select(muname, taxsubgrp, taxgrtgroup, muacres, mukey, cokey) \n\npits\n\n\n\n  \n\n\n# it appears that all munames that include \"dump\"\n# are classified as anthroportic udorthents \ndumps &lt;- mu_detail %&gt;% \n  filter(str_detect(muname, \"Dump\") |\n           str_detect(muname, \"dump\")) %&gt;% \n  select(muname, taxsubgrp, muacres, mukey, cokey) \n\ndumps\n\n\n\n  \n\n\n# look at the other Udorthents to be sure\n# excluding the Anthroportic Udorthents here\n# b/c they are already on the cut list\nudorthents &lt;- mu_detail %&gt;% \n  filter(taxgrtgroup == \"Udorthents\",\n         !str_detect(taxsubgrp, \"Anthroportic\")) %&gt;% \n  #safer to do this because sometimes udorthent is singular, sometimes plural\n  select(muname, taxsubgrp, taxgrtgroup, muacres, mukey, cokey) \n\n# these all seem fine to keep\nudorthents\n\n\n\n  \n\n\n\n\n\n9.1.2 Compile list of MUKEYs to exclude\n\nanthro_mukeys &lt;- anthro %&gt;% pull(mukey)\n\npit_mukeys &lt;- pits %&gt;% \npull(mukey)\n\n# the \"all\" list contains duplicates, \n# keep only unique mukeys\nexcl_mukeys_all &lt;- c(anthro_mukeys, pit_mukeys)\nexcl_mukeys_unique &lt;- unique(excl_mukeys_all)\n\n\n\n9.1.3 Exclude\n\nmu_no_anthro &lt;- mu %&gt;% \n  filter(!mukey %in% excl_mukeys_unique)\n\nWe started with 6912 MUKEYs, after excluding the Anthroportic Udorthents and pits, we have 6873 MUKEYs, a difference of 39 ."
  },
  {
    "objectID": "10-additional-data-prep.html#update-ec-values",
    "href": "10-additional-data-prep.html#update-ec-values",
    "title": "9  Additional Data Preparation",
    "section": "9.2 Update EC Values",
    "text": "9.2 Update EC Values\nAfter reviewing maps of the EC values across the state in Chapter 8 , we decided to set all EC values that were &gt;0 and &lt;1 to 0.\n\nmu_ec_update &lt;- mu_no_anthro %&gt;% \n  mutate(ec_r_value = case_when(\n    (ec_r_value &gt; 0 & ec_r_value &lt; 1) ~ 0,\n    TRUE ~ ec_r_value\n  ))\n  \n# how many mukeys were set to zero with the above rule?\nec_set_zero &lt;- mu_no_anthro %&gt;% \n  filter(ec_r_value &gt; 0, ec_r_value &lt; 1) %&gt;% \n  pull(mukey) %&gt;% \n  unique() %&gt;% \n  length()\n\nThere were 205 MUKEYs with EC value set to zero using the above rule."
  },
  {
    "objectID": "10-additional-data-prep.html#update-lep-values",
    "href": "10-additional-data-prep.html#update-lep-values",
    "title": "9  Additional Data Preparation",
    "section": "9.3 Update LEP Values",
    "text": "9.3 Update LEP Values\nThere are a small number of map units, covering a small area, that have LEP=0. Set these to 0.5 (based on the spatial pattern, we suspect these aren’t truly measured 0 values).\n\nmu_lep_update &lt;- mu_ec_update %&gt;% \n  mutate(lep_r_value = case_when(\n    lep_r_value == 0 ~ 0.5,\n    TRUE ~ lep_r_value\n  ))\n\n# how many mukeys were set to 0.5 with the above rule?\nlep_update_mukeys &lt;- mu_ec_update %&gt;% \n  filter(lep_r_value == 0) %&gt;% \n  pull(mukey) %&gt;% \n  unique() %&gt;% \n  length()\n\nThere were 117 affected by the above rule."
  },
  {
    "objectID": "10-additional-data-prep.html#sec-fen",
    "href": "10-additional-data-prep.html#sec-fen",
    "title": "9  Additional Data Preparation",
    "section": "9.4 High carbonates?",
    "text": "9.4 High carbonates?\nIn Chapter 11 , I noticed some of our MUKEYs have REALLY high carbonates for topsoil (&gt;20% ). Look into these a little more, is it possible these are data entry errors?\nThe one with 70% carbonates in particular seems goofy. I wonder if this is a calcareous fen? Do we want to include it? Represents ~700 acres across the state. I mapped this and it appears to be mostly in Steele County (SE MN).\nMost of the others that have carbonates in the 20-30% range are rims on depressions in till plains or lake plains. So that seems reasonable given the setting.\n\nhighcarb &lt;- mu_lep_update %&gt;% \n  filter(caco3_r_value &gt; 20)\n\nhighcarb %&gt;% \n  ggplot() + \n  geom_point(aes(x = ph1to1h2o_r_value, y = caco3_r_value)) + \n  theme_bw()\n\n\n\nhc_mukeys &lt;- highcarb %&gt;% pull(mukey)\n\n# clicked through these, to see if any seemed \n# like data entry errors\nmu_detail %&gt;% \n  filter(mukey %in% hc_mukeys) %&gt;% \n  select(mukey, caco3_r_value, \n         muname, muacres, compname, geomdesc, contains(\"tax\")) \n\n\n\n  \n\n\n# this is the really high one (Marsh) \nmu_detail %&gt;% filter(muname == \"Marsh\")\n\n\n\n  \n\n\n# I want to plot these Marsh areas in QGIS. What \n# is the short MUKEY? \nmu_detail %&gt;% \n  filter(muname == \"Marsh\") %&gt;% \n  left_join(., cwalk, by = c(\"mukey\" = \"MUKEY\")) %&gt;% \n  select(mukey, MUKEY_New)\n\n\n\n  \n\n\n\nGiven the very small area, and extreme carbonate values, I’m going to drop this one MUKEY (428275 in SSURGO; 2682 in my reclass).\n\ndropmarsh &lt;- mu_lep_update %&gt;% \n  filter(mukey != 428275)"
  },
  {
    "objectID": "10-additional-data-prep.html#save-dataset",
    "href": "10-additional-data-prep.html#save-dataset",
    "title": "9  Additional Data Preparation",
    "section": "9.5 Save dataset",
    "text": "9.5 Save dataset\nThis dataset is now ready for modeling.\n\nwrite_csv(dropmarsh, \"data/clean_mu_weighted_soil_props.csv\")"
  },
  {
    "objectID": "11-implement-kmeans.html#overview",
    "href": "11-implement-kmeans.html#overview",
    "title": "10  Implement k-means",
    "section": "10.1 Overview",
    "text": "10.1 Overview\nIn this section, I will use the cleaned dataset created in the last chapter to build a k-means pipeline that runs a model for a range of different cluster sizes (k).\nAfter fitting the models, I review some common metrics for determining best cluster sizes, and save the model objects for further investigation in the next chapter."
  },
  {
    "objectID": "11-implement-kmeans.html#setup",
    "href": "11-implement-kmeans.html#setup",
    "title": "10  Implement k-means",
    "section": "10.2 Setup",
    "text": "10.2 Setup\n\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glue)\nlibrary(tidyclust)\nlibrary(factoextra) # trying fviz_nbclust(), which gives elbow, silhouette, and gap statistic\nlibrary(hopkins)\nlibrary(fpc)\n\n\nd &lt;- read_csv(\"./data/clean_mu_weighted_soil_props.csv\") %&gt;% \n  select(-contains(\"comp_pct\"))\n\nold_names &lt;- colnames(d)\n\nnew_names &lt;- str_replace_all(old_names, \"_r_value\", \"\")\n\ncolnames(d) &lt;- new_names\n\nReminder of what the data look like:\n\nhead(d)\n\n\n\n  \n\n\n\nReminder of the transformations I chose to improve variable distributions and make them normal-ish.\n\nSquare root: clay, carbonates\nLog10: organic matter, cec, lep, ksat, awc\nCube (^3): bulk density\nNone: ec, ph"
  },
  {
    "objectID": "11-implement-kmeans.html#pre-process-data-recipe",
    "href": "11-implement-kmeans.html#pre-process-data-recipe",
    "title": "10  Implement k-means",
    "section": "10.3 Pre-process data (recipe)",
    "text": "10.3 Pre-process data (recipe)\nThis is something specific to the tidymodels modeling workflow, and I like it because it’s very explicit about how variables are transformed (pre-processed) prior to initializing the model.\nHere I apply some transformations to achieve more normal distributions, and then standardize (step_normalize) by subtracting the mean and dividing by 1 sd.\n\nrec_spec &lt;-   recipe(~., data = d) %&gt;% \n    update_role(mukey, new_role = \"ID\") %&gt;% \n  # note this is log10 (the default is ln)\n    step_log(om, cec7, ksat, awc, lep, base = 10) %&gt;% \n    step_mutate(dbthirdbar = dbthirdbar^3) %&gt;% \n    step_sqrt(claytotal, caco3) %&gt;% \n    step_normalize(all_numeric_predictors())\n\nrec_spec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n predictor         10\n\nOperations:\n\nLog transformation on om, cec7, ksat, awc, lep\nVariable mutation for dbthirdbar^3\nSquare root transformation on claytotal, caco3\nCentering and scaling for all_numeric_predictors()\n\n\n\n10.3.1 Check that it worked\nAlthough I did make plots of the data distributions after transforming in Chapter 7 , I wanted to do it again here as a check that the pre-processing that I am specifying is working as intended. Here, I do that with two functions from {recipes} :\n\nprep() estimates a pre-processing recipe. It takes my input dataset (“training set”) , and estimates the parameters (model inputs), reporting back on the specific operations and missing data\nbake() to return the training data (because I set new_data to NULL)\n\n\n# retain argument here tells prep to keep \n# the pre-processed training data \n# note this can make the final recipe size large, \n# so this is not the recipe object I probably want to use\n# in my list col below\ncheck_prep &lt;- prep(rec_spec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \ncheck_prepped_df &lt;- bake(check_prep, new_data = NULL)\n\nhead(check_prepped_df)\n\n\n\n  \n\n\n# save the pre-processed data for making a \n# correlation matrix\n\nwrite_csv(check_prepped_df, \"./data/data_preprocessed_all_var.csv\")\n\nOK, now making a plot of the transformed variables to take a look at their distributions:\n\ncheck_prepped_df %&gt;% \n  select(-mukey) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"var\", values_to = \"value\")  %&gt;% \n  ggplot(aes(x = value)) + \n  geom_histogram(bins =25) + \n  facet_wrap(vars(var), scales = \"free\") + \n  theme_bw() + \n  ggtitle(\"Distributions of transformed and standardized vars\")"
  },
  {
    "objectID": "11-implement-kmeans.html#sec-mod-opt",
    "href": "11-implement-kmeans.html#sec-mod-opt",
    "title": "10  Implement k-means",
    "section": "10.4 Set model options",
    "text": "10.4 Set model options\nDo I need to think about the initializer here? Based on what I was reading in the book chapter by Tan et al., 2018 (“Cluster Analysis: Basic Concepts and Algorithms”), it sounds like using kmeans++ for the initialization could results in better clustering (lower SSE). See page 543. See also this paper referenced in the clusterR documentation about seeding (initialization).\nstats::kmeans() the default seems to be Hartigan-Wong method, which uses random partition for the initialization. From the {tidyclust} documentation:\n\nThe observations are assigned to a cluster uniformly at random. The centroid of each cluster is computed, and these are used as the initial centers.\n\nI guess the other benefit of the Hartigan-Wong method is that it results in more consistent human-verified clusters (again, per tidyclust documentation listed above). Could read this blog post for a deeper dive into these methods if needed.\nClusterR::KMeans_rcpp() uses the Lloyd/Forgy method (per the tidyclust docs, this info wasn’t easy to find in the ClusterR docs)\nFor now I’m going with stats::kmeans, and setting the nstart to 10. Because the random initial configuration (random starting centroids) can have an impact on the final clusters, it sounds like it’s a good idea to do multiple starts, and then let kmeans return the best one (using lowest within cluster SSE as the metric for “best”)\n\n# writing a custom function here so I can be explicit \n# about the options I'm choosing, and also use within the \n# list-col framework I set up with map() below. \nkm_spec &lt;- function(nclust){\n  \n  tidyclust::k_means(num_clusters = nclust) %&gt;%\n    parsnip::set_engine(engine = \"stats\",\n               nstart = 10, # 1 is default, &gt;1 recommended\n               algorithm = \"Hartigan-Wong\", # H-W is default\n               iter.max = 20) # default is 10, wasn't always enough\n  \n}"
  },
  {
    "objectID": "11-implement-kmeans.html#set-up-data-structure",
    "href": "11-implement-kmeans.html#set-up-data-structure",
    "title": "10  Implement k-means",
    "section": "10.5 Set up data structure",
    "text": "10.5 Set up data structure\nHere I set up a dataframe that will catch my modeling results in list columns of the different model objects and return values. The first column I define specifies the range of different cluster sizes (k) that we will try.\n\ntry_clusts &lt;- c(2:20)\n\nkm_df &lt;- data.frame(n_clust = try_clusts)"
  },
  {
    "objectID": "11-implement-kmeans.html#specify-model-for-each-value-of-k",
    "href": "11-implement-kmeans.html#specify-model-for-each-value-of-k",
    "title": "10  Implement k-means",
    "section": "10.6 Specify model (for each value of k)",
    "text": "10.6 Specify model (for each value of k)\nFor each unique value of k (2-20), this returns a model specification object (in the kmeans_spec column) based on the custom function I wrote above. The model specification has all the options set about how we want the algorithm to run (methods, number of starts, etc.). We need a different one for each value of k.\nThe kmeans_wflow column here holds our workflow objects. These objects combine our model specification (from kmeans_spec) with the data recipe (preprocessor) we made above (rec_spec, is same for all models).\n\n# for each unique value of clusters (2:20), returns a model\n# specification (kmeans_spec) and a workflow (kmeans_wflow) \n# note that the workflow \nkm_df &lt;- km_df %&gt;%\n  mutate(\n    kmeans_spec = map(n_clust, ~ km_spec(nclust = .x)),\n    kmeans_wflow = map(kmeans_spec,\n                       ~ workflow(\n                         preprocessor = rec_spec, spec = .x\n                       ))\n  )\n\n# our current data structure\nhead(km_df, n=3L )\n\n\n\n  \n\n\n# take a look at an example workflow\nkm_df$kmeans_wflow[3]\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_mutate()\n• step_sqrt()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = nclust\n\nEngine-Specific Arguments:\n  nstart = 10\n  algorithm = Hartigan-Wong\n  iter.max = 20\n\nComputational engine: stats"
  },
  {
    "objectID": "11-implement-kmeans.html#fit-the-models",
    "href": "11-implement-kmeans.html#fit-the-models",
    "title": "10  Implement k-means",
    "section": "10.7 Fit the models",
    "text": "10.7 Fit the models\nAll the steps above were related to specifying different aspects of this model. Now we can actually fit the models.\nSome troubleshooting here:\n\nStarted by specifying tidyclust::fit() but something weird was happening where my step_normalize() wasn’t included in the pre-processor recipe when I looked at the fitted model object.\nIf I specify parsnip::fit() , then step_normalize() is included and the values of the cluster centroids are in the expected ranges (centered, scaled).\nI also tried this without explicitly specifying the package (so just fit() ) and it worked as expected.\n\n\n# make a quiet version of fit(), so we can capture results \n# and any warning messages from the models \n# see troubleshooting notes below\nquiet_fit &lt;- purrr::quietly(.f = parsnip::fit)\n\nset.seed(4) # for reproducibility \nkm_fit_df &lt;- km_df %&gt;%\n  mutate(km_result = map(.x = kmeans_wflow,\n                      .f = quiet_fit,\n       # data comes after .f b/c not vectorized over            \n                      data = d),\n       km_fit = map(km_result, ~pluck(.x, 'result')),\n       warn = map(km_fit, ~pluck(.x, 'warnings')),\n       msg = map(km_fit, ~pluck(.x, 'messages')),\n       n_iter = map_dbl(km_fit, \n                      ~pluck(.x, 'fit', 'fit', 'fit', 'iter' ))) \n         \n\n# check out current data structure\nhead(km_fit_df, n = 3L)\n\n\n\n  \n\n\n# don't need anymore, cleaning up\nrm(km_df)\n\n\n10.7.1 Notes about model fit troubleshooting\nIf I set.seed(123) and run kmeans with max.iter=10, get warnings about ‘no convergence at 10 iterations’, for k = 17 and k = 20. Changed to max.iter=20 and ran again, this time no convergence warnings.\nAs a result of this experience, I added some additional columns to the km_fit_df object using {purrr} ’s function quietly() so I could capture warnings and messages that would otherwise only appear in the console (and are hard to trace when I’m iterating through all these models at once). This blog post was very helpful for an example of how to do this.\nIn an earlier troubleshooting attempt, I was trying to see if the warning messages were stored anywhere in the fitted {tidyclust} model object? It seemed like maybe they would have been in ifault, but those values were all 0, even when I had model convergence warnings. In that case I tried indexing into the fitted model objects with map(km_fit, ~pluck(.x, 'result', 'fit', 'fit', 'fit', 'ifault' ))\n\n\n10.7.2 View messages & warnings\nWe can look at any warnings or messages from the modeling process:\n\nkm_fit_df %&gt;% \n  select(n_clust, warn, msg, n_iter)\n\n\n\n  \n\n\n\n\n\n10.7.3 Look at one fit object\nAs an example, these are what the fitted objects look like.\nNOTE the clustering vector here is using the cluster numbers directly from kmeans(). tidyclust assigns names like “Cluster_1”, “Cluster_2” etc. , but the numbers do NOT necessarily match with what kmeans() returns. The CLUSTERINGS are the same, but the numbers are not necessarily so. So 2 in this “Clustering Vector” below is NOT necessarily equal to tidyclust “Cluster_2” that you might get by using the extract_cluster_assignment function. To keep things consistent, I’m always using the cluster names assigned by tidyclust.\n\nexamp_fit &lt;- km_fit_df$km_result[[4]][['result']]\n\nexamp_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_mutate()\n• step_sqrt()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-means clustering with 5 clusters of sizes 479, 2019, 1832, 1065, 1477\n\nCluster means:\n   claytotal         om       cec7 dbthirdbar         ec  ph1to1h2o      caco3\n1  0.5174494  0.3745308  0.3208957 -0.6376723  3.3283137  1.1660203  1.0228662\n2 -0.3814723 -0.3465904 -0.2722727  0.3236714 -0.2571456 -0.5186223 -0.5279703\n3  0.7722551  0.6366879  0.8147342 -0.5735785 -0.2463628 -0.2631583 -0.5750294\n4 -1.5118222 -1.1990889 -1.6374004  1.0687407 -0.2271428 -0.7273531 -0.5026188\n5  0.4858861  0.4272060  0.4382179 -0.2948278 -0.2585251  1.1816599  1.4656479\n         lep       ksat        awc\n1  0.4284214 -0.5251382  0.2167607\n2 -0.4841584  0.2713584  0.0553920\n3  0.6939805 -0.7350181  0.5806865\n4 -0.9544845  1.5123302 -1.7551062\n5  0.3503424 -0.3794245  0.3992591\n\nClustering vector:\n   [1] 4 4 4 2 2 4 4 4 4 4 2 4 4 2 4 3 2 2 4 4 2 2 4 4 2 4 4 4 4 4 2 2 4 4 4 4 4\n  [38] 2 2 2 4 4 4 4 4 2 4 4 4 4 4 2 2 2 3 5 5 2 3 3 3 3 3 1 4 4 4 4 5 5 3 3 4 4\n  [75] 5 4 5 3 2 2 3 3 3 1 5 4 4 4 4 4 4 4 3 4 4 5 2 4 4 4 3 3 5 5 5 5 4 4 2 2 2\n [112] 4 4 4 3 5 4 4 4 4 5 3 3 3 3 4 4 4 4 2 2 4 2 4 4 2 4 2 5 2 4 4 4 4 4 4 5 4\n [149] 4 4 5 2 2 3 1 2 1 4 2 2 2 4 5 5 5 5 2 2 4 4 4 2 2 4 4 2 2 5 5 5 2 3 2 2 3\n [186] 5 5 3 3 3 2 2 5 3 3 2 2 2 4 4 4 4 4 4 4 2 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 5\n [223] 5 3 4 4 3 3 3 5 4 4 4 3 3 4 5 2 3 3 4 2 5 5 2 3 4 3 3 1 4 4 4 2 1 2 4 4 5\n [260] 4 5 4 4 4 4 2 4 4 2 4 4 4 4 4 4 4 4 2 2 2 2 4 4 2 2 4 4 4 2 4 4 4 4 2 2 2\n [297] 2 2 4 4 4 4 2 4 2 4 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4\n [334] 2 2 2 1 3 5 5 1 1 1 1 5 1 5 5 3 3 5 5 3 1 5 1 3 5 5 1 2 5 3 5 5 2 1 5 5 3\n [371] 2 2 3 2 2 3 3 1 2 3 4 4 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 5 5 3 3 3 5 5 5\n [408] 5 5 3 2 2 3 3 3 3 2 2 2 2 3 3 3 2 2 5 5 5 3 2 3 2 2 2 3 3 3 4 4 4 3 3 4 2\n [445] 2 3 3 3 3 3 5 3 3 3 3 5 4 3 4 4 2 3 3 3 3 2 3 3 3 5 5 3 3 3 3 3 5 3 3 3 3\n [482] 5 3 2 2 2 5 2 2 3 3 2 3 3 3 5 2 3 3 3 5 3 3 5 5 5 3 3 3 5 3 3 3 3 3 3 3 5\n [519] 2 2 5 5 5 3 3 3 3 3 4 4 2 2 3 3 3 2 2 3 3 2 2 5 3 2 5 5 2 4 2 5 5 1 2 4 2\n [556] 2 5 2 5 3 5 5 5 5 2 2 5 5 5 5 5 3 5 2 5 2 3 3 5 2 5 5 5 3 5 3 5 5 5 2 2 2\n [593] 3 3 5 5 5 3 3 5 2 4 5 3 3 2 3 3 5 3 4 2 2 3 5 2 5 2 3 3 3 3 3 3 5 5 5 5 5\n [630] 2 3 3 3 3 2 2 2 2 3 2 2 2 5 3 3 3 2 2 2 2 3 3 3 3 2 5 5 5 4 4 4 4 2 2 2 2\n [667] 3 5 2 2 2 2 2 2 2 2 2 2 3 2 2 2 4 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 2 2 2 2 4\n [704] 4 2 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 2 4 4 4 5 3 2 4 4 2 4 4 2 4\n [741] 4 4 4 4 2 4 4 2 4 4 5 5 5 5 5 2 5 2 2 4 5 3 2 3 2 5 3 5 5 3 2 3 3 2 3 3 5\n [778] 5 1 5 5 5 5 5 5 2 2 2 2 5 3 3 5 3 3 2 3 5 3 3 5 5 5 5 5 5 5 5 2 3 5 5 5 5\n [815] 3 5 5 5 5 5 5 5 5 2 2 2 3 3 2 2 3 3 3 5 3 3 5 5 2 2 2 2 2 4 5 5 5 3 1 3 3\n [852] 3 1 5 5 1 5 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 4 4 1 3 1 1 4 4 4 1 2 2 4 2\n [889] 2 5 1 3 3 5 3 5 3 5 1 5 5 3 3 1 4 1 2 5 5 1 3 4 5 5 5 5 5 1 1 1 2 1 1 2 1\n [926] 1 1 1 1 1 3 2 2 3 4 2 1 1 1 3 3 5 3 4 4 4 4 3 3 1 5 5 1 1 1 4 4 5 4 4 2 2\n [963] 2 3 5 3 5 4 4 4 3 3 5 5 2 2 4 4 5 2 3 4 2 4 4 4 2 5 5 2 4 4 4 4 3 3 3 1 4\n[1000] 4 2 3 1 2 1 2 4 5 4 4 4 4 4 5 2 2 5 5 2 2 2 2 5 3 3 3 5 5 5 2 5 2 2 3 5 5\n[1037] 5 5 3 3 5 3 5 5 5 2 2 5 3 3 5 3 4 4 5 3 3 3 3 3 3 3 5 3 2 5 5 3 3 3 5 2 2\n[1074] 5 3 3 2 3 5 3 3 5 5 5 2 2 5 5 5 5 3 3 3 3 3 3 3 3 5 5 5 3 5 5 3 3 5 5 5 2\n[1111] 2 2 3 2 3 3 5 5 5 3 3 5 3 3 3 4 5 3 3 3 4 4 4 4 4 5 4 2 4 4 4 4 4 4 4 4 4\n[1148] 4 4 4 3 4 4 4 4 2 4 4 4 2 3 2 4 4 4 4 4 4 4 4 3 2 5 2 4 3 2 3 2 2 4 4 4 4\n[1185] 4 4 4 2 2 2 2 2 4 2 4 4 4 3 3 2 2 2 2 2 2 4 4 4 3 3 4 4 4 2 2 3 3 3 3 2 2\n\n...\nand 162 more lines.\n\n\nA nicer way to look at the results is by accessing specific parts of the fitted model object, as below.\n\n# some basic model metrics\nglance(examp_fit)\n\n\n\n  \n\n\n# centroid data (transformed/standardized scale)\ncentroids &lt;- tidyclust::extract_centroids(examp_fit)\ncentroids\n\n\n\n  \n\n\n# helpful to add to future plots for examining indiv. clusters\nclust_stat &lt;- tidyclust::sse_within(examp_fit)\nclust_stat"
  },
  {
    "objectID": "11-implement-kmeans.html#sec-mod-metrics",
    "href": "11-implement-kmeans.html#sec-mod-metrics",
    "title": "10  Implement k-means",
    "section": "10.8 Model metrics",
    "text": "10.8 Model metrics\nSee also section 7.5 in the Chapter by Tan et al. for more about cluster evaluation.\n\n10.8.1 Extract metrics\n\nmetrics_df &lt;- km_fit_df %&gt;%\n  mutate(\n    # tot_sse = total sum of squared error\n    tot_sse = map_dbl(km_fit, ~ sse_total_vec(.x)),\n    # tot_wss = sum of within-cluster sse\n    tot_wss = map_dbl(km_fit, ~sse_within_total_vec(.x)),\n    # sse ratio = wss / total sse, \n    sse_ratio = map_dbl(km_fit, ~sse_ratio_vec(.x))\n    )\n\nrm(km_fit_df)\n\nmetrics_simple &lt;- metrics_df %&gt;% \n  select(n_clust, tot_sse, tot_wss, sse_ratio)\n\nmetrics_simple\n\n\n\n  \n\n\n\n\n\n10.8.2 Plot Total WSS\nNot a clear “elbow” here, although by the time we get to 10-11 it does seem to be leveling off.\n\nmetrics_simple %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Compare values of k: looking for elbow\")\n\n\n\nmetrics_simple %&gt;% \n  filter(n_clust %in% c(2:12)) %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:12)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Zoom in a bit: looking for elbow\")\n\n\n\n\n\n\n10.8.3 Average Silhouette\nFrom the {tidyclust} documentation:\n\nAnother common measure of cluster structure is called the silhouette. The silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\n\nIn principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\nSee also pg. 581 in Tan2018 Chap 7 Cluster Analysis: Basic Concepts and Algorithms\n\nprepped_rec &lt;- prep(rec_spec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \nbaked_df &lt;- bake(prepped_rec, new_data = NULL) %&gt;% \n  select(-mukey) \n\ndists &lt;- baked_df %&gt;% as.matrix() %&gt;% dist(method = \"euclidean\")\n\nsilh_df &lt;- metrics_df %&gt;% \n  mutate(avg_sil = map_dbl(km_fit, \n                       tidyclust::silhouette_avg_vec,\n                       dists = dists),\n         indiv_sil = map(km_fit, \n                         tidyclust::silhouette,\n                         dists = dists))\n\nindiv_sil_df &lt;- silh_df %&gt;% select(n_clust, indiv_sil) %&gt;% \n  unnest(indiv_sil) %&gt;% \n  mutate(across(.cols = c(cluster, neighbor),\n                .fns = as.character))\n\nwrite_csv(indiv_sil_df, \"data/kmeans_points_silhouettes.csv\")\n\nrm(metrics_df)  \nrm(dists)\nrm(prepped_rec)\n\nHigher silhouette is better (means observations are closer to their centroids than to other observations). Seems to suggest that 4, 6, 10-12 would be OK (those are local maxima), but not greater than 12.\n\nsilh_df %&gt;% \n  ggplot(aes(x = n_clust, y = avg_sil)) +\n  geom_point() + \n  geom_line() + \n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  ggtitle(\"Overall Average Silhouette\") +\n  labs(subtitle = \"Higher is better, possible values [-1,1]\")\n\n\n\n\nCan also plot the individual silhouettes. For each clustering (model version), we have a silhouette value per observation in the dataset (n=6872). We also have the closest “neighbor” cluster, or the cluster that specific observation would belong to if its home cluster didn’t exist.\nHere’s an example of this data for the k=6 clustering. The\n\nneighbor_counts &lt;- indiv_sil_df %&gt;% \n  group_by(n_clust, cluster, neighbor) %&gt;% \n  count()  %&gt;% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c\"),\n         neighbor = str_replace(neighbor, \"Cluster_\", \"c\"))\n\nk6_neighbor_counts &lt;- neighbor_counts %&gt;% \n  filter(n_clust == 6)\n\n\nindiv_sil_df %&gt;% \n  mutate(across(.cols = c(cluster, neighbor),\n         ~str_replace(.x, \"Cluster_\", \"c\"))) %&gt;% \n  filter(n_clust == 6) %&gt;% \n  ggplot() + \n  geom_boxplot(aes(x = neighbor, y = sil_width)) +\n    geom_point(aes(x = neighbor, y = sil_width),\n             position = position_jitter(width = 0.1),\n             alpha = 0.2,\n             color = \"pink\") + \n  geom_text(data = k6_neighbor_counts,\n            aes(x = neighbor, y = 0.7, label = n),\n            color = \"blue\") +\n  facet_wrap(vars(cluster), scales = \"free_x\") +\n  theme_bw() +\n  ggtitle(\"k=6 silhouettes\")\n\n\n\n\n\nclust_sil_avgs &lt;- indiv_sil_df %&gt;% \n  group_by(cluster,\n           n_clust) %&gt;% \n  summarise(mean_sil = mean(sil_width),\n            sd_sil = sd(sil_width), \n            .groups = \"drop\")\n\nclust_sil_avgs %&gt;% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c0\"),\n         cluster = case_when(\n           cluster %in% c(\"c010\", \"c011\", \"c012\", \"c013\", \"c014\", \"c015\", \"c016\",\n                          \"c017\", \"c018\", \"c019\", \"c020\") ~ str_replace(cluster, \"c0\", \"c\"),\n           TRUE ~ cluster\n         )) %&gt;% \n  filter(n_clust %in% c(6:12)) %&gt;% \n  ggplot() +\n  geom_col(aes(y = cluster, x = mean_sil)) +\n  facet_wrap(vars(n_clust), scales = \"free_y\") + \n  ggtitle(\"Average silhouette width per cluster for k=6-12\") +\n  theme_bw()\n\n\n\n\n\n\n10.8.4 Not used: Gap statistic\nFor fviz_nbclust(), first couple times running this, got Warning: Quick-TRANSFER stage steps exceeded maximum… Looking online, this seems to be a problem with the model not converging. I added some arguments here that are passed on to kmeans(), to make sure that the algorithm settings here match what I run above, including set.seed()\nContinued to get warnings, even though I’m using all the same settings as I use for kmeans up above. Not sure why this is, but I’m not going to spend any more time on it right now. Maybe see if getting the gap statistic through NbClust works better? (Later note: NbClust won’t be a good option either, I can’t alter important kmeans() settings in NbClust). Expect it will take a long time either way, consider running this in a separate script and pulling in the results.\n\nset.seed(4)\nfviz_gap_stat(x = baked_df, \n             FUNcluster = kmeans,\n             method = c(\"gap_stat\"),\n             k.max = 10, # only considering 2-10 clusters\n             nboot = 50, # default is 100\n             verbose = TRUE, \n             iter.max = 20, # passed to kmeans\n             nstart = 10 # passed to kmeans\n             )\n\n\n\n10.8.5 Calinski-Harabasz index\nNot used: {NbClust} , using {fpc} instead.\n\nFor Calinski-Harabasz index, higher values are better\nRealized after setting this up with NbClust that I don’t have the option to pass additional arguments to the kmeans function here. So I can’t make the algorithm settings exactly match my main clustering pipeline above (where I implement k-means using tidyclust and the tidymodels framework, and where I save the results for further analysis). This is a problem because I know from my original tests that I need to change the iter.max value to avoid non-convergence issues, and I also want to change nstart because nstart &gt;1 is typically known to be best practice (find citation for this).\n\n\n# keeping this here as a record, but I\"m NOT USING this function for the C-H index. \n\nnbc_indices &lt;- NbClust::NbClust(data = baked_df,\n                 distance = \"euclidean\",\n                 method = \"kmeans\",\n                 min.nc = 2,\n                 max.nc = 20,\n                 index = \"ch\") # Calinski and Harabasz\n\n# enframe turns a named vector into a dataframe\nch_index_vals &lt;- enframe(nbc_indices$All.index) %&gt;% \n  mutate(name = as.integer(name)) %&gt;% \n  rename(n_clust = name)\n\nTrying a different implementation of the Calinski-Harabasz index from the {fpc} package. This is preferred to the above approach, where I originally used the NbClust function from {NbClust} package because I can give this function my clustering generated above (NbClust does its own run of kmeans but I can’t customize it to keep it consistent with\n\n# calinhara wants an observations/variables matrix\n# as first argument (as opposed to a distance matrix)\nobsvar_mx &lt;- as.matrix(baked_df)\n\n# function to extract and modify tidyclust clusters\n# into a integer vector, which I will pass to calinhara()\ncreate_clust_vec &lt;- function(fit_obj){\n  \n  extract_cluster_assignment(fit_obj) %&gt;% \n    pull(.cluster) %&gt;% \n    str_replace(., \"Cluster_\", \"\") %&gt;% \n    as.integer()\n  \n}\n\n# apply function to extract clusterings as integer vectors\n# map to get a c-h index value for every value of k (2-20)\nch_metrics &lt;- silh_df %&gt;%\n  select(n_clust, km_fit) %&gt;%\n  mutate(\n    clustering_vec = map(km_fit, create_clust_vec),\n    ch_index = map_dbl(clustering_vec,\n                       ~ fpc::calinhara(x = obsvar_mx,\n                                        clustering = .x)\n    ))\n\nch_metrics %&gt;% \n  ggplot(aes(x = n_clust, y = ch_index)) + \n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  ylab(\"Calinski-Harabasz index\") + \n  ggtitle(\"Calinski-Harabasz\") +\n  labs(subtitle = \"Higher is better\") +\n  scale_x_continuous(breaks = c(2:20))\n\n\n\n\n\n\n10.8.6 Hopkins Statistic\nUsing the {hopkins} package for this. Citations included in the package documentation (also cite Tan et al., 2019 who give an example of using this for evaluating kmeans clusters).\n\nHopkins, B. and Skellam, J.G., 1954. A new method for determining the type of distribution of plant individuals. Annals of Botany, 18(2), pp.213-227.\nCross, G. R., and A. K. Jain. (1982). Measurement of clustering tendency. Theory and Application of Digital Control. Pergamon, 1982. 315-320.\n\nAnd a third citation, helpful illustrations:\n\nLawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of Chemical Information and Computer Sciences, 30(1), 36–41. https://doi.org/10.1021/ci00065a010\n\nApparently {factoextra} also has a Hopkins statistic, try that here too. (It takes a very long time to run, but returns 0.93, similar to 0.99 returned by hopkins::hopkins()\n\nset.seed(4)\nhstat &lt;- hopkins(X = baked_df,\n                 # default, number of rows to sample from the df\n                 m = ceiling(nrow(baked_df)/10), \n                 # default, dimension of the data\n                 d = ncol(baked_df),\n                 # default, kth nearest neighbor to find\n                 k = 1) \nhstat\n\n[1] 0.9999999\n\nhopkins.pval(x = hstat,\n             # this is the default for hopkins() above\n             n = ceiling(nrow(baked_df)/10)) \n\n[1] 0\n\n# commenting out because it takes a very long time to run\n# factoextra::get_clust_tendency(data = baked_df,\n#                                n = 687, \n#                                graph = FALSE)\n\n\n\n10.8.7 WSS and Silhouette metrics on one plot\n\n#| echo: false\n\nsil_totwss &lt;- silh_df %&gt;% \n  select(n_clust, avg_sil, tot_wss, tot_sse, sse_ratio)\n\nch &lt;- ch_metrics %&gt;% \n  select(n_clust, ch_index)\n\nmet_combined &lt;- left_join(sil_totwss, ch, by = \"n_clust\")\n\nwrite_csv(met_combined, \"data/kmeans_cluster_metrics.csv\")\n\nmet2 &lt;- met_combined %&gt;% \n  pivot_longer(cols = -c('n_clust'), names_to = \"metric\",\n               values_to = \"value\")\n\nmet2 %&gt;% \n  ggplot(aes(x = n_clust, y = value)) + \n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(2:20)) + \n  facet_wrap(vars(metric), ncol = 1, scales = \"free\") +\n  theme_bw()"
  },
  {
    "objectID": "11-implement-kmeans.html#save-model-fits",
    "href": "11-implement-kmeans.html#save-model-fits",
    "title": "10  Implement k-means",
    "section": "10.9 Save model fits",
    "text": "10.9 Save model fits\nWill save these as Rdata so I can call them up and investigate the cluster centroids more closely in the next chapter.\n\nmods &lt;- silh_df %&gt;% \n  select(n_clust, km_fit)\n\nsave(mods, file = \"data/fitted_kmeans_mods.RData\")"
  },
  {
    "objectID": "11-implement-kmeans.html#save-cluster-assignments",
    "href": "11-implement-kmeans.html#save-cluster-assignments",
    "title": "10  Implement k-means",
    "section": "10.10 Save cluster assignments",
    "text": "10.10 Save cluster assignments\nFor each version of the model (each value of k, different numbers of clusters), a MUKEY is assigned to a specific cluster. Here, I’m pulling that data, shaping it into one dataframe (one row per MUKEY, cluster assingments in separate columns). I’m also adding back in the soil property data so we can use this in the next step when evaluating different cluster sizes.\n\nclust_assign_df &lt;- mods %&gt;% \n  mutate(clust_assign = map(km_fit, ~augment(.x, new_data = d)),\n         mukey_clust = map(clust_assign, ~select(.x, mukey, .pred_cluster)))\n\n\nassign_mukey_df &lt;- clust_assign_df %&gt;% \n  select(n_clust, mukey_clust) %&gt;% \n  unnest(mukey_clust) %&gt;% pivot_wider(names_from = n_clust, values_from = .pred_cluster, names_prefix = \"k_\")\n\n\nclust_props &lt;- full_join(d, assign_mukey_df, by = \"mukey\")\n\nwrite_csv(clust_props, \"data/mukey_cluster_assignments_and_soilprops.csv\")"
  },
  {
    "objectID": "12-explore-clust-props.html#extract-centroids",
    "href": "12-explore-clust-props.html#extract-centroids",
    "title": "11  Explore clusters",
    "section": "11.1 Extract Centroids",
    "text": "11.1 Extract Centroids\nRecall that these are still the standardized values.\n\ncent_df &lt;- mods %&gt;% \n  mutate(centroids = map(km_fit, ~extract_centroids(.x))) %&gt;% \n  select(n_clust, centroids) %&gt;% \n  unnest(centroids) # debated whether to also rescale here, as I did below for the radar plots? \n\n\nhead(cent_df)\n\n\n\n  \n\n\n# scaled 0-1 for radar plot\nsc_cent &lt;- cent_df %&gt;% \n  mutate(across(.cols = c(3:12),\n                   .fns = ~round(rescale(.x,to = c(0,1)), 1)))"
  },
  {
    "objectID": "12-explore-clust-props.html#about-individual-clusters",
    "href": "12-explore-clust-props.html#about-individual-clusters",
    "title": "11  Explore clusters",
    "section": "11.2 About individual clusters",
    "text": "11.2 About individual clusters\nWe can also extract information about how many members (unique MUKEYs) have been allocated to each cluster, and the sum of squared error within each cluster. This tells us about how how “cohesive” each cluster is (lower SSE means points are closer to their cluster centroid, more tightly packed).\n\niclust &lt;- mods %&gt;% \n  mutate(clust_stat = map(km_fit, ~within_cluster_sse(.x))) %&gt;% \n  select(n_clust, clust_stat) %&gt;% \n  unnest(clust_stat) %&gt;% \n  mutate(.cluster = str_replace_all(.cluster, \"er_\", \"\"))\n\nhead(iclust)"
  },
  {
    "objectID": "12-explore-clust-props.html#cluster-stats-overall",
    "href": "12-explore-clust-props.html#cluster-stats-overall",
    "title": "11  Explore clusters",
    "section": "11.3 Cluster Stats Overall",
    "text": "11.3 Cluster Stats Overall"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-split-the-difference",
    "href": "12-explore-clust-props.html#clusters-split-the-difference",
    "title": "11  Explore clusters",
    "section": "11.4 2 Clusters “Split the difference”",
    "text": "11.4 2 Clusters “Split the difference”\n\nCluster 1: sandy, high Ksat and low AWC. Low OM and clay.\nCluster 2: high clay, CEC, OM, AWC, LEP.\n\n\ncoord2 &lt;- parcoor_plot(nclust = 2)\nstat2 &lt;- plot_cluster_stats(2)\n\ncoord2 + stat2 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\ncreate_summary_table(\"k_2\")\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 2,3451\nCluster_2, N = 4,5271\n\n\n\n\nclaytotal\n10 (4)\n25 (8)\n\n\nom\n2.40 (1.30)\n5.84 (8.29)\n\n\ncec7\n10 (4)\n24 (16)\n\n\ndbthirdbar\n1.46 (0.10)\n1.30 (0.19)\n\n\nec\n0.02 (0.19)\n0.13 (0.42)\n\n\nph1to1h2o\n6.50 (0.51)\n7.01 (0.58)\n\n\ncaco3\n0.7 (2.1)\n4.0 (5.7)\n\n\nlep\n1.18 (0.50)\n3.08 (1.92)\n\n\nksat\n48 (36)\n9 (7)\n\n\nawc\n0.14 (0.04)\n0.20 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\nradar_plot(2)\n\n\n\n\n\nprop %&gt;% \n  select(k_2, claytotal, om, ph1to1h2o, caco3) %&gt;% \n  pivot_longer(-k_2) %&gt;% \nggplot() + \n  geom_violin(aes(x = name, y = value, color = k_2)) +\n  facet_wrap(vars(name), scales = \"free\")"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-distinguishing-nw",
    "href": "12-explore-clust-props.html#clusters-distinguishing-nw",
    "title": "11  Explore clusters",
    "section": "11.5 3 Clusters “Distinguishing NW?”",
    "text": "11.5 3 Clusters “Distinguishing NW?”\n\ncoord3 &lt;- parcoor_plot(nclust = 3)\nstat3 &lt;- plot_cluster_stats(3)\n\ncoord3 + stat3 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\nradar_plot(3)\n\n\n\n\n\ncreate_summary_table(\"k_3\")\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 2,0841\nCluster_2, N = 2,9671\nCluster_3, N = 1,8211\n\n\n\n\nclaytotal\n9 (4)\n24 (8)\n24 (9)\n\n\nom\n2.30 (1.21)\n5.82 (9.79)\n5.50 (4.01)\n\n\ncec7\n10 (4)\n25 (19)\n22 (10)\n\n\ndbthirdbar\n1.47 (0.10)\n1.31 (0.18)\n1.30 (0.18)\n\n\nec\n0.01 (0.12)\n0.04 (0.20)\n0.28 (0.60)\n\n\nph1to1h2o\n6.47 (0.49)\n6.62 (0.36)\n7.60 (0.30)\n\n\ncaco3\n0.4 (1.4)\n0.3 (0.9)\n9.9 (4.9)\n\n\nlep\n1.18 (0.48)\n2.99 (1.84)\n2.94 (2.05)\n\n\nksat\n52 (36)\n9 (6)\n11 (11)\n\n\nawc\n0.13 (0.03)\n0.21 (0.04)\n0.19 (0.03)\n\n\n\n1 Mean (SD)"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters",
    "href": "12-explore-clust-props.html#clusters",
    "title": "11  Explore clusters",
    "section": "11.6 4 Clusters",
    "text": "11.6 4 Clusters\n\ncoord4 &lt;- parcoor_plot(nclust = 4)\nstat4 &lt;- plot_cluster_stats(4)\n\ncoord4 + stat4 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\nradar_plot(4)"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-ec-artefacts-ec-om-clay-spread",
    "href": "12-explore-clust-props.html#clusters-ec-artefacts-ec-om-clay-spread",
    "title": "11  Explore clusters",
    "section": "11.7 5 Clusters “EC artefacts? EC-OM-Clay spread”",
    "text": "11.7 5 Clusters “EC artefacts? EC-OM-Clay spread”\n\ncoord5 &lt;- parcoor_plot(nclust = 5)\nstat5 &lt;- plot_cluster_stats(5)\n\ncoord5 + stat5 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\nI wonder if the difference between Cluster 1 and 5 here is actually being driven by the EC values we set to zero in the NW part of the state? It seems to be the only major thing distinguishing those 2 clusters.\n\nradar_plot(5)"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-histosols-break-free",
    "href": "12-explore-clust-props.html#clusters-histosols-break-free",
    "title": "11  Explore clusters",
    "section": "11.8 6 Clusters “Histosols break free”",
    "text": "11.8 6 Clusters “Histosols break free”\nWhat strikes me about this one is Cluster 2, it seems like those might be histosols that broke away into their own cluster? Cluster 2 very high OM, CEC, very low bulk density, high AWC.\n\ncoord6 &lt;- parcoor_plot(nclust = 6)\nstat6 &lt;- plot_cluster_stats(6)\n\ncoord6 + stat6 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\nradar_plot(6)\n\n\n\n\n\ncreate_summary_table(\"k_6\")\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 1,6601\nCluster_2, N = 2,2761\nCluster_3, N = 1,3761\nCluster_4, N = 1,1001\nCluster_5, N = 3931\nCluster_6, N = 671\n\n\n\n\nclaytotal\n9 (3)\n19 (5)\n23 (7)\n34 (7)\n22 (10)\n7 (4)\n\n\nom\n2.07 (1.11)\n3.58 (1.63)\n5.43 (3.13)\n5.99 (1.87)\n4.73 (1.38)\n68.17 (17.64)\n\n\ncec7\n9 (3)\n17 (4)\n21 (7)\n29 (6)\n19 (8)\n141 (29)\n\n\ndbthirdbar\n1.49 (0.09)\n1.36 (0.08)\n1.31 (0.20)\n1.27 (0.12)\n1.28 (0.09)\n0.30 (0.22)\n\n\nec\n0.01 (0.09)\n0.01 (0.07)\n0.00 (0.00)\n0.07 (0.26)\n1.35 (0.56)\n0.00 (0.00)\n\n\nph1to1h2o\n6.45 (0.50)\n6.55 (0.35)\n7.58 (0.30)\n6.81 (0.38)\n7.66 (0.37)\n6.57 (0.47)\n\n\ncaco3\n0.4 (1.4)\n0.3 (1.0)\n10.0 (4.9)\n0.8 (2.0)\n9.3 (5.8)\n1.4 (2.4)\n\n\nlep\n1.16 (0.47)\n1.96 (0.92)\n2.67 (1.38)\n5.04 (2.17)\n2.69 (2.09)\n0.59 (0.30)\n\n\nksat\n59 (37)\n13 (8)\n11 (9)\n4 (3)\n15 (16)\n24 (6)\n\n\nawc\n0.12 (0.03)\n0.20 (0.03)\n0.20 (0.02)\n0.19 (0.02)\n0.19 (0.03)\n0.39 (0.04)\n\n\n\n1 Mean (SD)"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-1",
    "href": "12-explore-clust-props.html#clusters-1",
    "title": "11  Explore clusters",
    "section": "11.9 7 Clusters",
    "text": "11.9 7 Clusters\n\ncoord7 &lt;- parcoor_plot(nclust = 7)\nstat7 &lt;- plot_cluster_stats(7)\n\ncoord7 + stat7 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\nradar_plot(7)"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-2",
    "href": "12-explore-clust-props.html#clusters-2",
    "title": "11  Explore clusters",
    "section": "11.10 8 Clusters",
    "text": "11.10 8 Clusters\n\ncoord8 &lt;- parcoor_plot(nclust = 8)\nstat8 &lt;- plot_cluster_stats(8)\n\ncoord8 + stat8 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\nradar_plot(8)"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-3",
    "href": "12-explore-clust-props.html#clusters-3",
    "title": "11  Explore clusters",
    "section": "11.11 9 Clusters",
    "text": "11.11 9 Clusters\n\ncoord9 &lt;- parcoor_plot(nclust = 9)\nstat9 &lt;- plot_cluster_stats(9)\n\ncoord9 + stat9 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\nradar_plot(9)"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-4",
    "href": "12-explore-clust-props.html#clusters-4",
    "title": "11  Explore clusters",
    "section": "11.12 10 Clusters",
    "text": "11.12 10 Clusters\n\ncoord10 &lt;- parcoor_plot(nclust = 10)\nstat10 &lt;- plot_cluster_stats(10)\n\ncoord10 + stat10 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\nradar_plot(10)"
  },
  {
    "objectID": "12-explore-clust-props.html#clusters-5",
    "href": "12-explore-clust-props.html#clusters-5",
    "title": "11  Explore clusters",
    "section": "11.13 11 Clusters",
    "text": "11.13 11 Clusters\n\ncoord11 &lt;- parcoor_plot(nclust = 11)\nstat11 &lt;- plot_cluster_stats(11)\n\ncoord11 + stat11 + plot_layout(widths = c(2,1), guides = \"collect\")\n\n\n\n\n\nradar_plot(11)"
  },
  {
    "objectID": "12-explore-clust-props.html#different-plot-example",
    "href": "12-explore-clust-props.html#different-plot-example",
    "title": "11  Explore clusters",
    "section": "11.14 Different plot example",
    "text": "11.14 Different plot example\nAnother way to compare soil properties between clusters. Only really works with a small number of clusters\n\n# the centroids are all on a standardized scale,\n# so we can plot all the vars together to compare\n# the properties of each cluster\ncent_df %&gt;%\n  filter(n_clust == 4) %&gt;% \n  select(-n_clust) %&gt;% \n  pivot_longer(cols = -.cluster,\n               names_to = 'variable',\n               values_to = 'value') %&gt;%\n  ggplot() +\n  geom_col(aes(x = value, y = variable)) +\n  facet_wrap(vars(.cluster), nrow = 1) +\n  geom_vline(xintercept = 0, color = \"blue\") +\n  expand_limits(y = 13.5) +\n  theme_bw()\n\n\n\n# +\n#   geom_label(data = clust_stat,\n#              aes(\n#                x = 0.25,\n#                y = 11.5,\n#                label = glue(\"n:{n_members}\")\n#              ),\n#              size = 2.5) +\n#   geom_label(data = clust_stat,\n#              aes(\n#                x = 0.25,\n#                y = 12.5,\n#                label = glue(\"wss:{round(wss,0)}\")\n#              ),\n#              size = 2.5) +\n#   theme_bw()"
  },
  {
    "objectID": "19-sample-points.html#validation-datasets",
    "href": "19-sample-points.html#validation-datasets",
    "title": "12  Sample validation points",
    "section": "12.1 Validation datasets",
    "text": "12.1 Validation datasets\nThere are 3 main validation datasets that I currently have access to:\n\nNRCS SHI project (Jess). Validation completed by Joe Brennan, saved in data &gt; validation data &gt; NCRS-SHI &gt; shi_site_allvar_validation_20221116.csv\nCIG field data\nNCSS data from KSSL. I completed the data cleaning and merging process with the script unqip_merge_ncss_data.R. The dataset (all props averaged to 0-20cm) is saved in data &gt; validation data &gt; NCSS-KSSL &gt; validation_ncss_kssl_0-20cm_aggr.csv\n\n\nlibrary(tidyverse)\nlibrary(terra)\n\n# points to validate\nncss_dat &lt;- read_csv(\"data/validation_data/NCSS-KSSL/validation_ncss_kssl_0-20cm_aggr.csv\")\n\n# ended up making these matrices so I can add ids as an \"attribute\" (atts) in the vect function \nncss_pts &lt;- ncss_dat %&gt;% \n  select(longitude_decimal_degrees, latitude_decimal_degrees) %&gt;% \n  rename(lon = longitude_decimal_degrees,\n         lat = latitude_decimal_degrees) %&gt;% \n  as.matrix()\n\ncig_dat &lt;- read_csv(\"../CIG/cig-main/cig_lab_data_all_20221129.csv\")\n\ncig_pts &lt;- cig_dat %&gt;% \n  select(lon, lat) %&gt;% \n  as.matrix()\n\n# I am using these dfs to see if setting sample ID as an attribute in the\n# SpatVector below will allow me to identify the points more readily after\n# extract()\ncig_ids &lt;- data.frame(sample_id = cig_dat$sample_id,\n                      row.names = NULL)\n\nncss_ids &lt;- data.frame(pedon_key = ncss_dat$pedon_key)"
  },
  {
    "objectID": "19-sample-points.html#reproject-points-to-nad-83-albers",
    "href": "19-sample-points.html#reproject-points-to-nad-83-albers",
    "title": "12  Sample validation points",
    "section": "12.2 Reproject points to NAD 83 Albers",
    "text": "12.2 Reproject points to NAD 83 Albers\nMy rasters are in NAD 1983 Albers projection (EPSG: 5070). My points are in WGS 84 (EPSG: 4326, they are lon/lat). I need to reproject the points before I extract the raster values. This involves two steps:\n\nTurn my 2-column dataframes (lon, lat) into SpatVector objects\nReproject the SpatVector objects to the correction CRS\n\nFrom the {terra} documentation: “You can use a data.frame to make a SpatVector of points; or a”geom” matrix to make a SpatVector of any supported geometry (see examples and geom)\n\n# turn the CIG and NCSS points dfs to SpatVectors\ncig_spat &lt;- vect(x = cig_pts,\n     type = \"points\",\n     # EPSG:4326 is WGS84 long/lat\n     crs = \"epsg:4326\",\n     atts = cig_ids\n     )\n\nncss_spat &lt;- vect(x = ncss_pts,\n                  type = \"points\",\n                  # EPSG:4326 is WGS84 long/lat\n                  crs = \"epsg:4326\",\n                  atts = ncss_ids)\n\n# reproject the CIG and NCSS points to NAD 83 (EPSG 5070)\ncig_reproj &lt;- project(x = cig_spat,\n                      y = \"epsg:5070\")\n\ncig_reproj\n\n class       : SpatVector \n geometry    : points \n dimensions  : 486, 1  (geometries, attributes)\n extent      : -83294.72, 263752.5, 2296221, 2843822  (xmin, xmax, ymin, ymax)\n coord. ref. : NAD83 / Conus Albers (EPSG:5070) \n names       : sample_id\n type        :     &lt;num&gt;\n values      :         1\n                       2\n                       3\n\nncss_reproj &lt;- project(x = ncss_spat,\n                    y = \"epsg:5070\")\n\nncss_reproj\n\n class       : SpatVector \n geometry    : points \n dimensions  : 49, 1  (geometries, attributes)\n extent      : -56529.96, 395663.8, 2374854, 2848762  (xmin, xmax, ymin, ymax)\n coord. ref. : NAD83 / Conus Albers (EPSG:5070) \n names       : pedon_key\n type        :     &lt;chr&gt;\n values      :   02N0123\n                 02N0796\n                 02N0797"
  },
  {
    "objectID": "19-sample-points.html#sample-rasters-extract-value",
    "href": "19-sample-points.html#sample-rasters-extract-value",
    "title": "12  Sample validation points",
    "section": "12.3 Sample Rasters (Extract value)",
    "text": "12.3 Sample Rasters (Extract value)\nNeed to review relevant function/s from {terra} for doing the raster sampling. Otherwise I know how to do it already in QGIS.\nLooks like I want the extract() function. “Sample” has a different meaning in {terra}, (taking a spatial sample / regular sample)\nImportant arguments:\n\nx is the SpatRaster\ny (in my case) is a SpatVector, re-projected above\nmethod should be “simple”, because I want the value of the cell the point falls into\nxy set to TRUE (return coordinates with results)\nID set to TRUE (return IDs, record numbers, of input SpatVector y)\n\n\n12.3.1 An example\nOK, got this working after some silly troubleshooting with attributes and CRS (had the wrong EPSG code for NAD 1983 Albers to start out).\n\n# start with the k=2 model\ntest_rast &lt;- rast(\"E:/big-files-backup/ch03-sh-groups/clust2_allvar.tif\")\n\next_df &lt;- extract(x = test_rast,\n        y = cig_reproj,\n        method = \"simple\",\n        xy = TRUE,\n        ID = TRUE)  \n\n# also try it with the NCSS points, just to see:\nextract(x = test_rast,\n        y = ncss_reproj,\n        method = \"simple\", \n        xy = TRUE,\n        ID = TRUE)\n\n\n\n  \n\n\n\n\n\n12.3.2 Brainstorm process to iterate with “extract”\nAgain, thinking about how I want to be able to set up the steps of this process in a {targets} pipeline, the meat of this process of extracting raster values will be done in a separate R script. Here I am thinking through how I would structure that.\n\nSetup: Like above, load the locations and sample IDs for the points I want to use in my validation. Combine the CIG and NCSS points into a single dataframe. Then do Data.frame &gt; matrix &gt; spat vector w/ ID attribute\nWrite a custom function that will:\n\nLoad a raster for a given value of k\nExtract raster values at our target points -&gt; this is a dataframe. Make sure the raster value column has an informative name that includes the k value so we can distinguish it\nRemove the raster (it’s big, we dont need it any more)\nReturn the dataframe\n\nUse map to run the custom function above, iterating over values of k from 2-20 and column-binding the results, so we end up with a dataframe that has rows = validation points, and columns = raster values for different k cluster sizes\n\n\n\n12.3.3 Finished raster extract process (CIG, NCSS)\nThe script R/extract_raster_values_at_validation_points.R implements the process described above. At the end of that script, I save a CSV file in wide format with the CIG and NCSS_KSSl validation data cig_ncss-kssl_allvar_validation_20221230.csv.\nThe next step is to combine the validation data, which identifies the cluster assignments, with the soil property data. I will do this in the next chapter."
  },
  {
    "objectID": "19-sample-points.html#number-points-per-dataset-soil-property",
    "href": "19-sample-points.html#number-points-per-dataset-soil-property",
    "title": "12  Sample validation points",
    "section": "12.4 Number points per dataset & soil property",
    "text": "12.4 Number points per dataset & soil property\nIn the manuscript, I’d like to report how many points came from each of our three datasets. Here is some code to determine this. The best place to start is the dataset that I prepped for pairwise comparisons using R/prep_validation_data_for_pairwise_comparisons.R. It has NA values removed and the CIG points have been reduced to independent points only.\n\nval_dat &lt;-  read_csv(\"data/validation_data_clusters_and_soil_props.csv\") %&gt;% \n  mutate(proj_id = case_when(\n    str_detect(val_unit_id, \"CC$\") ~ \"NRCS-SHI\",\n    str_detect(val_unit_id, \"[:digit:]{3}$\") ~ \"KSSL\",\n    TRUE ~ \"CIG\"\n  ))\n\nRows: 146 Columns: 34\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): val_unit_id\ndbl (33): k_10, k_11, k_12, k_13, k_14, k_15, k_16, k_17, k_18, k_19, k_2, k...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# number of validation pts per project\nval_dat %&gt;% \n  group_by(proj_id) %&gt;% \n  count()\n\n\n\n  \n\n\n# number of validation points overall\nnrow(val_dat)\n\n[1] 146\n\n\nOK, so how many points do we have for each of the soil properties we are using to calculate cluster means?\n\nval_dat %&gt;% \n  select(val_unit_id, claytotal, dbthirdbar, ph1to1h2o, caco3, om_loi) %&gt;% \n  summarise(across(.cols = -val_unit_id,\n                   .fns = \\(xcol) sum(!is.na(xcol))))"
  },
  {
    "objectID": "20-cluster-pairwise-comp.html#overview",
    "href": "20-cluster-pairwise-comp.html#overview",
    "title": "13  Cluster pairwise comparisons",
    "section": "13.1 Overview",
    "text": "13.1 Overview\nNow that I have the validation data extracted and combined with the soil property data, I can run pairwise comparisons between clusters for each value of k (different # of clusters). This may help us make an argument for why we chose one specific clustering (k=6 or k=8 for example) over others.\nAfter processing all the validation data, we ended up with ~140 points to include (from CIG, NRCS-SHI, and NCSS-KSSL combined). Devine et al. had 396 points. The first thing I want to do is see how the points are distributed across the clusters (and perhaps also on a map of MN) to assess if this is sufficient.\nI did identify a few sources for additional validation data, perhaps 20 additional points. Nothing huge, but could add if we want to strengthen the case. Notes in 2022-12-30 log entry about this."
  },
  {
    "objectID": "20-cluster-pairwise-comp.html#validation-points-per-cluster",
    "href": "20-cluster-pairwise-comp.html#validation-points-per-cluster",
    "title": "13  Cluster pairwise comparisons",
    "section": "13.2 Validation points per cluster",
    "text": "13.2 Validation points per cluster\nHere I am illustrating how many independent validation points we have for each cluster assignment, for the different model options from k = 2 - 20."
  },
  {
    "objectID": "20-cluster-pairwise-comp.html#soil-properties-for-pairwise-comparisons",
    "href": "20-cluster-pairwise-comp.html#soil-properties-for-pairwise-comparisons",
    "title": "13  Cluster pairwise comparisons",
    "section": "13.3 Soil properties for pairwise comparisons",
    "text": "13.3 Soil properties for pairwise comparisons\nBecause our validation data points are coming from different projects / datasets, we don’t have exactly the same variables from each one. This is a reminder of which variables exist in the three sources we used for validation points:\n\nKSSL : clay, bulk density, lep, awc, ec, cec, pH, carbonates, organic matter, (est org C)\nCIG: clay, bulk density, pH, carbonates, organic matter, (est org C)\nSHI: clay, bulk density, pH, organic matter\n\nIn summary: all three datasets include bulk density, pH, organic matter, and clay, and KSSL and CIG include carbonates. So I think it makes sense to focus on plotting and doing pairwise comparisons with these variables specifically"
  },
  {
    "objectID": "20-cluster-pairwise-comp.html#example-pairwise-om-k6",
    "href": "20-cluster-pairwise-comp.html#example-pairwise-om-k6",
    "title": "13  Cluster pairwise comparisons",
    "section": "13.4 Example pairwise: OM, k=6",
    "text": "13.4 Example pairwise: OM, k=6\nWorking out the steps I need to include in a function to do the pairwise comparisons. This first chunk shows how I would do it if using one-way ANOVA followed by\n\n# test case k=6 version and organic matter\n\ntest_dat &lt;- val_dat %&gt;% \n  select(val_unit_id, k_6, om_loi, claytotal, source) %&gt;% \n  drop_na(om_loi)\n\n# plot to see distributions \ntest_dat %&gt;% \n  ggplot(aes(x = k_6, y = om_loi)) + \n  geom_boxplot() + \n  geom_point() + \n  theme_bw()\n\n\n\ntest_lm &lt;- lm(formula = om_loi ~ k_6,\n   data = test_dat)\n\n# look at some diagnostic plots for our model \n# note homogeneity of variance looks sketchy\nperformance::check_model(test_lm, check = c(\"normality\", \"homogeneity\", \"linearity\"))\n\n\n\n\nThis shows how I would do pairwise t-tests with unpooled variance. This might be more appropriate than the ANOVA approach I originally tried, because the diagnostic plots above suggest that we don’t have homogeneity of variance for our validation dataset.\n“For some examples, one can use both the pooled t-procedure and the separate variances (non-pooled) t-procedure and obtain results that are close to each other. However, when the sample standard deviations are very different from each other, and the sample sizes are different, the separate variances 2-sample t-procedure is more reliable.” Penn State STAT 800 “Applied Research Methods” 5.6.1.2\nNote that the p-value adjustment method here is “holm”.\n\n# view the standard (console) output\nwith(test_dat, pairwise.t.test(om_loi, k_6, pool.sd = FALSE)) \n\n\n    Pairwise comparisons using t tests with non-pooled SD \n\ndata:  om_loi and k_6 \n\n        clust_1 clust_2 clust_3 clust_4\nclust_2 0.03925 -       -       -      \nclust_3 0.03313 0.93472 -       -      \nclust_4 0.00150 0.00062 0.93472 -      \nclust_5 0.01717 0.93472 0.93472 0.06359\n\nP value adjustment method: holm \n\n# tidy output, filter to significant comparisons only\nwith(test_dat, pairwise.t.test(om_loi, k_6, pool.sd = FALSE)) %&gt;% broom::tidy() %&gt;% \n  filter(p.value &lt; 0.05)"
  },
  {
    "objectID": "20-cluster-pairwise-comp.html#function-for-pairwise-comparisons",
    "href": "20-cluster-pairwise-comp.html#function-for-pairwise-comparisons",
    "title": "13  Cluster pairwise comparisons",
    "section": "13.5 Function for pairwise comparisons",
    "text": "13.5 Function for pairwise comparisons\nArguments: soil property and k option (model version / number of clusters) and validation dataframe.\nReturns: all pairwise comparisons in a dataframe\n\ncompare_clust_pairwise &lt;- function(soil_var, k_opt, df) {\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # can't do pairwise t-tests with only 1 obs in a \n  # group, so need to filter those out \n  \n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n  \n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;% \n    filter(n == 1) %&gt;% \n    pull(.data[[k_opt]])\n  \n  if(length(single_obs_clusters) == 0){\n    \n    dat_subset &lt;- dat_no_na\n    \n  }else{\n    \n    dat_subset &lt;- dat_no_na %&gt;% \n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n    \n  } \n  \n  soil_var_vec &lt;- dat_subset %&gt;% pull(soil_var)\n  clust_vec &lt;- dat_subset %&gt;% pull(k_opt)\n\n  pairs_df &lt;- pairwise.t.test(soil_var_vec,\n                              clust_vec,\n                              pool.sd = FALSE) %&gt;%\n    broom::tidy()\n\n  return(pairs_df)\n  \n}\n\nquiet_compare_clust_pairwise &lt;- purrr::quietly(.f = compare_clust_pairwise)"
  },
  {
    "objectID": "20-cluster-pairwise-comp.html#example-function-output",
    "href": "20-cluster-pairwise-comp.html#example-function-output",
    "title": "13  Cluster pairwise comparisons",
    "section": "13.6 Example function output",
    "text": "13.6 Example function output\nThis is what the pairwise function returns:\n\ncompare_clust_pairwise(soil_var = \"ph1to1h2o\",\n                       k_opt = \"k_10\",\n                       df = val_dat)"
  },
  {
    "objectID": "20-cluster-pairwise-comp.html#run-pairwise-comparisons",
    "href": "20-cluster-pairwise-comp.html#run-pairwise-comparisons",
    "title": "13  Cluster pairwise comparisons",
    "section": "13.7 Run pairwise comparisons",
    "text": "13.7 Run pairwise comparisons\nHere I create a dataframe to hold the results of the pairwise comparisons, then use map2() to iterate over the variables and cluster sizes, running all the tests.\n\n# vars to compare on \nvar_names &lt;- c(\"claytotal\", \"ph1to1h2o\", \"om_loi\", \"caco3\", \"dbthirdbar\")\n\n# all possible values of k (number of clusters)\ncluster_opts &lt;- glue(\"k_{2:20}\")\n\n# create df with all combinations of var_names x clusters\ncomp_template &lt;- tidyr::crossing(var_names, cluster_opts)\n\n# run the pairwise comparisons for each var and cluster size\ndiffs_df &lt;- comp_template %&gt;%\n  mutate(comps_all = map2(.x = var_names,\n                          .y = cluster_opts,\n                          .f = compare_clust_pairwise,\n                          df = val_dat))\n\nNow need to count how many of the tests have an adjusted p-value &lt; 0.05. All of the p-values are adjusted with the Tukey method.\n\ncount_sig_comps &lt;- function(df){\n  \n  df %&gt;% \n    filter(p.value&lt;0.05) %&gt;% \n    nrow()\n  \n}\n\nsig_diffs_df &lt;- diffs_df %&gt;%\n  mutate(n_sig_pair_diffs = map_int(comps_all, count_sig_comps)) %&gt;% select(-comps_all) %&gt;%\n  mutate(num_regions = as.numeric(str_extract(cluster_opts, \"[:digit:]+\")),\n         possible_comps = (num_regions * (num_regions - 1)) / 2)\n\nwrite_csv(sig_diffs_df, file = \"data/original_pairwise_comparisons.csv\")"
  },
  {
    "objectID": "20-cluster-pairwise-comp.html#plot-comparisons",
    "href": "20-cluster-pairwise-comp.html#plot-comparisons",
    "title": "13  Cluster pairwise comparisons",
    "section": "13.8 Plot comparisons",
    "text": "13.8 Plot comparisons\n\n13.8.1 All clusters\nFirst we can look at a plot of all the cluster sizes, from 2-20. For carbonates, bulk density, and organic matter, this plot seems to level off around k=11. For clay and pH, we continue to see an increase in the number of significant contrasts beyond k=11. This pattern is more apparent when looking at the smoothed trends in the second plot here.\n\n\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nFor context, this plot shows a black line for the total number of possible contrasts\n\n\n\n\n\nTwo other ways to contextualize the number of significant contrasts: 1) with a table, and 2) with a plot showing how the % significant contrasts (as a function of total possible) changes as the number of clusters goes up.\nFor the % significant contrasts, we see local maxima at k = 8 and k = 11, although the one at k = 11 is slightly smaller.\n\n\n\n\n  \n\n\n\n\n\n\n\n\n13.8.2 k = 5-15\nLet’s look more closely at our model options in the middle range, from 5-15.\n\n\n\n\n\nDropping the “total possible” line makes marginal changes from one model to another easier to see (below). What I notice about the graph below:\n\nIncrease in number of significant contrasts from k=7 to k=8, driven by clay, ph, bulk density\nNo sig contrasts added from k=8 to k=9, and only a couple added from bulk density from k=9 to k=10\nFrom k=10 to k=11. we add significant contrasts in pH, bulk density, organic matter, and clay\nfrom k=11 to k=12, big jump in contrasts driven by pH\n\n\n\n\n\n\n\n\n\n\n\n13.8.3 Clay and OM contrasts for k = 7-12\nFrom the plots above, we’ve learned that differences in clay and organic matter are primarily responsible for the significant contrasts we see being added from the k=7 model up to the k=12 model. In particular, we know that the move from the k=10 model to k=11 model results in an increased % of significant contrasts in these two variables (see immediately preceding plot)\nThe table below illustrates the number of significant contrasts for each model version in the k=7-12 range, for clay and organic matter. Note that there are 10 significant contrasts added for clay when going from k=10 to k=11.\nI was curious to know why this was, so I looked at some of the alluvial plots in _refs. The dimensions of these make it hard to display them legibly in HTML format, but looking at alluvial_k8-11.pdf , I can see that the new cluster added in k_11 is a relatively small (n=200 mapunits) group of the highest clay soils which also have EC ~1 and 3% carbonates. This is also illustrated in the k=11 map grids in the folllowing chapter."
  },
  {
    "objectID": "21-map-indiv-regions.html#overview",
    "href": "21-map-indiv-regions.html#overview",
    "title": "14  Mapping individual clusters",
    "section": "14.1 Overview",
    "text": "14.1 Overview\nI’ve been wanting to make some maps that show the distribution of individual clusters/regions throughout the state of MN, for different “clusterings” (versions of the model, different values of k).\nHere I’m going to try doing that, using some similar code to what I used in Chapter 8 to highlight where values of LEP and EC were changed during our data prep/lumping."
  },
  {
    "objectID": "21-map-indiv-regions.html#aoi-area-original-final",
    "href": "21-map-indiv-regions.html#aoi-area-original-final",
    "title": "14  Mapping individual clusters",
    "section": "14.2 AOI Area: Original & Final",
    "text": "14.2 AOI Area: Original & Final\nWant to include in my results the overall area of my AOI, after the data cleaning. Probably the easiest way to do this is to get the included MUKEYs, then subset my original MUKEY crosswalk, which included a count of the raster cells that fell under each MUKEY (can multiply the number of raster cells by area of cell to get area of AOI).\nRecall that each raster cell is 10 m^2\n\n# created in ch11 \"implement k-means\"\n# ONLY included MUKEYs n=6872\nincl_mukeys_df &lt;- read_csv(\"data/mukey_cluster_assignments_and_soilprops.csv\")\n\n# original AOI list of MUKEYs (short & original MUKEY names)\n# ALL MUKEYs in AOI = 7862\nall_aoi_mukeys &lt;- read.delim(\"data/gSSURGO_MN/mukey_new_crosswalk.txt\", sep = \",\") %&gt;% \n  select(MUKEY, MUKEY_New, Count)\n\n# percent MUKEYs from original AOI retained (others dropped due to lack of data / not relevant area (pits, rock outrcrops))\nround((6872/7862)*100, digits = 1)\n\n[1] 87.4\n\n\nOK, first let’s calculate the total area in the AOI before we dropped any MUKEYs due to insufficient data, etc.\n\n# 7862 MUKEYs to start\norig_cells &lt;- sum(all_aoi_mukeys$Count)\norig_cells\n\n[1] 1212251517\n\n# calculate hectares: cell * (10 m2/1 cell) * (1 hectare/10000 m2)\n\norig_m2 &lt;- orig_cells * 10 \n\norig_m2/10000 # hectares (original AOI)\n\n[1] 1212252\n\n\nNow calculate only the MUKEYs / area included in the final analysis after data cleaning, etc.\n\n# n=6872\nmukeys_final_incl &lt;- incl_mukeys_df$mukey\n\nfinal_incl_cells &lt;-  all_aoi_mukeys %&gt;% \n  filter(MUKEY %in% mukeys_final_incl) %&gt;% \n  pull(Count) %&gt;% \n  sum()\n\n# calculate hectares: cell * (10 m2/1 cell) * (1 hectare/10000 m2)\n\nfinal_m2 &lt;- final_incl_cells * 10\n\nfinal_m2/10000 # hectares\n\n[1] 1090532\n\n# percent area in final (relative to original AOI, before data\n# preparation)\n\nround((1090532/1212252)*100, digits = 1)\n\n[1] 90"
  },
  {
    "objectID": "21-map-indiv-regions.html#k6",
    "href": "21-map-indiv-regions.html#k6",
    "title": "14  Mapping individual clusters",
    "section": "14.3 k=6",
    "text": "14.3 k=6"
  },
  {
    "objectID": "21-map-indiv-regions.html#k7",
    "href": "21-map-indiv-regions.html#k7",
    "title": "14  Mapping individual clusters",
    "section": "14.4 k=7",
    "text": "14.4 k=7\nGoing from k=6 to k=7, the main difference is that some members of Cluster 1 (coarsest, slightly acid, low OM) and Cluster 2 (Loamy/coarse-loamy, neutral to sl. acid, mod OM) in the k=6 model split off to form a third cluster with intermediate characteristics in the k=7 model. In the k=7 model, this is Cluster 2 (Coarse-loamy, neutral-sl. acid, low OM)"
  },
  {
    "objectID": "21-map-indiv-regions.html#k8",
    "href": "21-map-indiv-regions.html#k8",
    "title": "14  Mapping individual clusters",
    "section": "14.5 k=8",
    "text": "14.5 k=8\nGoing from k=7 to k=8, the main difference is a split in Cluster 5 (Loamy, slightly alkaline, mod-high OM, high CaCO3) from the k=7 model. This cluster splits into 2 groups for the k=8 model: Cluster 4 (Loamy-clayey, slightly alkaline, highest OM, high CaCO3) and Cluster 7 (Loamy, slightly alkaline, low-mod OM, high CaCO3)."
  },
  {
    "objectID": "21-map-indiv-regions.html#k9",
    "href": "21-map-indiv-regions.html#k9",
    "title": "14  Mapping individual clusters",
    "section": "14.6 k=9",
    "text": "14.6 k=9\nGoing from k=8 to k=9, we have a very small number of mapunits (n=81) break away from Cluster 3 in the k-8 model (coarse-loamy, neutral-slightly acid, low-mod OM) to become Cluster 9 in the k=9 model (Coarse-loamy, neutral-slightly acid, low OM).\nLooking at the maps below, mapunits belonging to Cluster 9 are isolated to the SE corner of the state. Some artefacts are visible on the map, it looks like perhaps the Olmsted county is producing the rectangular boundary pattern we are seeing?\nThe 4 most common geomorphic descriptions for mapunits in this cluster are: loess hills, valley sides, valley sides on loess hills\nThe top 4 mapunit names are:\n\nDowns-Nasset complex, sinkhole karst, 2 to 6 percent slopes\nBarremills silt loam, drainageway, 1 to 5 percent slopes, occasionally flooded\nTama-Dinsmore complex, 2 to 6 percent slopes\nFayette-Pepin complex, sinkhole karst, 6 to 12 percent slopes, moderately eroded"
  },
  {
    "objectID": "21-map-indiv-regions.html#k10",
    "href": "21-map-indiv-regions.html#k10",
    "title": "14  Mapping individual clusters",
    "section": "14.7 k=10",
    "text": "14.7 k=10\nGoing from k=9 to k=10, we have a new cluster appear in the coarse texture category, Cluster 2 (Coarse, neutral, low OM, 1.3% CaCO3). Most of the mapunits in this cluster came from Cluster 1 in the k=9 model (Coarsest, lowest OM, sl. acid, 0.3% CaCO3) but a few came from Cluster 2 (Coarse-loamy, neutral-slightly acid, low OM, 0.3% CaCO3)."
  },
  {
    "objectID": "21-map-indiv-regions.html#k11",
    "href": "21-map-indiv-regions.html#k11",
    "title": "14  Mapping individual clusters",
    "section": "14.8 k=11",
    "text": "14.8 k=11\nGoing from k=10 to k=11, we see a new cluster at the highest clay level created. It has 204 unique MUKEYs, but appears to cover a relatively large area concentrated in the Red River Valley. This is Cluster 7 in the k=11 model (Highest clay, neutral-slightly alkaline, moderate OM, 3% CaCO3, EC=1). It is comprised of members from Cluster 6 (Highest clay, neutral, high OM) and Cluster 7 (Loamy, slightly alkaline, mod-high OM, 9.3% CaCO3, detect EC) in the k=10 model."
  },
  {
    "objectID": "22-pca-kmeans.html#overview",
    "href": "22-pca-kmeans.html#overview",
    "title": "15  Do PCA before k-means",
    "section": "15.1 Overview",
    "text": "15.1 Overview\nAfter lots of reading during the drafting phase of methods & results for my original k-means analysis, I found many authors in soil science, climate/atomspheric science, and geochemistry who use principal components analysis (PCA) for data reduction prior to doing k-means. The argument for doing this is that when you have highly correlated variables (which we do, see ?sec-corr ), including them all basically gives more weight to the correlated variables. If they are highly correlated, they contain most of the same “information”, and by including both variables you are giving that “information” more weight in the calculation of (dis)similarity matrix (Euclidean distances) that we ultimately use for clustering.\nMy plan is to do PCA, run k-means again, and compare the results with my original k-means analysis. I think this would be a valuable thing to add to my paper, as we can make an argument for whether one method or another might be more generalizable for others who want to use this technique for creating similar conceptual clusters from a regional set of soil data (presumably with a slightly different set of variables given the specific context - think about depth to restrictive horizon in Devine vs. carbonates in my analysis)."
  },
  {
    "objectID": "22-pca-kmeans.html#implementation",
    "href": "22-pca-kmeans.html#implementation",
    "title": "15  Do PCA before k-means",
    "section": "15.2 Implementation",
    "text": "15.2 Implementation\nThere is an easy way to do PCA as a pre-processing step in the {tidymodels} framework I’ve been doing using the step_pca() function. Under the hood, this uses stats::prcomp() to do the PCA.\nMuch of this code will be similar to Chapter 10, but with some additional exploratory plots as I think about how to incorporate this information in my manuscript."
  },
  {
    "objectID": "22-pca-kmeans.html#setup",
    "href": "22-pca-kmeans.html#setup",
    "title": "15  Do PCA before k-means",
    "section": "15.3 Setup",
    "text": "15.3 Setup\n\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glue)\nlibrary(tidyclust)\nlibrary(factoextra) # trying fviz_nbclust(), which gives elbow, silhouette, and gap statistic\nlibrary(hopkins)\nlibrary(fpc)\nlibrary(ggforce)\nlibrary(gt)\n\n\nd &lt;- readr::read_csv(\"./data/clean_mu_weighted_soil_props.csv\") %&gt;% \n  select(-contains(\"comp_pct\"))\n\nold_names &lt;- colnames(d)\n\nnew_names &lt;- stringr::str_replace_all(old_names, \"_r_value\", \"\")\n\ncolnames(d) &lt;- new_names\n\n# this is a dataframe of the results from my first k-means\n# run using same methods as Devine et al. Loading it so I \n# can use the colors in my PCA plots to get a qualitative \n# sense of whether we \"see\" similar clusters after PCA\nclust_membership &lt;- readr::read_csv(\"data/mukey_cluster_assignments_and_soilprops.csv\") %&gt;% \n  select(mukey, k_6)\n\nd &lt;- dplyr::left_join(d, clust_membership, by = \"mukey\")\n\nReminder of what the data look like:\n\nhead(d)\n\n\n\n  \n\n\n\nReminder of the transformations I chose to improve variable distributions and make them normal-ish.\n\nSquare root: clay, carbonates\nLog10: organic matter, cec, lep, ksat, awc\nCube (^3): bulk density\nNone: ec, ph"
  },
  {
    "objectID": "22-pca-kmeans.html#pre-process-data-build-recipe",
    "href": "22-pca-kmeans.html#pre-process-data-build-recipe",
    "title": "15  Do PCA before k-means",
    "section": "15.4 Pre-process data (build recipe)",
    "text": "15.4 Pre-process data (build recipe)\n\napply some transformations to achieve more normal distributions,\nthen standardize (step_normalize) by subtracting the mean and dividing by 1 sd\nthen PCA\n\n\nrec_spec &lt;- recipe(~., data = d) %&gt;% \n    update_role(mukey, new_role = \"ID\") %&gt;% \n    update_role(k_6, new_role = \"cluster_k6\") %&gt;% \n  # note this is log10 (the default is ln)\n    step_log(om, cec7, ksat, awc, lep, base = 10) %&gt;% \n    step_mutate(dbthirdbar = dbthirdbar^3) %&gt;% \n    step_sqrt(claytotal, caco3) %&gt;% \n    step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors(), num_comp = 10)\n\nrec_spec\n\nRecipe\n\nInputs:\n\n       role #variables\n cluster_k6          1\n         ID          1\n  predictor         10\n\nOperations:\n\nLog transformation on om, cec7, ksat, awc, lep\nVariable mutation for dbthirdbar^3\nSquare root transformation on claytotal, caco3\nCentering and scaling for all_numeric_predictors()\nPCA extraction with all_numeric_predictors()"
  },
  {
    "objectID": "22-pca-kmeans.html#functions-to-run-pca-visualize-results",
    "href": "22-pca-kmeans.html#functions-to-run-pca-visualize-results",
    "title": "15  Do PCA before k-means",
    "section": "15.5 Functions to run PCA, visualize results",
    "text": "15.5 Functions to run PCA, visualize results\nThis is modified from the “Tidy models with R” book, section 16.5 “Feature Extraction Techniques”.\nI removed the “dat” argument because I’m using the dataset that is already in my recipe. I also set new_data = NULL in bake as a reminder of this. In prep, it is default to have retain=TRUE, but again I’m explicitly typing it as a reminder to myself of what the defaults / where the data is coming from.\n\nrun_pca &lt;- function(recipe){\n  \n  recipe %&gt;% \n    # here, prep estimates the added PCA step\n    prep(retain = TRUE) %&gt;%\n    # Process the data (new_data=NULL means use data in recipe)\n    bake(new_data = NULL) \n  \n  \n}\n\n# Create the scatterplot matrix\nplot_validation_results &lt;- function(pca_df) {\n  \n  pca_df %&gt;%\n    ggplot(aes(\n      x = .panel_x,\n      y = .panel_y,\n      color = k_6,\n      fill = k_6\n    )) +\n    geom_point(alpha = 0.4, size = 0.5) +\n    geom_autodensity(alpha = .3) +\n    facet_matrix(vars(-c(mukey, k_6)), layer.diag = 2) +\n    scale_color_brewer(palette = \"Dark2\") +\n    scale_fill_brewer(palette = \"Dark2\")\n  \n}"
  },
  {
    "objectID": "22-pca-kmeans.html#principal-components-analysis-pca",
    "href": "22-pca-kmeans.html#principal-components-analysis-pca",
    "title": "15  Do PCA before k-means",
    "section": "15.6 Principal Components Analysis (PCA)",
    "text": "15.6 Principal Components Analysis (PCA)\n\n15.6.1 Visualize PCA results\n\npca_results &lt;- run_pca(rec_spec)\n\nwrite_csv(pca_results, \"data/pca_scores.csv\")\n\n# just plotting the first 5 PCs so we can actually seem them\n\npca_results %&gt;% \nselect(mukey, k_6, PC01, PC02, PC03, PC04, PC05, PC06) %&gt;% \nplot_validation_results() +\n  ggtitle(\"PCA pairwise plots: colors indicate k_6 clusters from original k-means\")\n\n\n\n\n\n\n15.6.2 Extract loadings\nTook me FOREVER to figure out why I wasn’t able to extract my loadings with the example code from the step_pca documentation here using tidy(prepped_rec, number = 2, type = \"coef\"). After much frustration, I figured out that the “number” argument needs to correspond to the PCA step in the tidied dataframe of my prepped recipe… in all the documentation this number is 2, so I kept trying that and getting my bulk density mutation step. In my case, the number is 5.\n\npca_prep &lt;- prep(rec_spec)\n\n# 'type = \"coef\"' here gets variables loadings per component\npca_loadings &lt;- tidy(pca_prep, 5, type = \"coef\")\n\nwrite_csv(pca_loadings, \"data/pca_loadings.csv\")\n\n\n\n\n\n\n\nltab &lt;- loadings_dat %&gt;% \n  select(terms, value, component) %&gt;% \n  pivot_wider(names_from = component, \n              values_from = value,\n              names_prefix = \"PC\") %&gt;% \n  gt(rowname_col = \"terms\") %&gt;%\n  tab_stubhead(label = \"term\") %&gt;% \n  fmt_number(\n    columns  = contains(\"PC\"),\n    decimals = 2\n  ) %&gt;% \n  sub_missing() %&gt;% \n  tab_header(title = \"PC Loadings\")\n\ngtsave(ltab, filename = \"figs/pc_loadings_table.docx\")\n\n\n\n15.6.3 Extract variance\nNote the number of rows in the pca_var dataframe: 40. We have 4 different terms calculated for each component:\n\nvariance\ncumulative variance\npercent variance\ncumulative percent variance\n\n\npca_var &lt;- tidy(pca_prep, 5, type = \"variance\")\n\nwrite_csv(pca_var, \"data/pca_variance.csv\")\n\nFirst we can look at % variance explained by each component:\n\npca_var %&gt;% \n  filter(terms == \"percent variance\") %&gt;% \n  ggplot(aes(x = component, y = value)) +\n  geom_col() + \n  geom_text(aes(x = component, y = value+2, label = round(value, 1)), color = \"red\") +\n  theme_bw() +\n  ylab(\"Percent variation explained\") +\n  scale_x_continuous(breaks = c(1:10))\n\n\n\n\nNext, we can look at cumulative % explained\n\npca_var %&gt;% \n  filter(terms == \"cumulative percent variance\") %&gt;% \n  ggplot(aes(x = component, y = value)) +\n  geom_col() + \n  geom_text(aes(x = component, y = value+2, label = round(value, 1)), color = \"red\") +\n  theme_bw() +\n  ylab(\"Cumulative percent variation explained\") +\n  scale_x_continuous(breaks = c(1:10))\n\n\n\n\nAnd now a more traditional scree plot:\n\npca_var %&gt;% \n  filter(terms == \"variance\") %&gt;% \n  ggplot(aes(x = component, y = value)) +\n  geom_point() + \n  geom_line() + \n  scale_x_continuous(breaks = c(1:10)) +\n  theme_minimal() +\n  ylab(\"Variance (Eigenvalue?)\")\n\n\n\n\n\n\n15.6.4 Select PCs to keep\nFor this round, I am keeping the first 5 PCs. This will always be a somewhat subjective decision, something noted by both Jolliffe & Cadima (2016) and the helpfully detailed climate zone paper by Fovell & Fovell 1993 that walks through their process and comparison of 3 vs. 5 PCs for their modeling scenario.\nKeeping the first 5 PCs accounts for 90% of the variation in my dataset. It also happens to be the number at which all of my original variables have been loaded on at least one PC (PC5 is the first time we see AWC loaded at all).\nMight be interesting, like Fovell & Fovell (1993), to do an additional version with 7 PCs. That gets us at &gt;95% variation accounted for.\n\ndat_for_kmeans &lt;- pca_results %&gt;% \n  select(mukey, PC01, PC02, PC03, PC04, PC05)"
  },
  {
    "objectID": "22-pca-kmeans.html#sec-mod-opt",
    "href": "22-pca-kmeans.html#sec-mod-opt",
    "title": "15  Do PCA before k-means",
    "section": "15.7 Set model options",
    "text": "15.7 Set model options\n\n# writing a custom function here so I can be explicit \n# about the options I'm choosing, and also use within the \n# list-col framework I set up with map() below. \nkm_spec &lt;- function(nclust){\n  \n  tidyclust::k_means(num_clusters = nclust) %&gt;%\n    parsnip::set_engine(engine = \"stats\",\n               nstart = 10, # 1 is default, &gt;1 recommended\n               algorithm = \"Hartigan-Wong\", # H-W is default\n               iter.max = 20) # default is 10, wasn't always enough\n  \n}"
  },
  {
    "objectID": "22-pca-kmeans.html#set-up-data-structure-and-kmeans-recipe",
    "href": "22-pca-kmeans.html#set-up-data-structure-and-kmeans-recipe",
    "title": "15  Do PCA before k-means",
    "section": "15.8 Set up data structure and kmeans recipe",
    "text": "15.8 Set up data structure and kmeans recipe\nHere I set up a dataframe that will catch my modeling results in list columns of the different model objects and return values. The first column I define specifies the range of different cluster sizes (k) that we will try.\nI also set up my recipe here, which is much simpler compared to the original version because all of my data has been processed already before running it through k-means. Per the reading I did on 2023-01-11, especially Green & Krieger (1995) and Schaffer & Green (1998), I’m not further standardizing my component scores.\n\ntry_clusts &lt;- c(2:20)\n\nkm_df &lt;- data.frame(n_clust = try_clusts)\n\nkm_rec &lt;- recipe(~., data = dat_for_kmeans) %&gt;% \n  update_role(mukey, new_role = \"ID\")\n\nkm_rec\n\nRecipe\n\nInputs:\n\n      role #variables\n        ID          1\n predictor          5"
  },
  {
    "objectID": "22-pca-kmeans.html#specify-model-for-each-value-of-k",
    "href": "22-pca-kmeans.html#specify-model-for-each-value-of-k",
    "title": "15  Do PCA before k-means",
    "section": "15.9 Specify model (for each value of k)",
    "text": "15.9 Specify model (for each value of k)\nFor each unique value of k (2-20), this returns a model specification object (in the kmeans_spec column) based on the custom function I wrote above. The model specification has all the options set about how we want the algorithm to run (methods, number of starts, etc.). We need a different one for each value of k.\nThe kmeans_wflow column here holds our workflow objects. These objects combine our model specification (from kmeans_spec) with the data recipe (preprocessor) we made above (rec_spec, is same for all models).\n\n# for each unique value of clusters (2:20), returns a model\n# specification (kmeans_spec) and a workflow (kmeans_wflow) \n# note that the workflow \nkm_df &lt;- km_df %&gt;%\n  mutate(\n    kmeans_spec = map(n_clust, ~ km_spec(nclust = .x)),\n    kmeans_wflow = map(kmeans_spec,\n                       ~ workflow(\n                         preprocessor = km_rec, spec = .x\n                       ))\n  )\n\n# our current data structure\nhead(km_df, n=3L )\n\n\n\n  \n\n\n# take a look at an example workflow\nkm_df$kmeans_wflow[3]\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = nclust\n\nEngine-Specific Arguments:\n  nstart = 10\n  algorithm = Hartigan-Wong\n  iter.max = 20\n\nComputational engine: stats"
  },
  {
    "objectID": "22-pca-kmeans.html#fit-the-models",
    "href": "22-pca-kmeans.html#fit-the-models",
    "title": "15  Do PCA before k-means",
    "section": "15.10 Fit the models",
    "text": "15.10 Fit the models\nAll the steps above were related to specifying different aspects of this model. Now we can actually fit the models.\nSome troubleshooting here:\n\nStarted by specifying tidyclust::fit() but something weird was happening where my step_normalize() wasn’t included in the pre-processor recipe when I looked at the fitted model object.\nIf I specify parsnip::fit() , then step_normalize() is included and the values of the cluster centroids are in the expected ranges (centered, scaled).\nI also tried this without explicitly specifying the package (so just fit() ) and it worked as expected.\n\n\n# make a quiet version of fit(), so we can capture results \n# and any warning messages from the models \n# see troubleshooting notes below\nquiet_fit &lt;- purrr::quietly(.f = parsnip::fit)\n\nset.seed(4) # for reproducibility \nkm_fit_df &lt;- km_df %&gt;%\n  mutate(km_result = map(.x = kmeans_wflow,\n                      .f = quiet_fit,\n       # data comes after .f b/c not vectorized over            \n                      data = dat_for_kmeans),\n       km_fit = map(km_result, ~pluck(.x, 'result')),\n       warn = map(km_fit, ~pluck(.x, 'warnings')),\n       msg = map(km_fit, ~pluck(.x, 'messages')),\n       n_iter = map_dbl(km_fit, \n                      ~pluck(.x, 'fit', 'fit', 'fit', 'iter' ))) \n         \n\n# check out current data structure\nhead(km_fit_df, n = 3L)\n\n\n\n  \n\n\n# don't need anymore, cleaning up\nrm(km_df)\n\n\n15.10.1 View messages & warnings\nWe can look at any warnings or messages from the modeling process:\n\nkm_fit_df %&gt;% \n  select(n_clust, warn, msg, n_iter)\n\n\n\n  \n\n\n\n\n\n15.10.2 Look at one fit object\nAs an example, these are what the fitted objects look like.\nNOTE the clustering vector here is using the cluster numbers directly from kmeans(). tidyclust assigns names like “Cluster_1”, “Cluster_2” etc. , but the numbers do NOT necessarily match with what kmeans() returns. The CLUSTERINGS are the same, but the numbers are not necessarily so. So 2 in this “Clustering Vector” below is NOT necessarily equal to tidyclust “Cluster_2” that you might get by using the extract_cluster_assignment function. To keep things consistent, I’m always using the cluster names assigned by tidyclust.\n\nexamp_fit &lt;- km_fit_df$km_result[[4]][['result']]\n\nexamp_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-means clustering with 5 clusters of sizes 1463, 431, 1809, 1096, 2073\n\nCluster means:\n        PC01       PC02        PC03         PC04       PC05\n1  1.5136139 -1.1675498 -0.04287291  1.011715005 -0.1041855\n2  1.7443425 -2.6232190 -0.74284903 -2.312509274 -0.1599810\n3  1.6349352  1.1528420 -0.02833620 -0.175076730  0.2152876\n4 -3.7357281 -0.3166354 -0.15275027  0.001684125  0.4859401\n5 -0.8825222  0.5307642  0.29019078 -0.081321539 -0.3379982\n\nClustering vector:\n   [1] 4 4 4 5 5 4 4 4 4 4 5 4 4 5 4 3 5 5 4 4 5 5 4 4 5 4 4 4 4 4 5 5 4 4 4 4 4\n  [38] 5 5 5 4 4 4 4 4 5 4 4 4 4 4 5 5 5 3 1 1 5 3 3 3 3 3 2 4 4 4 4 1 1 3 3 4 4\n  [75] 1 4 1 5 5 5 3 3 3 2 1 4 4 4 4 4 4 4 3 4 4 1 5 4 4 4 3 3 1 1 1 1 4 4 5 5 5\n [112] 4 4 4 3 1 4 4 4 4 1 3 3 3 3 4 4 4 4 5 5 4 4 4 4 5 4 5 1 5 4 4 4 4 4 4 1 4\n [149] 4 4 1 5 5 3 2 5 2 4 5 5 5 4 1 1 1 1 5 5 4 4 4 5 5 4 4 5 5 1 1 1 5 3 5 5 3\n [186] 1 1 3 3 3 5 5 1 3 5 5 5 5 4 4 4 4 4 4 4 5 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1\n [223] 1 3 4 4 3 3 3 1 4 4 4 3 3 4 1 5 3 3 4 5 1 1 5 3 4 3 3 2 4 4 4 5 2 5 4 4 1\n [260] 4 1 4 4 4 4 5 4 4 5 4 4 4 4 4 4 4 4 5 5 5 5 4 4 5 5 4 4 4 5 4 4 4 4 5 5 5\n [297] 5 5 4 4 4 4 5 4 5 4 4 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4\n [334] 5 5 5 2 3 1 1 2 2 2 2 1 2 1 1 3 3 1 1 3 2 1 3 3 1 1 2 5 1 3 1 1 5 2 1 1 3\n [371] 5 5 3 5 5 3 3 2 5 3 4 4 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 1 3 3 3 1 1 1\n [408] 1 1 3 5 5 3 3 3 3 5 5 5 5 3 5 3 5 5 1 1 1 3 5 3 5 5 5 3 3 3 4 4 4 3 3 4 5\n [445] 5 3 3 3 3 3 1 3 3 3 3 1 4 3 4 4 5 3 3 3 3 5 3 3 3 1 1 3 3 3 3 3 1 3 3 3 3\n [482] 1 3 5 5 5 1 5 5 3 3 5 3 3 3 1 5 3 3 3 1 3 3 1 1 1 3 3 3 1 3 3 3 3 3 3 3 1\n [519] 5 5 1 1 1 3 3 3 3 3 4 4 5 5 3 3 3 5 5 3 3 5 5 1 3 5 1 1 5 4 5 1 1 2 5 4 5\n [556] 5 1 5 1 5 1 1 1 1 5 5 1 1 1 1 1 3 1 5 1 5 3 3 1 5 1 1 1 3 1 3 1 1 1 5 5 5\n [593] 3 3 1 1 1 3 3 1 5 4 1 3 3 5 3 3 1 3 4 5 5 5 1 5 1 5 3 3 3 3 3 3 1 1 1 1 1\n [630] 5 3 3 3 3 5 5 5 5 3 5 5 5 1 3 3 3 5 5 5 5 3 3 3 3 5 1 1 1 4 4 4 4 5 5 5 5\n [667] 5 1 5 5 5 5 5 5 5 5 5 5 3 5 5 5 4 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 5 5 5 5 4\n [704] 4 5 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4 4 5 4 4 4 1 5 5 4 4 5 4 4 5 4\n [741] 4 4 4 4 5 4 4 5 4 4 1 1 1 1 1 5 1 5 5 4 1 3 5 3 5 1 3 1 1 3 5 3 3 5 3 3 1\n [778] 1 2 1 1 1 1 1 1 5 5 4 4 1 3 3 1 3 3 5 3 1 3 3 1 1 1 1 5 1 1 1 5 3 1 1 1 1\n [815] 3 1 1 1 1 1 1 1 1 5 5 5 3 3 5 5 3 3 3 1 3 3 1 1 5 5 5 5 5 4 1 1 1 3 2 3 3\n [852] 3 2 1 1 2 1 2 2 3 3 3 2 2 2 2 5 2 2 2 2 2 2 5 4 4 2 5 2 2 4 4 4 2 5 5 4 5\n [889] 5 1 3 3 3 1 3 1 3 1 2 1 1 3 3 2 4 2 5 1 1 3 3 4 1 1 1 1 1 2 2 2 5 2 2 5 2\n [926] 2 2 2 2 2 3 5 5 3 4 5 2 2 2 3 3 1 3 4 4 4 4 3 3 2 1 1 2 2 2 4 4 1 4 4 5 5\n [963] 5 3 1 3 1 4 4 4 3 3 1 1 5 5 4 4 1 5 3 4 5 4 4 4 5 1 1 5 4 4 4 4 3 3 3 2 4\n[1000] 4 5 3 2 5 2 5 4 1 4 4 4 4 4 1 5 5 1 1 5 5 5 5 1 3 3 3 1 1 1 5 1 5 5 3 1 1\n[1037] 1 1 3 3 1 5 1 1 1 5 5 1 3 3 3 3 4 4 1 3 3 3 3 3 3 3 1 5 5 1 1 3 3 3 1 5 5\n[1074] 1 3 3 5 3 1 3 3 1 1 1 5 5 1 1 1 1 3 3 3 3 3 3 3 3 1 1 1 3 1 1 3 3 1 1 1 5\n[1111] 5 5 3 5 3 3 1 1 1 3 3 1 3 3 3 4 1 3 3 3 4 4 4 4 4 1 4 5 4 4 4 4 4 4 4 4 4\n[1148] 4 4 4 3 4 4 4 4 5 4 4 4 5 3 5 4 4 4 4 4 4 4 4 3 5 1 5 4 3 5 3 5 5 4 4 4 4\n[1185] 4 4 4 5 5 5 5 5 4 5 4 4 4 5 3 5 5 5 5 4 4 4 4 4 3 3 4 4 4 5 5 3 3 5 3 5 5\n[1222] 3 3 1 5 5 4 4 4 5 5 3 5 5 5 5 1 1 5 5 5 5 5 5 3 3 3 3 1 5 5 5 5 5 5 4 4 4\n[1259] 1 5 5 5 5 5 5 5 5 4 4 1 1 1 4 3 5 3 5 5 5 4 5 5 5 5 3 3 3 5 5 5 3 5 5 5 5\n[1296] 4 4 4 4 1 3 3 3 3 3 5 5 5 1 5 5 5 3 5 3 1 1 4 4 4 5 5 4 5 2 2 5 3 1 3 5 1\n[1333] 1 3 3 1 3 1 3 1 1 1 3 2 1 5 5 1 1 1 1 1 1 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5\n[1370] 3 3 1 3 1 3 3 4 4 4 4 5 5 5 5 5 3 1 3 3 5 3 3 3 3 4 5 5 5 5 5 5 5 1 5 5 3\n[1407] 3 3 3 3 3 1 1 1 1 3 1 3 3 1 5 1 1 3 1 1 3 1 1 5 5 3 3 3 1 3 3 3 3 3 3 1 1\n\n...\nand 156 more lines.\n\n\nA nicer way to look at the results is by accessing specific parts of the fitted model object, as below.\n\n# some basic model metrics\nglance(examp_fit)\n\n\n\n  \n\n\n# centroid data (transformed/standardized scale)\ncentroids &lt;- tidyclust::extract_centroids(examp_fit)\ncentroids\n\n\n\n  \n\n\n# helpful to add to future plots for examining indiv. clusters\nclust_stat &lt;- tidyclust::sse_within(examp_fit)\nclust_stat"
  },
  {
    "objectID": "22-pca-kmeans.html#sec-mod-metrics",
    "href": "22-pca-kmeans.html#sec-mod-metrics",
    "title": "15  Do PCA before k-means",
    "section": "15.11 Model metrics",
    "text": "15.11 Model metrics\nSee also section 7.5 in the Chapter by Tan et al. for more about cluster evaluation.\n\n15.11.1 Extract metrics\n\nmetrics_df &lt;- km_fit_df %&gt;%\n  mutate(\n    # tot_sse = total sum of squared error\n    tot_sse = map_dbl(km_fit, ~ sse_total_vec(.x)),\n    # tot_wss = sum of within-cluster sse\n    tot_wss = map_dbl(km_fit, ~sse_within_total_vec(.x)),\n    # sse ratio = wss / total sse, \n    sse_ratio = map_dbl(km_fit, ~sse_ratio_vec(.x))\n    )\n\nrm(km_fit_df)\n\nmetrics_simple &lt;- metrics_df %&gt;% \n  select(n_clust, tot_sse, tot_wss, sse_ratio)\n\nmetrics_simple\n\n\n\n  \n\n\nrm(pca_prep)\nrm(rec_spec)\n\n\n\n15.11.2 Plot Total WSS\nNot a clear “elbow” here, although by the time we get to 10-11 it does seem to be leveling off.\n\nmetrics_simple %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Compare values of k: looking for elbow\")\n\n\n\nmetrics_simple %&gt;% \n  filter(n_clust %in% c(2:12)) %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:12)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Zoom in a bit: looking for elbow\")\n\n\n\n\n\n\n15.11.3 Average Silhouette\nFrom the {tidyclust} documentation:\n\nAnother common measure of cluster structure is called the silhouette. The silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\n\nIn principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\nSee also pg. 581 in Tan2018 Chap 7 Cluster Analysis: Basic Concepts and Algorithms\n\nprepped_rec &lt;- prep(km_rec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \nbaked_df &lt;- bake(prepped_rec, new_data = NULL) %&gt;% \n  select(-mukey) \n\ndists &lt;- baked_df %&gt;% as.matrix() %&gt;% dist(method = \"euclidean\")\n\nsilh_df &lt;- metrics_df %&gt;% \n  mutate(avg_sil = map_dbl(km_fit, \n                       tidyclust::silhouette_avg_vec,\n                       dists = dists),\n         indiv_sil = map(km_fit, \n                         tidyclust::silhouette,\n                         dists = dists))\n\nindiv_sil_df &lt;- silh_df %&gt;% select(n_clust, indiv_sil) %&gt;% \n  unnest(indiv_sil) %&gt;% \n  mutate(across(.cols = c(cluster, neighbor),\n                .fns = as.character))\n\nwrite_csv(indiv_sil_df, \"data/pca_kmeans_point_silhouettes.csv\")\n\nrm(metrics_df)  \nrm(dists)\nrm(prepped_rec)\n\nHigher silhouette is better (means observations are closer to their centroids than to other observations). Seems to suggest that 4, 6, 8, 11 would be OK\n\nsilh_df %&gt;% \n  ggplot(aes(x = n_clust, y = avg_sil)) +\n  geom_point() + \n  geom_line() + \n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  ggtitle(\"Overall Average Silhouette\") +\n  labs(subtitle = \"Higher is better, possible values [-1,1]\")\n\n\n\n\nCan also plot the individual silhouettes. For each clustering (model version), we have a silhouette value per observation in the dataset (n=6872). We also have the closest “neighbor” cluster, or the cluster that specific observation would belong to if its home cluster didn’t exist.\nHere’s an example of this data for the k=6 clustering. The\n\nneighbor_counts &lt;- indiv_sil_df %&gt;% \n  group_by(n_clust, cluster, neighbor) %&gt;% \n  count()  %&gt;% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c\"),\n         neighbor = str_replace(neighbor, \"Cluster_\", \"c\"))\n\nk6_neighbor_counts &lt;- neighbor_counts %&gt;% \n  filter(n_clust == 6)\n\n\nindiv_sil_df %&gt;% \n  mutate(across(.cols = c(cluster, neighbor),\n         ~str_replace(.x, \"Cluster_\", \"c\"))) %&gt;% \n  filter(n_clust == 6) %&gt;% \n  ggplot() + \n  geom_boxplot(aes(x = neighbor, y = sil_width)) +\n    geom_point(aes(x = neighbor, y = sil_width),\n             position = position_jitter(width = 0.1),\n             alpha = 0.2,\n             color = \"pink\") + \n  geom_text(data = k6_neighbor_counts,\n            aes(x = neighbor, y = 0.7, label = n),\n            color = \"blue\") +\n  facet_wrap(vars(cluster), scales = \"free_x\") +\n  theme_bw() +\n  ggtitle(\"k=6 silhouettes\")\n\n\n\n\n\nclust_sil_avgs &lt;- indiv_sil_df %&gt;% \n  group_by(cluster,\n           n_clust) %&gt;% \n  summarise(mean_sil = mean(sil_width),\n            sd_sil = sd(sil_width), \n            .groups = \"drop\")\n\nclust_sil_avgs %&gt;% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c0\"),\n         cluster = case_when(\n           cluster %in% c(\"c010\", \"c011\", \"c012\", \"c013\", \"c014\", \"c015\", \"c016\",\n                          \"c017\", \"c018\", \"c019\", \"c020\") ~ str_replace(cluster, \"c0\", \"c\"),\n           TRUE ~ cluster\n         )) %&gt;% \n  filter(n_clust %in% c(6:12)) %&gt;% \n  ggplot() +\n  geom_col(aes(y = cluster, x = mean_sil)) +\n  facet_wrap(vars(n_clust), scales = \"free_y\") + \n  ggtitle(\"Average silhouette width per cluster for k=6-12\") +\n  theme_bw()\n\n\n\n\n\n\n15.11.4 Not used: Gap statistic\nFor fviz_nbclust(), first couple times running this, got Warning: Quick-TRANSFER stage steps exceeded maximum… Looking online, this seems to be a problem with the model not converging. I added some arguments here that are passed on to kmeans(), to make sure that the algorithm settings here match what I run above, including set.seed()\nContinued to get warnings, even though I’m using all the same settings as I use for kmeans up above. Not sure why this is, but I’m not going to spend any more time on it right now. Maybe see if getting the gap statistic through NbClust works better? (Later note: NbClust won’t be a good option either, I can’t alter important kmeans() settings in NbClust). Expect it will take a long time either way, consider running this in a separate script and pulling in the results.\n\nset.seed(4)\nfviz_gap_stat(x = baked_df, \n             FUNcluster = kmeans,\n             method = c(\"gap_stat\"),\n             k.max = 10, # only considering 2-10 clusters\n             nboot = 50, # default is 100\n             verbose = TRUE, \n             iter.max = 20, # passed to kmeans\n             nstart = 10 # passed to kmeans\n             )\n\n\n\n15.11.5 Calinski-Harabasz index\nNot used: {NbClust} , using {fpc} instead.\n\nFor Calinski-Harabasz index, higher values are better\nRealized after setting this up with NbClust that I don’t have the option to pass additional arguments to the kmeans function here. So I can’t make the algorithm settings exactly match my main clustering pipeline above (where I implement k-means using tidyclust and the tidymodels framework, and where I save the results for further analysis). This is a problem because I know from my original tests that I need to change the iter.max value to avoid non-convergence issues, and I also want to change nstart because nstart &gt;1 is typically known to be best practice (find citation for this).\n\n\n# keeping this here as a record, but I\"m NOT USING this function for the C-H index. \n\nnbc_indices &lt;- NbClust::NbClust(data = baked_df,\n                 distance = \"euclidean\",\n                 method = \"kmeans\",\n                 min.nc = 2,\n                 max.nc = 20,\n                 index = \"ch\") # Calinski and Harabasz\n\n# enframe turns a named vector into a dataframe\nch_index_vals &lt;- enframe(nbc_indices$All.index) %&gt;% \n  mutate(name = as.integer(name)) %&gt;% \n  rename(n_clust = name)\n\nTrying a different implementation of the Calinski-Harabasz index from the {fpc} package. This is preferred to the above approach, where I originally used the NbClust function from {NbClust} package because I can give this function my clustering generated above (NbClust does its own run of kmeans but I can’t customize it to keep it consistent with\n\n# calinhara wants an observations/variables matrix\n# as first argument (as opposed to a distance matrix)\nobsvar_mx &lt;- as.matrix(baked_df)\n\n# function to extract and modify tidyclust clusters\n# into a integer vector, which I will pass to calinhara()\ncreate_clust_vec &lt;- function(fit_obj){\n  \n  extract_cluster_assignment(fit_obj) %&gt;% \n    pull(.cluster) %&gt;% \n    str_replace(., \"Cluster_\", \"\") %&gt;% \n    as.integer()\n  \n}\n\n# apply function to extract clusterings as integer vectors\n# map to get a c-h index value for every value of k (2-20)\nch_metrics &lt;- silh_df %&gt;%\n  select(n_clust, km_fit) %&gt;%\n  mutate(\n    clustering_vec = map(km_fit, create_clust_vec),\n    ch_index = map_dbl(clustering_vec,\n                       ~ fpc::calinhara(x = obsvar_mx,\n                                        clustering = .x)\n    ))\n\nch_metrics %&gt;% \n  ggplot(aes(x = n_clust, y = ch_index)) + \n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  ylab(\"Calinski-Harabasz index\") + \n  ggtitle(\"Calinski-Harabasz\") +\n  labs(subtitle = \"Higher is better\") +\n  scale_x_continuous(breaks = c(2:20))\n\n\n\n\n\n\n15.11.6 Hopkins Statistic\nUsing the {hopkins} package for this. Citations included in the package documentation (also cite Tan et al., 2019 who give an example of using this for evaluating kmeans clusters).\n\nHopkins, B. and Skellam, J.G., 1954. A new method for determining the type of distribution of plant individuals. Annals of Botany, 18(2), pp.213-227.\nCross, G. R., and A. K. Jain. (1982). Measurement of clustering tendency. Theory and Application of Digital Control. Pergamon, 1982. 315-320.\n\nAnd a third citation, helpful illustrations:\n\nLawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of Chemical Information and Computer Sciences, 30(1), 36–41. https://doi.org/10.1021/ci00065a010\n\nApparently {factoextra} also has a Hopkins statistic, try that here too. (It takes a very long time to run, but returns 0.93, similar to 0.99 returned by hopkins::hopkins()\n\nset.seed(4)\nhstat &lt;- hopkins(X = baked_df,\n                 # default, number of rows to sample from the df\n                 m = ceiling(nrow(baked_df)/10), \n                 # default, dimension of the data\n                 d = ncol(baked_df),\n                 # default, kth nearest neighbor to find\n                 k = 1) \nhstat\n\n[1] 0.9999704\n\nhopkins.pval(x = hstat,\n             # this is the default for hopkins() above\n             n = ceiling(nrow(baked_df)/10)) \n\n[1] 0\n\n# below gives 0.9331899 as the result\n# which agrees with above\n# commenting out because it takes a very long time to run\n# factoextra::get_clust_tendency(data = baked_df,\n#                                n = 687, \n#                                graph = FALSE)\n\n\n\n15.11.7 WSS and Silhouette metrics on one plot\n\n#| echo: false\n\nsil_totwss &lt;- silh_df %&gt;% \n  select(n_clust, avg_sil, tot_wss, tot_sse, sse_ratio)\n\nch &lt;- ch_metrics %&gt;% \n  select(n_clust, ch_index)\n\nmet_combined &lt;- left_join(sil_totwss, ch, by = \"n_clust\")\n\nwrite_csv(met_combined, \"data/pca_kmeans_cluster_metrics.csv\")\n\nmet2 &lt;- met_combined %&gt;% \n  pivot_longer(cols = -c('n_clust'), names_to = \"metric\",\n               values_to = \"value\")\n\nmet2 %&gt;% \n  ggplot(aes(x = n_clust, y = value)) + \n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(2:20)) + \n  facet_wrap(vars(metric), ncol = 1, scales = \"free\") +\n  theme_bw()"
  },
  {
    "objectID": "22-pca-kmeans.html#save-model-fits",
    "href": "22-pca-kmeans.html#save-model-fits",
    "title": "15  Do PCA before k-means",
    "section": "15.12 Save model fits",
    "text": "15.12 Save model fits\nWill save these as Rdata so I can call them up and investigate the cluster centroids more closely in the next chapter.\n\npca_mods &lt;- silh_df %&gt;% \n  select(n_clust, km_fit)\n\nsave(pca_mods, file = \"data/fitted_pca_kmeans_mods.RData\")"
  },
  {
    "objectID": "22-pca-kmeans.html#save-cluster-assignments",
    "href": "22-pca-kmeans.html#save-cluster-assignments",
    "title": "15  Do PCA before k-means",
    "section": "15.13 Save cluster assignments",
    "text": "15.13 Save cluster assignments\nFor each version of the model (each value of k, different numbers of clusters), a MUKEY is assigned to a specific cluster. Here, I’m pulling that data, shaping it into one dataframe (one row per MUKEY, cluster assingments in separate columns). I’m also adding back in the soil property data so we can use this in the next step when evaluating different cluster sizes.\n\nclust_assign_df &lt;- pca_mods %&gt;% \n  mutate(clust_assign = map(km_fit, ~augment(.x, new_data = dat_for_kmeans)),\n         mukey_clust = map(clust_assign, ~select(.x, mukey, .pred_cluster)))\n\n\nassign_mukey_df &lt;- clust_assign_df %&gt;% \n  select(n_clust, mukey_clust) %&gt;% \n  unnest(mukey_clust) %&gt;% pivot_wider(names_from = n_clust, values_from = .pred_cluster, names_prefix = \"k_\")\n\n# drop k_6 column from d (it was from orig k-means clusters,\n# used above for viz purposes only)\n\nsoil_props &lt;- d %&gt;% select(-k_6) \n\nclust_props &lt;- full_join(soil_props, assign_mukey_df, by = \"mukey\")\n\nwrite_csv(clust_props, \"data/pca_mukey_cluster_assignments_and_soilprops.csv\")"
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#overview",
    "href": "23-pca-kmeans-finalists.html#overview",
    "title": "16  PCA-kmeans Clustering Finalists",
    "section": "16.1 Overview",
    "text": "16.1 Overview\nOriginal k-means: silhouette suggests 6, 9-11. C-H suggests 6, 11. Pairwise comparisons suggest 6, 8, or 11\nPCA before k-means: silhouette suggests 4, 6, 8, 11. C-H 4-8 would be similar, 9-11 would be similar.\nSo between these two, it seems like it would make sense to compare 6, 8, 11.\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(stringr)\nlibrary(gtsummary)\nlibrary(terra)\n\n\n# input files for spatial exploration of clusters \n\n# generated in 22-pca-kmeans.qmd\nprop1 &lt;- read_csv(\"data/pca_mukey_cluster_assignments_and_soilprops.csv\")\n\nk4all &lt;- rast(\"E:/big-files-backup/ch03-sh-groups/clust4_pca.tif\")\nk6all &lt;- rast(\"E:/big-files-backup/ch03-sh-groups/clust6_pca.tif\")\nk8all &lt;- rast(\"E:/big-files-backup/ch03-sh-groups/clust8_pca.tif\")\nk11all &lt;- rast(\"E:/big-files-backup/ch03-sh-groups/clust11_pca.tif\")\n\n\n# input files for taxonomic exploration of clusters \n\n# generated ch7 map unit aggregation\nmu &lt;- read_csv(\"data/mu_weighted_soil_props.csv\")\n\ncmp_lookup &lt;- read_csv(\"data/key_cokey_mukey_complete_cases_include.csv\")\n\ncmp_details &lt;- read_csv(\"data/component_list.csv\")\n\nmunames &lt;- read_csv(\"data/target_mapunit_table.csv\") %&gt;% \n  select(mukey, muname, muacres)\n\n# more rows than mu b/c there are multiple cmps\n# included in some mus\nmu_cmp &lt;- left_join(mu, cmp_lookup, by = \"mukey\")\n\n# adds munames and muacres\nmu_cmp_nm &lt;- left_join(mu_cmp, munames, by = \"mukey\")\n\n# one row for each component (there can be multiple components\n# for an included mukey), now includes munames and component details\nmu_detail &lt;- left_join(mu_cmp_nm, cmp_details, by = c(\"cokey\",\"mukey\"))\n\n# for translating MUKEY from gSSURGO to my shortened version\ncwalk &lt;- aoi_mu &lt;- read.delim(\"data/gSSURGO_MN/mukey_new_crosswalk.txt\", sep = \",\") %&gt;% \n  select(MUKEY, MUKEY_New, Count)\n\n\n\n Cluster_0  Cluster_1  Cluster_2  Cluster_3  Cluster_4  Cluster_5  Cluster_6 \n \"#FFF8DC\"  \"#FF5A5F\"  \"#FFB400\"  \"#007A87\"  \"#8CE071\"  \"#7B0051\"  \"#00D1C1\" \n Cluster_7  Cluster_8  Cluster_9 Cluster_10 Cluster_11 Cluster_12 \n \"#FFAA91\"  \"#B4A76C\"  \"#9CA299\"  \"#565A5C\"  \"#00A04B\"  \"#E54C20\""
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#clusters",
    "href": "23-pca-kmeans-finalists.html#clusters",
    "title": "16  PCA-kmeans Clustering Finalists",
    "section": "16.2 4 clusters",
    "text": "16.2 4 clusters\n\n16.2.1 Summary table\n\ncreate_summary_table(kcol = \"k_4\", prop_df = prop1)\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 2,0221\nCluster_2, N = 2,8211\nCluster_3, N = 1,5441\nCluster_4, N = 4851\n\n\n\n\nclaytotal\n9 (4)\n24 (8)\n24 (8)\n25 (12)\n\n\nom\n2.27 (1.19)\n5.87 (10.03)\n5.56 (4.31)\n4.80 (1.65)\n\n\ncec7\n9 (4)\n24 (19)\n22 (10)\n22 (10)\n\n\ndbthirdbar\n1.47 (0.10)\n1.30 (0.19)\n1.31 (0.20)\n1.28 (0.09)\n\n\nec\n0.01 (0.08)\n0.00 (0.03)\n0.00 (0.00)\n1.28 (0.52)\n\n\nph1to1h2o\n6.46 (0.49)\n6.60 (0.35)\n7.54 (0.31)\n7.53 (0.47)\n\n\ncaco3\n0.4 (1.4)\n0.2 (0.8)\n9.4 (5.0)\n7.8 (6.2)\n\n\nlep\n1.19 (0.47)\n2.88 (1.75)\n2.89 (1.71)\n3.53 (3.10)\n\n\nksat\n53 (36)\n9 (6)\n11 (9)\n12 (15)\n\n\nawc\n0.13 (0.03)\n0.21 (0.04)\n0.20 (0.03)\n0.19 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n16.2.2 Violin plots\n\nplot_cluster_violins(kcol = k_4, claytotal, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_4, om, prop_df = prop1) + scale_y_log10()\n\n\n\nplot_cluster_violins(kcol = k_4, ph1to1h2o, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_4, caco3, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_4, ec, prop_df = prop1)\n\n\n\n\n\n\n16.2.3 Maps\n\nk4all_colors &lt;- create_mapcol_df(4)\n\nplot(k4all, col = k4all_colors, plg=list(legend = c(\"C0\", \"C1\", \"C2\", \"C3\", \"C4\")), main = \"k=4 all variables\")\n\n\n\n16.2.4 Taxonomy\n\n\n\n\n\n\n\n16.2.5 Visualize clusters in 2D\n\nprop1 %&gt;% \n  ggplot() + \n  geom_point(aes(x = claytotal, y = ph1to1h2o, color = k_4)) +\n  theme_bw()\n\n\n\nprop1 %&gt;% \n  ggplot() + \n  geom_point(aes(x = claytotal, y = om, color = k_4)) +\n  theme_bw() + \n  scale_y_log10()"
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#clusters-1",
    "href": "23-pca-kmeans-finalists.html#clusters-1",
    "title": "16  PCA-kmeans Clustering Finalists",
    "section": "16.3 6 clusters",
    "text": "16.3 6 clusters\n\n16.3.1 Summary table\n\ncreate_summary_table(kcol = \"k_6\", prop_df = prop1)\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 1,1291\nCluster_2, N = 2,1581\nCluster_3, N = 1,6641\nCluster_4, N = 1,4361\nCluster_5, N = 4201\nCluster_6, N = 651\n\n\n\n\nclaytotal\n7 (2)\n16 (4)\n30 (8)\n24 (7)\n23 (11)\n7 (4)\n\n\nom\n1.83 (0.99)\n3.20 (1.53)\n5.32 (2.00)\n5.35 (3.11)\n4.80 (1.71)\n69.79 (15.21)\n\n\ncec7\n7 (2)\n15 (4)\n26 (6)\n22 (7)\n20 (9)\n143 (28)\n\n\ndbthirdbar\n1.50 (0.10)\n1.40 (0.09)\n1.29 (0.11)\n1.31 (0.19)\n1.28 (0.09)\n0.27 (0.15)\n\n\nec\n0.01 (0.10)\n0.00 (0.02)\n0.04 (0.20)\n0.00 (0.00)\n1.33 (0.54)\n0.00 (0.00)\n\n\nph1to1h2o\n6.41 (0.53)\n6.51 (0.38)\n6.73 (0.35)\n7.56 (0.32)\n7.63 (0.39)\n6.61 (0.42)\n\n\ncaco3\n0.5 (1.5)\n0.3 (1.1)\n0.4 (1.2)\n9.8 (4.9)\n8.9 (5.8)\n1.5 (2.4)\n\n\nlep\n1.08 (0.50)\n1.50 (0.54)\n4.21 (1.94)\n2.74 (1.46)\n3.00 (2.62)\n0.59 (0.31)\n\n\nksat\n73 (36)\n18 (11)\n6 (3)\n11 (9)\n14 (16)\n24 (5)\n\n\nawc\n0.11 (0.02)\n0.18 (0.03)\n0.20 (0.02)\n0.19 (0.02)\n0.19 (0.03)\n0.40 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n16.3.2 Violin plots\n\nplot_cluster_violins(kcol = k_6, claytotal, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_6, om, prop_df = prop1) + scale_y_log10()\n\n\n\nplot_cluster_violins(kcol = k_6, ph1to1h2o, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_6, caco3, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_6, ec, prop_df = prop1)\n\n\n\n\n\n\n16.3.3 Maps\n\nk6all_colors &lt;- create_mapcol_df(6)\n\nplot(k6all, col = k6all_colors, plg=list(legend = c(\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\")), main = \"k=6 all variables\")\n\n\n\n16.3.4 Taxonomy\n\n\n\n\n\n\n\n16.3.5 Visualize clusters in 2D\n\nprop1 %&gt;% \n  ggplot() + \n  geom_point(aes(x = claytotal, y = ph1to1h2o, color = k_6)) +\n  theme_bw()\n\n\n\nprop1 %&gt;% \n  ggplot() + \n  geom_point(aes(x = claytotal, y = om, color = k_6)) +\n  theme_bw() + \n  scale_y_log10()"
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#clusters---all-variables-included",
    "href": "23-pca-kmeans-finalists.html#clusters---all-variables-included",
    "title": "16  PCA-kmeans Clustering Finalists",
    "section": "16.4 8 clusters - all variables included",
    "text": "16.4 8 clusters - all variables included\n\n16.4.1 Summary table\n\ncreate_summary_table(kcol = \"k_8\", prop_df = prop1)\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 9131\nCluster_2, N = 1,6291\nCluster_3, N = 1,3511\nCluster_4, N = 6281\nCluster_5, N = 9711\nCluster_6, N = 3821\nCluster_7, N = 9331\nCluster_8, N = 651\n\n\n\n\nclaytotal\n6 (2)\n22 (4)\n12 (3)\n30 (6)\n34 (7)\n22 (9)\n19 (5)\n7 (4)\n\n\nom\n1.66 (0.82)\n3.77 (1.79)\n2.94 (1.35)\n7.32 (3.34)\n6.03 (1.83)\n4.74 (1.40)\n3.90 (1.88)\n69.79 (15.21)\n\n\ncec7\n6 (2)\n19 (4)\n12 (3)\n28 (7)\n30 (6)\n19 (8)\n17 (4)\n143 (28)\n\n\ndbthirdbar\n1.50 (0.10)\n1.35 (0.07)\n1.44 (0.09)\n1.21 (0.24)\n1.26 (0.13)\n1.28 (0.09)\n1.39 (0.08)\n0.27 (0.15)\n\n\nec\n0.01 (0.11)\n0.01 (0.11)\n0.00 (0.03)\n0.01 (0.12)\n0.08 (0.27)\n1.36 (0.56)\n0.00 (0.00)\n0.00 (0.00)\n\n\nph1to1h2o\n6.38 (0.51)\n6.51 (0.32)\n6.53 (0.43)\n7.66 (0.25)\n6.81 (0.35)\n7.67 (0.36)\n7.45 (0.35)\n6.61 (0.42)\n\n\ncaco3\n0.3 (1.2)\n0.1 (0.5)\n0.3 (1.1)\n10.9 (4.9)\n0.5 (1.3)\n9.4 (5.7)\n8.4 (4.8)\n1.5 (2.4)\n\n\nlep\n1.06 (0.51)\n2.32 (0.95)\n1.26 (0.46)\n3.91 (1.81)\n5.05 (2.16)\n2.58 (1.95)\n2.00 (0.92)\n0.59 (0.31)\n\n\nksat\n81 (34)\n10 (3)\n27 (14)\n6 (3)\n4 (3)\n15 (16)\n14 (11)\n24 (5)\n\n\nawc\n0.10 (0.02)\n0.21 (0.02)\n0.16 (0.03)\n0.20 (0.02)\n0.19 (0.02)\n0.19 (0.03)\n0.19 (0.03)\n0.40 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n16.4.2 Violin plots\n\nplot_cluster_violins(kcol = k_8, claytotal, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_8, om, prop_df = prop1) + scale_y_log10()\n\n\n\nplot_cluster_violins(kcol = k_8, ph1to1h2o, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_8, caco3, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_8, ec, prop_df = prop1)\n\n\n\n\n\n\n16.4.3 Maps\n\nk8all_colors &lt;- create_mapcol_df(8)\n\nplot(k8all, col = k8all_colors, plg=list(legend = c(\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\")), main = \"k=8 all variables\")\n\n\n\n16.4.4 Taxonomy"
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#clusters---all-variables",
    "href": "23-pca-kmeans-finalists.html#clusters---all-variables",
    "title": "16  PCA-kmeans Clustering Finalists",
    "section": "16.5 11 clusters - all variables",
    "text": "16.5 11 clusters - all variables\n\n16.5.1 Summary table\n\ncreate_summary_table(kcol = \"k_11\", prop_df = prop1)\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 4991\nCluster_10, N = 2801\nCluster_11, N = 651\nCluster_2, N = 4701\nCluster_3, N = 1,5311\nCluster_4, N = 4731\nCluster_5, N = 9031\nCluster_6, N = 6181\nCluster_7, N = 9191\nCluster_8, N = 2041\nCluster_9, N = 9101\n\n\n\n\nclaytotal\n6 (2)\n18 (7)\n7 (4)\n7 (2)\n22 (3)\n13 (3)\n12 (3)\n30 (6)\n33 (6)\n35 (12)\n20 (5)\n\n\nom\n1.28 (0.64)\n4.71 (1.44)\n69.79 (15.21)\n2.39 (1.05)\n3.61 (1.42)\n4.52 (2.55)\n2.39 (0.84)\n7.36 (3.35)\n6.06 (1.74)\n4.92 (1.91)\n3.85 (1.81)\n\n\ncec7\n5 (1)\n16 (6)\n143 (28)\n8 (2)\n19 (3)\n14 (6)\n12 (3)\n28 (7)\n29 (5)\n29 (8)\n18 (4)\n\n\ndbthirdbar\n1.55 (0.06)\n1.28 (0.09)\n0.27 (0.15)\n1.43 (0.10)\n1.36 (0.07)\n1.34 (0.08)\n1.48 (0.07)\n1.20 (0.24)\n1.27 (0.13)\n1.26 (0.09)\n1.39 (0.08)\n\n\nec\n0.00 (0.00)\n1.45 (0.60)\n0.00 (0.00)\n0.03 (0.16)\n0.00 (0.00)\n0.01 (0.08)\n0.00 (0.00)\n0.00 (0.00)\n0.00 (0.00)\n1.05 (0.22)\n0.00 (0.00)\n\n\nph1to1h2o\n6.11 (0.36)\n7.80 (0.21)\n6.61 (0.42)\n6.85 (0.44)\n6.51 (0.32)\n6.66 (0.37)\n6.40 (0.42)\n7.66 (0.25)\n6.78 (0.34)\n7.16 (0.48)\n7.46 (0.35)\n\n\ncaco3\n0.0 (0.5)\n11.3 (5.1)\n1.5 (2.4)\n1.3 (2.2)\n0.1 (0.5)\n0.3 (1.0)\n0.2 (0.7)\n10.9 (5.0)\n0.4 (1.1)\n3.1 (4.0)\n8.5 (4.8)\n\n\nlep\n1.08 (0.53)\n1.83 (0.97)\n0.59 (0.31)\n1.05 (0.50)\n2.34 (0.92)\n0.98 (0.53)\n1.41 (0.33)\n3.84 (1.74)\n4.77 (1.73)\n5.90 (3.45)\n2.03 (0.93)\n\n\nksat\n88 (37)\n18 (18)\n24 (5)\n72 (27)\n9 (2)\n23 (13)\n26 (12)\n6 (3)\n4 (3)\n4 (4)\n14 (10)\n\n\nawc\n0.10 (0.02)\n0.19 (0.03)\n0.40 (0.03)\n0.11 (0.01)\n0.21 (0.02)\n0.18 (0.03)\n0.15 (0.02)\n0.20 (0.02)\n0.19 (0.02)\n0.18 (0.03)\n0.19 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\nrank_clusters(k_11, claytotal, prop1)\n\n\n\n  \n\n\nrank_clusters(k_11, ph1to1h2o, prop1)\n\n\n\n  \n\n\nrank_clusters(k_11, om, prop1)\n\n\n\n  \n\n\nrank_clusters(k_11, caco3, prop1)\n\n\n\n  \n\n\n\n\n\n16.5.2 Violin plots\n\nplot_cluster_violins(kcol = k_11, claytotal, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_11, om, prop_df = prop1) + scale_y_log10()\n\n\n\nplot_cluster_violins(kcol = k_11, ph1to1h2o, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_11, caco3, prop_df = prop1)\n\n\n\nplot_cluster_violins(kcol = k_11, ec, prop_df = prop1)\n\n\n\n\n\n\n16.5.3 Taxonomy"
  },
  {
    "objectID": "24-compare-kmeans.html#which-clusters-are-we-comparing",
    "href": "24-compare-kmeans.html#which-clusters-are-we-comparing",
    "title": "17  Comparing k-means scenarios",
    "section": "17.1 Which clusters are we comparing?",
    "text": "17.1 Which clusters are we comparing?\nOriginal k-means: silhouette suggests 6, 9-11. C-H suggests 6, 11. Pairwise comparisons suggest 6, 8, or 11\nPCA before k-means: silhouette suggests 4, 6, 8, 11. C-H 4-8 would be similar, 9-11 would be similar"
  },
  {
    "objectID": "24-compare-kmeans.html#compare-model-metrics",
    "href": "24-compare-kmeans.html#compare-model-metrics",
    "title": "17  Comparing k-means scenarios",
    "section": "17.2 Compare model metrics",
    "text": "17.2 Compare model metrics\n\norig_metrics &lt;- read_csv(\"data/kmeans_cluster_metrics.csv\") %&gt;% \n  mutate(version = \"original\")\n\npca_metrics &lt;- read_csv(\"data/pca_kmeans_cluster_metrics.csv\") %&gt;% \n  mutate(version = \"pca\")\n\nall_metrics &lt;- bind_rows(orig_metrics, pca_metrics)\n\n\n17.2.1 Sum of within-cluster SSE\nIt doesn’t make sense to plot these two on the same graph, because the errors are in different units (z-score for the original, PCA score for the PCA).\n\n\n\n\n\n\n\n17.2.2 Average Silhouette\nFrom the {tidyclust} documentation:\n\nAnother common measure of cluster structure is called the silhouette.\nThe silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\nIn principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\n\n\n\n\n\n\n\n17.2.3 Comparing Calinski-Harabasz index\nHigher values are better. This is also known as the “Variance Ratio Criterion,”, which is how Calinski & Harabasz refer to it in their 1974 paper introducing it. The paper title is “A dendrite method for cluster analysis”\nNice summary from PyShark:\n\n“The Calinski-Harabasz index (also known as the Variance Ratio Criterion) is calculated as a ratio of the sum of inter-cluster dispersion and the sum of intra-cluster dispersion for all clusters (where the dispersion is the sum of squared distances).\nA high CH means better clustering since observations in each cluster are closer together (more dense), while clusters themselves are further away from each other (well separated).”"
  },
  {
    "objectID": "24-compare-kmeans.html#jaccard-similarity-coefficient",
    "href": "24-compare-kmeans.html#jaccard-similarity-coefficient",
    "title": "17  Comparing k-means scenarios",
    "section": "17.3 Jaccard similarity coefficient",
    "text": "17.3 Jaccard similarity coefficient\nSo what am I comparing? Visually, I’d like to make an alluvial plot or Sankey-type plot to see how the two compare (and I did this, see PDFS in _refs/. But I’d also like to compare membership between the different clusterings. In Lasantha et al., (2022), they use the Jaccard similarity coefficient.\n\n“The Jaccard similarity coefficient is the ratio between the intersection and union of two sets; it has values ranging from zero for non-intersection to one for exact similarity. This is index is widely used in the evaluation of similarity in clustering in addition to applications such as image recognition and text analysis” Lasantha et al., 2022\n\nThis appears to be the citation for the original coefficient:\nP. Jaccard, “The distribution of the flora in the alpine zone zone” New Phytologist, vol. 11, no. 2, pp. 37–50, 1912.\nFrom Tan et al. (2018): “… the simple matching coefficient, which is known as the Rand statistic in this context, and the Jaccard coefficient are two of the most frequently used cluster validity measures.”\nAdditional relevant Jaccard citation:\nHennig, C. (2007). Cluster-wise assessment of cluster stability. Computational Statistics & Data Analysis, 52(1), 258–271. https://doi.org/10.1016/j.csda.2006.11.025\nTan, P.-N., Steinbach, M., Karpatne, A., & Kumar, V. (2018). Introduction to data mining (2nd ed.). Pearson.\nEquation:\nJaccard coefficient = f11 / (f01 + f10 + f11)\n\n\n\n\nSame Cluster\nDifferent Cluster\n\n\n\n\nSame Class\nf11\nf10\n\n\nDifferent Class\nf01\nf00\n\n\n\nSo in words, when we are comparing to sets of classes (like two different clusterings produced by k-means), the Jaccard coefficient is telling us the ratio of : objects that are in both sets (the intersection) divided by the union (total number of objects in both sets, subtracting the number they share)\nApparently the {vegan} package can do this with the vegdist() function, method = 'jaccard' .\nThe thing I didn’t understand at first was that the Jaccard index is for use on a specific pair of clusters. So I need to do this pairwise for all the clusters in the original and PCA versions\n\n17.3.1 Example from online tutorial\nJaccard index example from UC Riverside GEN 242 Course\n\nsource(\"R/cindex_tgirke.R\") \n\nlibrary(cluster)\n\nWarning: package 'cluster' was built under R version 4.2.2\n\ny &lt;- matrix(rnorm(5000), 1000, 5, dimnames=list(paste(\"g\", 1:1000, sep=\"\"), paste(\"t\", 1:5, sep=\"\")))\n\nclarax &lt;- clara(y, 49)\n\n# length = 1000, 49 classes (numeric 1:49)\nclV1 &lt;- clarax$clustering\n\nclarax &lt;- clara(y, 50)\n\nclV2 &lt;- clarax$clustering \n\nci &lt;- cindex(clV1=clV1, clV2=clV2, self=FALSE, minSZ=1, method=\"jaccard\")\n\n\n\nci[2:3] # Returns Jaccard index and variables used to compute it\n\n$variables\n   a    b    c \n4568 8080 8218 \n\n$Jaccard_Index\n[1] 0.2189207\n\n  n_intersect &lt;- length(intersect(clV1, clV2))\n  \n  jac_index &lt;- n_intersect/(length(clV1) + length(clV2) - n_intersect)\n\n\n\n17.3.2 My Jaccard function\nThis was a helpful blog post (Jaccard Index is quite simple to calculate).\nRecall that what I want here is the MUKEYs. That allows us to calculate an intersection (so how many MUKEYs appear in both clusters?).\n\njaccard &lt;- function(orig_clust, pca_clust, dat, orig_col, pca_col){\n  \n  orig_sym &lt;- rlang::sym(orig_col)\n  pca_sym &lt;- rlang::sym(pca_col)\n  \n  orig_members &lt;- d %&gt;% \n    filter(!!orig_sym == orig_clust) %&gt;% \n    pull(mukey)\n  \n  pca_members &lt;- d %&gt;% \n    filter(!!pca_sym == pca_clust) %&gt;% \n    pull(mukey)\n  \n  n_intersect &lt;- length(intersect(orig_members, pca_members))\n  \n  jac_index &lt;- n_intersect/(length(orig_members) + length(pca_members) - n_intersect)\n  \n  return(jac_index)\n}\n\nTry it for k=6, calculating the Jaccard similarity coefficient for Cluster 1 from the original model set and Cluster 1 from the PCA model set.\nTo see if the result I’m getting makes sense, I’m looking at the alluvial diagram I made comparing original clusters and the PCA clusters. What I see is that Cluster 1 from the original version is split between Cluster 1 and Cluster 2 in the PCA version, with roughly 2/3 of the MUKEYs shared between Cluster 1 (original) and Cluster 1 (PCA). So it makes sense that the value below is 0.68.\n\njaccard(orig_clust = \"c1\", \n        pca_clust = \"p1\",\n        dat = d,\n        orig_col = \"k_6\",\n        pca_col = \"pk_6\"\n        )\n\n[1] 0.6801205"
  },
  {
    "objectID": "24-compare-kmeans.html#calculate-jaccard-similarity-coefficients",
    "href": "24-compare-kmeans.html#calculate-jaccard-similarity-coefficients",
    "title": "17  Comparing k-means scenarios",
    "section": "17.4 Calculate Jaccard similarity coefficients",
    "text": "17.4 Calculate Jaccard similarity coefficients\nI think it makes the most sense to do this with three dataframes, one for each value of K we are interested in evaluating. So that would be: 6, 8, 11.\n\n17.4.1 Set up data structure\n\n# data structure for k=6\nclust_orig_k6 &lt;- unique(d$k_6)\nclust_pca_k6 &lt;- unique(d$pk_6)\n\nk6_pairs &lt;- tidyr::crossing(k6_orig = clust_orig_k6,\n            k6_pca = clust_pca_k6)\n\n# data structure for k=8\nclust_orig_k8 &lt;- unique(d$k_8)\nclust_pca_k8 &lt;- unique(d$pk_8)\n\nk8_pairs &lt;- tidyr::crossing(k8_orig = clust_orig_k8,\n            k8_pca = clust_pca_k8)\n\n# data structure for k=11\nclust_orig_k11 &lt;- unique(d$k_11)\nclust_pca_k11 &lt;- unique(d$pk_11)\n\nk11_pairs &lt;- tidyr::crossing(k11_orig = clust_orig_k11,\n            k11_pca = clust_pca_k11)\n\n\n\n17.4.2 Map over Jaccard function\n\njac_k6 &lt;- k6_pairs %&gt;% \n  mutate(jaccard_coef = map2_dbl(.x = k6_orig,\n                                 .y = k6_pca,\n                                 .f = jaccard,\n                                 dat = d,\n                                 orig_col = \"k_6\",\n                                 pca_col = \"pk_6\"),\n         k = \"k_6\") %&gt;% \n  rename(orig = k6_orig,\n         pca = k6_pca)\n\njac_k8 &lt;- k8_pairs %&gt;% \n  mutate(jaccard_coef = map2_dbl(.x = k8_orig,\n                                 .y = k8_pca,\n                                 .f = jaccard,\n                                 dat = d,\n                                 orig_col = \"k_8\",\n                                 pca_col = \"pk_8\"),\n         k = \"k_8\") %&gt;% \n  rename(orig = k8_orig,\n         pca = k8_pca)\n\n  \n  \njac_k11 &lt;- k11_pairs %&gt;% \n  mutate(jaccard_coef = map2_dbl(.x = k11_orig,\n                                 .y = k11_pca,\n                                 .f = jaccard,\n                                 dat = d,\n                                 orig_col = \"k_11\",\n                                 pca_col = \"pk_11\"), \n         k = \"k_11\") %&gt;% \n  rename(orig = k11_orig,\n         pca = k11_pca)\n\n\njac_all &lt;- bind_rows(jac_k6, jac_k8, jac_k11)\n\nwrite_csv(jac_all, \"data/jaccard_coefficients_k6_k8_k11.csv\")\n\n\n\n17.4.3 Visualize Results\n\n\n\n\n\n\n\n\n\n\nPCA-6 Clusters\n\n\np1\np2\np3\np4\np5\np6\n\n\n\n\nFull-6 Clusters\nc1\n0.680\n0.162\n0.000\n0.000\n0.000\n0.000\n\n\nc2\n0.000\n0.565\n0.181\n0.017\n0.003\n0.000\n\n\nc3\n0.000\n0.007\n0.000\n0.925\n0.000\n0.000\n\n\nc4\n0.000\n0.000\n0.621\n0.009\n0.013\n0.000\n\n\nc5\n0.000\n0.000\n0.000\n0.000\n0.936\n0.000\n\n\nc6\n0.000\n0.001\n0.000\n0.000\n0.000\n0.970\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA-8 Clusters\n\n\np1\np2\np3\np4\np5\np6\np7\np8\n\n\n\n\nFull-8 Clusters\nc1\n0.993\n0.000\n0.001\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nc2\n0.000\n0.951\n0.008\n0.000\n0.012\n0.000\n0.001\n0.000\n\n\nc3\n0.002\n0.002\n0.973\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nc4\n0.000\n0.000\n0.000\n0.989\n0.000\n0.002\n0.000\n0.000\n\n\nc5\n0.000\n0.005\n0.000\n0.003\n0.952\n0.000\n0.000\n0.000\n\n\nc6\n0.000\n0.002\n0.000\n0.000\n0.000\n0.982\n0.000\n0.000\n\n\nc7\n0.000\n0.000\n0.000\n0.001\n0.000\n0.000\n0.997\n0.000\n\n\nc8\n0.000\n0.001\n0.000\n0.000\n0.000\n0.000\n0.000\n0.970\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA-11 Clusters\n\n\np1\np10\np11\np2\np3\np4\np5\np6\np7\np8\np9\n\n\n\n\nFull-11 Clusters\nc1\n0.902\n0.000\n0.000\n0.032\n0.000\n0.000\n0.015\n0.000\n0.000\n0.000\n0.000\n\n\nc10\n0.000\n0.000\n0.970\n0.000\n0.000\n0.004\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nc11\n0.000\n0.000\n0.000\n0.000\n0.000\n0.171\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nc2\n0.001\n0.000\n0.000\n0.845\n0.000\n0.017\n0.013\n0.000\n0.000\n0.000\n0.000\n\n\nc3\n0.000\n0.000\n0.000\n0.000\n0.938\n0.029\n0.004\n0.000\n0.009\n0.000\n0.001\n\n\nc4\n0.000\n0.000\n0.000\n0.007\n0.003\n0.217\n0.703\n0.000\n0.000\n0.000\n0.000\n\n\nc5\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.987\n0.001\n0.000\n0.000\n\n\nc6\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.003\n0.969\n0.000\n0.000\n\n\nc7\n0.000\n0.002\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.990\n0.000\n\n\nc8\n0.000\n0.000\n0.000\n0.000\n0.000\n0.016\n0.000\n0.001\n0.000\n0.000\n0.973\n\n\nc9\n0.000\n0.993\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.002\n0.000"
  },
  {
    "objectID": "25-pca-cluster-pairwise-comp.html#overview",
    "href": "25-pca-cluster-pairwise-comp.html#overview",
    "title": "18  PCA Cluster pairwise comparisons",
    "section": "18.1 Overview",
    "text": "18.1 Overview\nPairwise comparisons with the validation data for the clusterings produced with PCA model version. We have 140 validation points"
  },
  {
    "objectID": "25-pca-cluster-pairwise-comp.html#validation-points-per-cluster",
    "href": "25-pca-cluster-pairwise-comp.html#validation-points-per-cluster",
    "title": "18  PCA Cluster pairwise comparisons",
    "section": "18.2 Validation points per cluster",
    "text": "18.2 Validation points per cluster\nHere I am illustrating how many independent validation points we have for each cluster assignment, for the different model options from k = 6, 8, 11.\nNote that clusters with only 1 member won’t be included in pairwise comparison b/c not possible to calculate variance.."
  },
  {
    "objectID": "25-pca-cluster-pairwise-comp.html#soil-properties-for-pairwise-comparisons",
    "href": "25-pca-cluster-pairwise-comp.html#soil-properties-for-pairwise-comparisons",
    "title": "18  PCA Cluster pairwise comparisons",
    "section": "18.3 Soil properties for pairwise comparisons",
    "text": "18.3 Soil properties for pairwise comparisons\nBecause our validation data points are coming from different projects / datasets, we don’t have exactly the same variables from each one. This is a reminder of which variables exist in the three sources we used for validation points:\n\nKSSL : clay, bulk density, lep, awc, ec, cec, pH, carbonates, organic matter, (est org C)\nCIG: clay, bulk density, pH, carbonates, organic matter, (est org C)\nSHI: clay, bulk density, pH, organic matter\n\nIn summary: all three datasets include bulk density, pH, organic matter, and clay, and KSSL and CIG include carbonates. So I think it makes sense to focus on plotting and doing pairwise comparisons with these variables specifically"
  },
  {
    "objectID": "25-pca-cluster-pairwise-comp.html#example-pairwise-om-k6",
    "href": "25-pca-cluster-pairwise-comp.html#example-pairwise-om-k6",
    "title": "18  PCA Cluster pairwise comparisons",
    "section": "18.4 Example pairwise: OM, k=6",
    "text": "18.4 Example pairwise: OM, k=6\nWorking out the steps I need to include in a function to do the pairwise comparisons.\n\n# test case k=6 version and organic matter\n\ntest_dat &lt;- val_dat %&gt;% \n  select(val_unit_id, k_6, om_loi, claytotal, source) %&gt;% \n  drop_na(om_loi)\n\n# note only 1 observation for clust_1, need to drop it\n# b/c can't calculate variance for the pairwise comparison w/ only 1 obx.\ntest_dat %&gt;% count(k_6)\n\n\n\n  \n\n\ntest_dat_mult &lt;- test_dat %&gt;% filter(k_6 != \"clust_1\")\n\n# plot to see distributions \ntest_dat_mult %&gt;% \n  ggplot(aes(x = k_6, y = om_loi)) + \n  geom_boxplot() + \n  geom_point() + \n  theme_bw()\n\n\n\ntest_lm &lt;- lm(formula = om_loi ~ k_6,\n   data = test_dat_mult)\n\n# look at some diagnostic plots for our model \n# note homogeneity of variance looks sketchy\nperformance::check_model(test_lm, check = c(\"normality\", \"homogeneity\", \"linearity\"))\n\n\n\nperformance::check_homogeneity(test_lm)\n\nWarning: Variances differ between groups (Bartlett Test, p = 0.000).\n\n\nThis shows how I would do pairwise t-tests with unpooled variance. This might be more appropriate than the ANOVA approach I originally tried, because the diagnostic plots above suggest that we don’t have homogeneity of variance for our validation dataset.\n“For some examples, one can use both the pooled t-procedure and the separate variances (non-pooled) t-procedure and obtain results that are close to each other. However, when the sample standard deviations are very different from each other, and the sample sizes are different, the separate variances 2-sample t-procedure is more reliable.” Penn State STAT 800 “Applied Research Methods” 5.6.1.2\nNote that the p-value adjustment method here is “holm”.\n\n# view the standard (console) output\nwith(test_dat_mult, pairwise.t.test(om_loi, k_6, pool.sd = FALSE)) \n\n\n    Pairwise comparisons using t tests with non-pooled SD \n\ndata:  om_loi and k_6 \n\n        clust_2 clust_3 clust_4\nclust_3 0.011   -       -      \nclust_4 0.412   0.953   -      \nclust_5 0.164   0.862   0.953  \n\nP value adjustment method: holm \n\ntest_obj &lt;- with(test_dat_mult, pairwise.t.test(om_loi, k_6, pool.sd = FALSE)) \n\nclass(test_obj)\n\n[1] \"pairwise.htest\"\n\nbiostat::make_cld(test_obj)\n\n\n\n  \n\n\n# tidy output, filter to significant comparisons only\nwith(test_dat_mult, pairwise.t.test(om_loi, k_6, pool.sd = FALSE)) %&gt;% broom::tidy() %&gt;% \n  filter(p.value &lt; 0.05)"
  },
  {
    "objectID": "25-pca-cluster-pairwise-comp.html#function-for-pairwise-comparisons",
    "href": "25-pca-cluster-pairwise-comp.html#function-for-pairwise-comparisons",
    "title": "18  PCA Cluster pairwise comparisons",
    "section": "18.5 Function for pairwise comparisons",
    "text": "18.5 Function for pairwise comparisons\nArguments: soil property and k option (model version / number of clusters) and validation dataframe.\nReturns: all pairwise comparisons in a dataframe\n\ncompare_clust_pairwise &lt;- function(soil_var, k_opt, df) {\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # can't do pairwise t-tests with only 1 obs in a \n  # group, so need to filter those out \n  \n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n  \n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;% \n    filter(n == 1) %&gt;% \n    pull(.data[[k_opt]])\n  \n  if(length(single_obs_clusters) == 0){\n    \n    dat_subset &lt;- dat_no_na\n    \n  }else{\n    \n    dat_subset &lt;- dat_no_na %&gt;% \n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n    \n  } \n  \n  soil_var_vec &lt;- dat_subset %&gt;% pull(soil_var)\n  clust_vec &lt;- dat_subset %&gt;% pull(k_opt)\n\n  pairs_obj &lt;- pairwise.t.test(soil_var_vec,\n                              clust_vec,\n                              pool.sd = FALSE)\n  \n  tidy_pairs_df &lt;- pairs_obj %&gt;%\n    broom::tidy()\n  \n  cld_df &lt;- biostat::make_cld(pairs_obj)\n  \n  results_list &lt;- list(pairs_df = tidy_pairs_df,\n                       cld = cld_df)\n\n  return(results_list)\n  \n}"
  },
  {
    "objectID": "25-pca-cluster-pairwise-comp.html#example-function-output",
    "href": "25-pca-cluster-pairwise-comp.html#example-function-output",
    "title": "18  PCA Cluster pairwise comparisons",
    "section": "18.6 Example function output",
    "text": "18.6 Example function output\nThis is what the pairwise function I wrote above returns:\n\ncompare_clust_pairwise(soil_var = \"ph1to1h2o\",\n                       k_opt = \"k_8\",\n                       df = val_dat) \n\n$pairs_df\n# A tibble: 21 × 3\n   group1  group2   p.value\n   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;\n 1 clust_2 clust_1 1   e+ 0\n 2 clust_3 clust_1 1   e+ 0\n 3 clust_3 clust_2 1   e+ 0\n 4 clust_4 clust_1 1.66e- 1\n 5 clust_4 clust_2 2.39e-13\n 6 clust_4 clust_3 1.10e- 3\n 7 clust_5 clust_1 7.83e- 1\n 8 clust_5 clust_2 4.96e- 4\n 9 clust_5 clust_3 7.43e- 2\n10 clust_5 clust_4 2.43e- 3\n# … with 11 more rows\n\n$cld\n    group  cld spaced_cld\n1 clust_1 abcd       abcd\n2 clust_2    a       a___\n3 clust_3   ab       ab__\n4 clust_4    c       __c_\n5 clust_5   bd       _b_d\n6 clust_6    c       __c_\n7 clust_7    d       ___d"
  },
  {
    "objectID": "25-pca-cluster-pairwise-comp.html#run-pairwise-comparisons",
    "href": "25-pca-cluster-pairwise-comp.html#run-pairwise-comparisons",
    "title": "18  PCA Cluster pairwise comparisons",
    "section": "18.7 Run pairwise comparisons",
    "text": "18.7 Run pairwise comparisons\nHere I create a dataframe to hold the results of the pairwise comparisons, then use map2() to iterate over the variables and cluster sizes, running all the tests.\n\n# vars to compare on \nvar_names &lt;- c(\"claytotal\", \"ph1to1h2o\", \"om_loi\", \"caco3\", \"dbthirdbar\")\n\n# all possible values of k (number of clusters)\ncluster_opts &lt;- glue(\"k_{c(4, 5, 6, 7, 8, 9, 10, 11)}\")\n\n# create df with all combinations of var_names x clusters\ncomp_template &lt;- tidyr::crossing(var_names, cluster_opts)\n\n# run the pairwise comparisons for each var and cluster size\ndiffs_df &lt;- comp_template %&gt;%\n  mutate(comps_all = map2(.x = var_names,\n                          .y = cluster_opts,\n                          .f = compare_clust_pairwise,\n                          df = val_dat))\n\n# unnest once to get the pairs_df and cld_df dfs \n# as their own columns\ndiffs_unnest &lt;- diffs_df %&gt;% \n  unnest(comps_all) %&gt;% \n  mutate(obj_names = names(comps_all))\n\n# want to save pairs_df and cld_df dat separately, so filtering\n# and then unnesting again to get rectangular data \npairs_dat &lt;- diffs_unnest %&gt;% \n  filter(obj_names == \"pairs_df\") %&gt;% \n  unnest(comps_all) %&gt;% \n  select(-obj_names)\n\nwrite_csv(pairs_dat, \"data/pca_pairwise_results_all.csv\")\n\ncld_dat &lt;- diffs_unnest %&gt;% \n  filter(obj_names == \"cld\") %&gt;% \n  unnest(comps_all) %&gt;% \n  select(-obj_names)\n\nwrite_csv(cld_dat, \"data/cld_display.csv\")\n\nNow need to count how many of the tests have an adjusted p-value &lt; 0.05. All of the p-values are adjusted with the Holm method.\n\ncount_sig_comps &lt;- function(df){\n  \n  df %&gt;% \n    filter(p.value&lt;0.05) %&gt;% \n    nrow()\n  \n}\n\nsig_diffs_df &lt;- pairs_dat %&gt;%\n  group_by(cluster_opts, var_names) %&gt;%\n  nest(data = c(group1, group2, p.value)) %&gt;%\n  mutate(n_sig_comps = map_int(data, count_sig_comps)) %&gt;%\n  mutate(\n    num_regions = as.numeric(str_extract(cluster_opts, \"[:digit:]+\")),\n    possible_comps = (num_regions * (num_regions - 1)) / 2,\n    alpha_comps = round(possible_comps * 0.05, digits = 0)\n  ) %&gt;%\n  select(var_names,\n         cluster_opts,\n         num_regions,\n         data,\n         n_sig_comps,\n         possible_comps,\n         alpha_comps)\n\nsig_diffs_summary &lt;- sig_diffs_df %&gt;% \n  select(var_names,\n         cluster_opts,\n         num_regions,\n         n_sig_comps,\n         possible_comps,\n         alpha_comps)\n\nwrite_csv(sig_diffs_summary, \"data/pca_pairwise_comparisons_summary.csv\")"
  },
  {
    "objectID": "25-pca-cluster-pairwise-comp.html#plot-comparisons",
    "href": "25-pca-cluster-pairwise-comp.html#plot-comparisons",
    "title": "18  PCA Cluster pairwise comparisons",
    "section": "18.8 Plot comparisons",
    "text": "18.8 Plot comparisons\n\n18.8.1 All clusters\n\n\n\n\n\nFor context, this plot shows a black line for the total number of possible contrasts. Note that because we are showing each soil property variable separately, the “total possible” line illustrates the total number of possible comparisons for a single variable\n\n\n\n\n\nTwo other ways to contextualize the number of significant contrasts: 1) with a table, and 2) with a plot showing how the % significant contrasts (as a function of total possible) changes as the number of clusters goes up."
  },
  {
    "objectID": "26-notes-pca6-8.html#overview",
    "href": "26-notes-pca6-8.html#overview",
    "title": "19  Comparing PCA 6 & PCA 8 Models",
    "section": "19.1 Overview",
    "text": "19.1 Overview\nMy plan is to make maps of the k=6-8 clusterings (PCA version) to help explain differences between them, and potentially provide practical / interpretive evidence for why we might choose one over another. My gut feeling is that k=8 makes sense because it breaks up some of the biggest cluster groups (can see this in the alluvial plot comparing the PCA k_6 with the PCA k_8"
  },
  {
    "objectID": "26-notes-pca6-8.html#alluvial-plot",
    "href": "26-notes-pca6-8.html#alluvial-plot",
    "title": "19  Comparing PCA 6 & PCA 8 Models",
    "section": "19.2 Alluvial plot",
    "text": "19.2 Alluvial plot\nThe three largest clusters (k_6) are:\n\nClust 2 (n=2158)\nClust 3 (n=1664)\nClust 4 (n=1436)\n\nMoving from PCA k_6 to PCA k_8 versions of the model, the big differences we see are that the 3 largest clusters (6-2, 6-3, 6-4) split to create to two additional, intermediate clusters (8-3 and 8-2). An additional result of this split is that we have a smaller cluster (n=971) of the highest clay mukeys remaining in 8-5.\n\n~ 100 mukeys from the 6-1 (coarsest) and ~1200 mukeys from 6-2 (coarse-loamy) split off to form 8-3.\nThe remaining ~900 mukeys from 6-2 combine with ~500 mukeys from 6-3 to form 8-2, leaving the remaining ~1000 mukeys with the highest clay from 6-3 to become 8-5"
  },
  {
    "objectID": "26-notes-pca6-8.html#k6",
    "href": "26-notes-pca6-8.html#k6",
    "title": "19  Comparing PCA 6 & PCA 8 Models",
    "section": "19.3 k=6",
    "text": "19.3 k=6"
  },
  {
    "objectID": "26-notes-pca6-8.html#k8",
    "href": "26-notes-pca6-8.html#k8",
    "title": "19  Comparing PCA 6 & PCA 8 Models",
    "section": "19.4 k=8",
    "text": "19.4 k=8\nGoing from k=6 to k=8, the main difference is"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#overview",
    "href": "27-pairwise-nonparametric.html#overview",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.1 Overview",
    "text": "20.1 Overview\nCreating a page for non-parametric pairwise comparisons, because my assessment of the validation data is that we are violating the homogeneity of variance assumption, which we would normally require before doing ANOVA + Tukey’s HSD to test differences between groups."
  },
  {
    "objectID": "27-pairwise-nonparametric.html#validation-points-per-cluster",
    "href": "27-pairwise-nonparametric.html#validation-points-per-cluster",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.2 Validation points per cluster",
    "text": "20.2 Validation points per cluster\nHere I am illustrating how many independent validation points we have for each cluster assignment, for the different model options from k = 4, 6, 8.\nNote that clusters with only 1 member won’t be included in pairwise comparison b/c not possible to calculate variance with only 1 member…"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#soil-properties-for-pairwise-comparisons",
    "href": "27-pairwise-nonparametric.html#soil-properties-for-pairwise-comparisons",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.3 Soil properties for pairwise comparisons",
    "text": "20.3 Soil properties for pairwise comparisons\nBecause our validation data points are coming from different projects / datasets, we don’t have exactly the same variables from each project. This is a reminder of which variables exist in the three sources we used for validation points:\n\nKSSL : clay, bulk density, lep, awc, ec, cec, pH, carbonates, organic matter, (est org C)\nCIG: clay, bulk density, pH, carbonates, organic matter, (est org C)\nSHI: clay, bulk density, pH, organic matter\n\nIn summary: all three datasets include bulk density, pH, organic matter, and clay, and KSSL and CIG include carbonates. So I think it makes sense to focus on plotting and doing pairwise comparisons with these variables specifically"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#example-pairwise-om-k6",
    "href": "27-pairwise-nonparametric.html#example-pairwise-om-k6",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.4 Example pairwise: OM, k=6",
    "text": "20.4 Example pairwise: OM, k=6\nWorking out the steps I need to include in a function to do the pairwise comparisons.\n\n# test case k=6 version and organic matter\n\ntest_dat &lt;- val_dat %&gt;% \n  select(val_unit_id, k_6, om_loi, claytotal, source) %&gt;% \n  drop_na(om_loi)\n\n# note only 1 observation for clust_1, need to drop it\n# b/c can't calculate variance for the pairwise comparison w/ only 1 obs.\ntest_dat %&gt;% count(k_6)\n\n\n\n  \n\n\ntest_dat_mult &lt;- test_dat %&gt;% filter(k_6 != \"clust_1\")\n\n# plot to see distributions \ntest_dat_mult %&gt;% \n  ggplot(aes(x = k_6, y = om_loi)) + \n  geom_boxplot() + \n  geom_point() + \n  theme_bw()\n\n\n\ntest_lm &lt;- lm(formula = om_loi ~ k_6,\n   data = test_dat_mult)\n\n# look at some diagnostic plots for our model \n# note homogeneity of variance looks sketchy\nperformance::check_model(test_lm, check = c(\"normality\", \"homogeneity\", \"linearity\")) \n\n\n\n# runs Bartlett Test for homogeneity of variance\nperformance::check_homogeneity(test_lm) \n\nWarning: Variances differ between groups (Bartlett Test, p = 0.000).\n\n\nThis shows how I would do a Kruskal-Wallis test followed by pairwise comparisons with Dunn’s test, based on this blog post “Kruskal-Wallis Test in R”\nNote that the p-value adjustment method here is Holm, but there are other options (Bonferroni, Benjamani-Hochberg, etc.)\n\ntest_dat_mult %&gt;% \n  kruskal_test(om_loi ~ k_6)\n\n\n\n  \n\n\n# calculate eta-squared based on the H-statistic multiply by 100 for % variance\n# in dependent variable (OM) explained by the independent variable (clusters)\ntest_dat_mult %&gt;% \n  kruskal_effsize(om_loi ~ k_6)\n\n\n\n  \n\n\n# pairwise comps w/ Dunn's test \ntest_pairs &lt;- test_dat_mult %&gt;%\n  dunn_test(formula = as.formula(\"om_loi ~ k_6\"), p.adjust.method = \"holm\") %&gt;% \n# originally tried formatting the \"comparison\" column \n  # as in biostat::cld documentation, but the \"=0\" part\n  # was messing up the cld labels. \n  mutate(comparison = glue(\"{group1}-{group2}\"))\n\n\nbiostat::make_cld(p.adj ~ comparison, data = test_pairs)"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#function-to-plot-model-checks",
    "href": "27-pairwise-nonparametric.html#function-to-plot-model-checks",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.5 Function to plot model checks",
    "text": "20.5 Function to plot model checks\n\n# working with these variables for the model checks\n# independent:  k= 4, 6, 8\n# dependent:  vars = om_loi, dbthirdbar,  ph1to1h2o, caco3, claytotal\n\ncheck_plots_anova &lt;- function(soil_var, k_opt, df, log_trans_adj = FALSE) {\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # checking if any clusters are represented by 1 or less data points. Want to drop these\n  \n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n  \n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[k_opt]])\n  \n  if (length(single_obs_clusters) == 0) {\n    dat_subset &lt;- dat_no_na\n    \n  } else{\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n    \n  }\n  \n  if(log_trans_adj){\n    \n    # note this is for CaCO3, need to add 1 to avoid\n    # zero values\n    f &lt;- paste0(\"log10(\", soil_var, \"+1)~\", k_opt)\n    \n  }else{\n    \n    f &lt;- paste0(soil_var, \" ~ \", k_opt)\n    \n  }\n  \n  mod &lt;- lm(formula = f,\n            data = dat_subset)\n  \n  check_plots &lt;- performance::check_model(mod, check = c(\"normality\", \"homogeneity\", \"linearity\"))  \n  \n  return(list(var = glue(\"{soil_var}\"),\n              plots = check_plots))\n  \n}"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#k4-model-checks",
    "href": "27-pairwise-nonparametric.html#k4-model-checks",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.6 k=4 model checks",
    "text": "20.6 k=4 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_4\", \n                  df = val_dat,\n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#k6-model-checks",
    "href": "27-pairwise-nonparametric.html#k6-model-checks",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.7 k=6 model checks",
    "text": "20.7 k=6 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_6\", \n                  df = val_dat,\n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#k8-model-checks",
    "href": "27-pairwise-nonparametric.html#k8-model-checks",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.8 k=8 model checks",
    "text": "20.8 k=8 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_8\", \n                  df = val_dat, \n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#function-for-testing-homogeneity-of-variance",
    "href": "27-pairwise-nonparametric.html#function-for-testing-homogeneity-of-variance",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.9 Function for testing homogeneity of variance",
    "text": "20.9 Function for testing homogeneity of variance\nHere I’m running the Levene Test for Equality of Variances as a way to check the homogeneity of variance assumption required for ANOVA. If our k groups fail this test, would be better to do Kruskal-Wallis followed by Dunn’s test for pairwise comparisons instead of ANOVA followed by Tukey’s HSD.\n\nlevene_test_clusts &lt;- function(soil_var, k_opt, df) {\n  \n  ### prep data \n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var)) \n  \n  # can't do pairwise t-tests with only 1 obs in a\n  # group (b/c need variance calc), so want to filter those out\n\n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n\n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[k_opt]])\n\n  if(length(single_obs_clusters) == 0){\n\n    dat_subset &lt;- dat_no_na\n\n  }else{\n\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n\n  }\n  \n  # gets rid of warning from levene_test about factor coercion\n  dat_subset$k_opt_factor &lt;- as.factor(dat_subset %&gt;% pull(k_opt))\n  \n  ### perform levene test\n\n  lev_df &lt;- levene_test(data = dat_subset,\n                        formula = as.formula(paste0(soil_var, \" ~ \", \"k_opt_factor\")), center = median) %&gt;% \n    mutate(var = soil_var, \n           k = k_opt)\n\n  return(lev_df)\n\n}"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#run-levene-tests",
    "href": "27-pairwise-nonparametric.html#run-levene-tests",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.10 Run Levene tests",
    "text": "20.10 Run Levene tests\n\n# example of what levene test function returns: \nlevene_test_clusts(soil_var = \"ph1to1h2o\",\n                       k_opt = \"k_8\",\n                       df = val_dat)\n\n\n\n  \n\n\n# dependent vars\nvar_names &lt;- c(\"claytotal\", \"ph1to1h2o\", \"om_loi\", \"caco3\", \"dbthirdbar\")\n\n# independent vars all possible values of k (number of clusters)\ncluster_opts &lt;- as.character(glue(\"k_{c(4, 5, 6, 7, 8, 9, 10, 11)}\"))\n\n# create df with all combinations of var_names x clusters\nmod_template &lt;- tidyr::crossing(var_names, cluster_opts)\n\n\nph &lt;- map(.x = cluster_opts, \n    .f = ~levene_test_clusts(soil_var = \"ph1to1h2o\",\n                             k_opt = .x, \n                             df = val_dat)) %&gt;% \n  list_rbind()\n\nclay &lt;- map(.x = cluster_opts, \n    .f = ~levene_test_clusts(soil_var = \"claytotal\",\n                             k_opt = .x, \n                             df = val_dat)) %&gt;% \n  list_rbind()\n\nom &lt;- map(.x = cluster_opts, \n    .f = ~levene_test_clusts(soil_var = \"om_loi\",\n                             k_opt = .x, \n                             df = val_dat)) %&gt;% \n  list_rbind()\n\ncarbs &lt;- map(.x = cluster_opts, \n    .f = ~levene_test_clusts(soil_var = \"caco3\",\n                             k_opt = .x, \n                             df = val_dat)) %&gt;% \n  list_rbind()\n\nbd &lt;- map(.x = cluster_opts, \n    .f = ~levene_test_clusts(soil_var = \"dbthirdbar\",\n                             k_opt = .x, \n                             df = val_dat)) %&gt;% \n  list_rbind()\n\nlev_tests &lt;- reduce(.x = list(ph, clay, om, carbs, bd),\n                    .f = bind_rows)"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#check-levene-tests",
    "href": "27-pairwise-nonparametric.html#check-levene-tests",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.11 Check Levene Tests",
    "text": "20.11 Check Levene Tests\nOnly bulk density passes the homogeneity of variance assumption, and it does so consistently for all values of k.\n\nlev_tests %&gt;% \n  filter(p&gt;0.05)"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#function-for-non-parametric-pairwise-comparisons",
    "href": "27-pairwise-nonparametric.html#function-for-non-parametric-pairwise-comparisons",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.12 Function for non-parametric pairwise comparisons",
    "text": "20.12 Function for non-parametric pairwise comparisons\nArguments: soil property and k option (model version / number of clusters) and validation dataframe.\nReturns: all pairwise comparisons in a dataframe\n\ncompare_clust_pairwise &lt;- function(soil_var, k_opt, df) {\n  \n  ### prep data \n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # can't do pairwise t-tests with only 1 obs in a \n  # group, so need to filter those out \n  \n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n  \n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;% \n    filter(n &lt;= 1) %&gt;% \n    pull(.data[[k_opt]])\n  \n  if(length(single_obs_clusters) == 0){\n    \n    dat_subset &lt;- dat_no_na\n    \n  }else{\n    \n    dat_subset &lt;- dat_no_na %&gt;% \n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n    \n  } \n  \n  ### perform pairwise comparison\n  \n  pairs_df &lt;- dunn_test(data = dat_subset,\n                        formula = as.formula(paste0(soil_var, \" ~ \", k_opt)), p.adjust.method = \"holm\") %&gt;% \n    mutate(comparison = glue(\"{group1} - {group2}\"))\n  \n  cld_df &lt;- biostat::make_cld(p.adj ~ comparison, data = pairs_df)\n\n  ### results\n  \n  results_list &lt;- list(pairs_df = pairs_df,\n                       cld = cld_df)\n\n  return(results_list)\n  \n}"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#example-function-output",
    "href": "27-pairwise-nonparametric.html#example-function-output",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.13 Example function output",
    "text": "20.13 Example function output\nThis is what the pairwise function I wrote above returns:\n\nph_test &lt;- compare_clust_pairwise(soil_var = \"ph1to1h2o\",\n                       k_opt = \"k_8\",\n                       df = val_dat) \n\nph_test\n\n$pairs_df\n# A tibble: 21 × 10\n   .y.       group1  group2     n1    n2 statistic        p   p.adj p.adj.signif\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n 1 ph1to1h2o clust_1 clust_2     5    47  -0.330   7.42e- 1 1   e+0 ns          \n 2 ph1to1h2o clust_1 clust_3     5     6  -0.00407 9.97e- 1 1   e+0 ns          \n 3 ph1to1h2o clust_1 clust_4     5     9   2.58    9.76e- 3 1.17e-1 ns          \n 4 ph1to1h2o clust_1 clust_5     5    34   1.47    1.41e- 1 8.49e-1 ns          \n 5 ph1to1h2o clust_1 clust_6     5    12   3.48    5.03e- 4 8.05e-3 **          \n 6 ph1to1h2o clust_1 clust_7     5    27   1.67    9.44e- 2 8.49e-1 ns          \n 7 ph1to1h2o clust_2 clust_3    47     6   0.352   7.25e- 1 1   e+0 ns          \n 8 ph1to1h2o clust_2 clust_4    47     9   4.39    1.15e- 5 2.29e-4 ***         \n 9 ph1to1h2o clust_2 clust_5    47    34   3.82    1.32e- 4 2.37e-3 **          \n10 ph1to1h2o clust_2 clust_6    47    12   6.21    5.45e-10 1.14e-8 ****        \n# ℹ 11 more rows\n# ℹ 1 more variable: comparison &lt;glue&gt;\n\n$cld\n    group cld spaced_cld\n1 clust_1  ab        ab_\n2 clust_2   a        a__\n3 clust_3  ab        ab_\n4 clust_4  bc        _bc\n5 clust_5   b        _b_\n6 clust_6   c        __c\n7 clust_7   b        _b_\n\ncomps &lt;- ph_test[[\"pairs_df\"]]\n\ncomps %&gt;% \n  select(group1, group2, p.adj) %&gt;% \n  filter(p.adj &lt; 0.05)"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#run-pairwise-comparisons",
    "href": "27-pairwise-nonparametric.html#run-pairwise-comparisons",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.14 Run pairwise comparisons",
    "text": "20.14 Run pairwise comparisons\nHere I create a dataframe to hold the results of the pairwise comparisons, then use map2() to iterate over the variables and cluster sizes, running all the tests.\n\n# run the pairwise comparisons for each var and cluster size\ndiffs_df &lt;- mod_template %&gt;%\n  mutate(comps_all = map2(.x = var_names,\n                          .y = cluster_opts,\n                          .f = compare_clust_pairwise,\n                          df = val_dat))\n\n# unnest once to get the pairs_df and cld_df dfs \n# as their own columns\ndiffs_unnest &lt;- diffs_df %&gt;% \n  unnest(comps_all) %&gt;% \n  mutate(obj_names = names(comps_all))\n\n# want to save pairs_df and cld_df dat separately, so filtering\n# and then unnesting again to get rectangular data \npairs_dat &lt;- diffs_unnest %&gt;% \n  filter(obj_names == \"pairs_df\") %&gt;% \n  unnest(comps_all) %&gt;% \n  select(-obj_names)\n\nwrite_csv(pairs_dat, \"data/pca_nonpara_pairwise_results_all.csv\")\n\ncld_dat &lt;- diffs_unnest %&gt;% \n  filter(obj_names == \"cld\") %&gt;% \n  unnest(comps_all) %&gt;% \n  select(-obj_names)\n\nwrite_csv(cld_dat, \"data/cld_display_non_para.csv\")\n\nNow need to count how many of the tests have an adjusted p-value &lt; 0.05. All of the p-values are already adjusted with the Holm method (you can see this in the function definition for compare_clust_pairwise() )\n\ncount_sig_comps &lt;- function(df){\n  \n  df %&gt;% \n    filter(p.adj&lt;0.05) %&gt;% \n    nrow()\n  \n}\n\nsig_diffs_df &lt;- pairs_dat %&gt;%\n  select(cluster_opts, var_names, group1, group2, p.adj) %&gt;% \n  group_by(cluster_opts, var_names) %&gt;%\n  nest(data = c(group1, group2, p.adj)) %&gt;%\n  mutate(n_sig_comps = map_int(data, count_sig_comps)) %&gt;%\n  mutate(\n    num_regions = as.numeric(str_extract(cluster_opts, \"[:digit:]+\")),\n    possible_comps = (num_regions * (num_regions - 1)) / 2,\n    alpha_comps = round(possible_comps * 0.05, digits = 0)\n  ) %&gt;%\n  select(var_names,\n         cluster_opts,\n         num_regions,\n         data,\n         n_sig_comps,\n         possible_comps,\n         alpha_comps)\n\nsig_diffs_summary &lt;- sig_diffs_df %&gt;% \n  select(var_names,\n         cluster_opts,\n         num_regions,\n         n_sig_comps,\n         possible_comps,\n         alpha_comps)\n\nwrite_csv(sig_diffs_summary, \"data/pca_nonpara_pairwise_comparisons_summary.csv\")"
  },
  {
    "objectID": "27-pairwise-nonparametric.html#plot-comparisons",
    "href": "27-pairwise-nonparametric.html#plot-comparisons",
    "title": "20  (non-parametric) PCA Cluster pairwise comparisons",
    "section": "20.15 Plot comparisons",
    "text": "20.15 Plot comparisons\n\n20.15.1 All clusters\n\n\n\n\n\nFor context, this plot shows a black line for the total number of possible contrasts. Note that because we are showing each soil property variable separately, the “total possible” line illustrates the total number of possible comparisons for a single variable\n\n\n\n\n\nTwo other ways to contextualize the number of significant contrasts: 1) with a table, and 2) with a plot showing how the % significant contrasts (as a function of total possible) changes as the number of clusters goes up."
  },
  {
    "objectID": "28-compare-variation-demo.html#overview",
    "href": "28-compare-variation-demo.html#overview",
    "title": "21  Compare variation explained",
    "section": "21.1 Overview",
    "text": "21.1 Overview\nThe goal is to use selected soil health indicators from the CIG dataset to compare the amount of variation explained when grouping the data by geographic region (per the CIG design) vs. taxaonomy vs. soil health group (generated by k-means clustering).\nThis is meant to be a demonstration of how the soil health groups generated by k-means might be useful in helping to set benchmarks and expectations for soil health indicators measured on different types of soil. The logic is that by reducing the within-group variation through stratification by group or region, we make it easier to detect changes resulting from management differences."
  },
  {
    "objectID": "28-compare-variation-demo.html#models",
    "href": "28-compare-variation-demo.html#models",
    "title": "21  Compare variation explained",
    "section": "21.2 Models",
    "text": "21.2 Models\nResponse variables: soil health indicators (perhaps the NRCS tech note list?). Should we use just 1 year of data to avoid pulling in year-to-year variability? Could choose 2019, because this is the year for which we have PLFA data, and PLFA is on the NRCS list.\nVariables from the NRCS Soil Health Tech Note recommended methods:\n\nAggregate stability\nPMC\nenzyme activities (BG, NAG, PASE) we have cellobiohydrolase data too, but skipping it b/c we have more missing data for this one (supply chain issues) and it’s not on the NRCS list, which I’m using as my justification for choosing these ones\nPOXC\nACE Protein\nPLFA\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(glue)\nlibrary(ggbeeswarm)\nlibrary(lmerTest)\n\nLoading required package: lme4\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nAttaching package: 'lmerTest'\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(rstatix)\n\nWarning: package 'rstatix' was built under R version 4.2.2\n\n\n\nAttaching package: 'rstatix'\n\nThe following object is masked from 'package:stats':\n\n    filter"
  },
  {
    "objectID": "28-compare-variation-demo.html#data",
    "href": "28-compare-variation-demo.html#data",
    "title": "21  Compare variation explained",
    "section": "21.3 Data",
    "text": "21.3 Data\nAs part of the validation process for my k-means analysis, I determined the soil health group assigned to each CIG sampling area at the mapunit level (hillslope within management condition).\n\n21.3.1 Load data\n\n# this file created in \"extract_raster_values_at_validation_points.R\"\n# it is at the \"sample\" level\ncig_comps &lt;- read_csv(\"data/cig_incl_components_table.csv\") %&gt;% \n  # for now, just using 2019 data so we avoid the year-to-year variability we\n  # know is there, and b/c we have PLFA data for 2019 only\n  filter(sample_id %in% c(1:243))\n\n\n# this is for disambiguating which MUKEY should be assigned\n# to each CIG val_unit_id (mapunit), because sometimes the\n# GPS points landed in multiple mapunits\n# the code that handles this is in \"extract_raster_values_at_validation_points.R\ncig_mukey_votes &lt;- read_csv(\"data/cig_mukey_votes_for_validation.csv\") \n\n# cluster assigned to each CIG point. this dataset is created in \"prep_validation_data_for_pca_version.R\"\n# it is at the \"validation unit\" level (summarized to hillslope within management)\ncig_val &lt;- read_csv(\"data/validation_data_pca_clusters_and_soil_props.csv\") %&gt;% \n  filter(str_detect(val_unit_id, \"CV\") |\n           str_detect(val_unit_id, \"SH\") |\n         str_detect(val_unit_id, \"UD\")) %&gt;%\n  mutate(region = str_extract(val_unit_id, \"[:alpha:]{2}\"),\n         soil_group = glue(\"grp-{k_8}\")) %&gt;% \n  select(val_unit_id, soil_group, k_8, region)\n\n# want all the CIG lab observations (not averaged to the plot level)\nlab &lt;- read_csv(\"../CIG/cig-main/cig_lab_data_all_20230301.csv\") %&gt;%\n  mutate(val_unit_id = glue(\"{site}-{treatment}-{position}\")) %&gt;%\n  select(\n    sample_id,\n    val_unit_id,\n    region,\n    position,\n    corr_percent_stable_gr2mm,\n    ugC_g_day,\n    BG_activity,\n    NAG_activity,\n    P_activity,\n    poxc_mg_kg,\n    mean_protein_mg_g,\n    mbc_ug_g_soil,\n    mbn_ug_g_soil,\n    total_living_microbial_biomass_plfa_ng_g,\n    total_bacteria_plfa_ng_g,\n    total_fungi_plfa_ng_g,\n    total_bacteria_percent_of_tot_biomass,\n    total_fungi_percent_of_tot_biomass\n  ) %&gt;% \n  filter(sample_id %in% c(1:243))\n\n# missing data due to sample contamination in 2019 samples\n# that were run for PLFA. Pulling MBN values from 2020\n# for this validation site only after confirming that the values \n# are in a similar range for this soil type, hillslope position,\n# and region\nmbn_mw4_cva &lt;-\n  read_csv(\"../CIG/cig-main/cig_lab_data_all_20230301.csv\") %&gt;%\n  mutate(val_unit_id = glue(\"{site}-{treatment}-{position}\")) %&gt;%\n  filter(sample_id %in% c(474:476)) %&gt;%\n  pull(mbn_ug_g_soil) %&gt;%\n  mean(., na.rm = TRUE)\n\n\n\n21.3.2 Summarize to mapunit level\nRecall that the lab data and the component data need to be summarized to the “mapunit” level. This involves averaging the soil properties and determining the majority taxonomic classification for each MUKEY based on the included components (recall that most MUKEYs have just 1 contributing component, so we just need to account for those that have &gt;1 contributing component to determine, for each level of taxonomy I look at, what the majority assignment should be).\nStarting with the taxonomy data:\n\n# how many unique mukeys are we working with?\n(cig_mukeys &lt;- cig_comps %&gt;%\n    pull(mukey) %&gt;%\n    unique() %&gt;%\n    length())\n\n[1] 37\n\n# Q: which mukeys have multiple components contributing data?\n# A: 16 unique mukeys\ncig_comps %&gt;% \n  select(mukey, cokey, taxclname:taxpartsize) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey) %&gt;% \n  count() %&gt;% \n  filter(n&gt;1)\n\n\n\n  \n\n\n# goal: assign each mukey a representative suborder\n# based on the suborders of the components within it.\n# should get back object w/ length 37 (# of unique mukeys)\ncig_comps %&gt;% \n  select(mukey, cokey, comppct_r, taxsuborder) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey, taxsuborder) %&gt;% \n  summarise(tot_percent = sum(comppct_r), .groups = \"drop\") %&gt;% \n  group_by(mukey) %&gt;% \n  slice_max(tot_percent)\n\n\n\n  \n\n\n\nFunction for determining representative taxonomic level . Using similar code to the example with suborder above, this function allows me to summarize the representative taxonomic level (order, suborder, etc.) based on % area.\n\n# calculate representative taxonomy\ncalc_rep_tax &lt;- function(tax_level, comp_df){\n  \n  comp_df %&gt;% \n  select(mukey, cokey, comppct_r, {{tax_level}}) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey, {{tax_level}}) %&gt;% \n  summarise(tot_percent = sum(comppct_r), .groups = \"drop\") %&gt;% \n  group_by(mukey) %&gt;% \n  slice_max(tot_percent) %&gt;% \n    select(-tot_percent)\n  \n}\n\n\norder &lt;- calc_rep_tax(taxorder, cig_comps)\nsuborder &lt;- calc_rep_tax(taxsuborder, cig_comps)\ngrt_grp &lt;- calc_rep_tax(taxgrtgroup, cig_comps)\nsub_grp &lt;- calc_rep_tax(taxsubgrp, cig_comps)\nfamily &lt;- calc_rep_tax(taxclname, cig_comps)\n\ntax_joined &lt;- list(order, suborder, grt_grp, sub_grp, family) %&gt;% \n  purrr::reduce(., left_join, by = \"mukey\")\n\ntax_val_key &lt;- cig_comps %&gt;% \n  select(val_unit_id, mukey) %&gt;% \n  distinct()\n\nNow time to summarize the lab data to the mapunit level, this is easier than the taxonomy because we can just take the average. Recall that we are working with 2019 data only.\n\nlab_summary &lt;- lab %&gt;% \n  select(-sample_id) %&gt;% \n  group_by(val_unit_id) %&gt;% \n  summarise(across(where(is.numeric),\n                   ~mean(.x, na.rm = TRUE)))\n\n\n\n21.3.3 Join taxonomic and lab info with validation dataset\nHere, we create a dataset with one row for each validation unit in the CIG dataset. It includes information about representative taxonomic classification at different levels (order, suborder, great group) and also representative values for the soil health indicators we want to analyze further below when comparing variance explained by different stratification options (region, taxonomy, k-means group).\n\nval_mukey &lt;- left_join(cig_val, cig_mukey_votes, by = \"val_unit_id\") %&gt;% \n  select(-c(n, max_vote))\n\nval_mukey_lab &lt;- left_join(val_mukey, lab_summary, by = \"val_unit_id\")\n\nval_all &lt;- left_join(val_mukey_lab, tax_joined, by = \"mukey\") %&gt;%\n # on list from old key, this id no longer exists\n   filter(val_unit_id != \"RR4-CV-B\") %&gt;% \n  mutate(mbn_ug_g_soil = case_when(\n    # dealing with missing data, see note at end of \n    # \"load data\" code block\n    val_unit_id == \"MW4-CV-A\" ~ mbn_mw4_cva,\n    TRUE ~ mbn_ug_g_soil\n  ))"
  },
  {
    "objectID": "28-compare-variation-demo.html#stratification-options",
    "href": "28-compare-variation-demo.html#stratification-options",
    "title": "21  Compare variation explained",
    "section": "21.4 Stratification options",
    "text": "21.4 Stratification options\nWe could stratify the CIG points based on several different levels of Soil Taxonomy. How do the CIG sampling points break down in terms of number per cluster, and number per taxonomic level(s) (order, suborder, etc)?\nFor k-means clusters/groups: here we see that there is only 1 representative from group 4, so probably won’t be able to use that group for any calculations. But the others are workable.\nFor the different taxonomic levels, suborder seems workable, it has &gt;= 5 validation units in any given level. I think order is too broad, and by the time we get to sub-group, we have two subgroups with just 1 validation unit each, so would have to drop those.\n\nval_all %&gt;% \n  count(k_8)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(region)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(taxorder)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(taxsuborder)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(taxgrtgroup)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(taxsubgrp)\n\n\n\n  \n\n\nval_all %&gt;% count(taxclname)\n\n\n\n  \n\n\nxtabs(~region+taxorder, data = val_all)\n\n      taxorder\nregion Alfisols Mollisols Vertisols\n    MW       13        11         0\n    RR        0        10        11\n    ST       13         7         0\n    SW        0        15         0"
  },
  {
    "objectID": "28-compare-variation-demo.html#crosstabs-w-ud",
    "href": "28-compare-variation-demo.html#crosstabs-w-ud",
    "title": "21  Compare variation explained",
    "section": "21.5 Crosstabs w/ UD",
    "text": "21.5 Crosstabs w/ UD\nWas originally thinking about whether we should include or exclude UD (undisturbed/unfarmed sites) because we know they are a major source of variance (they have much higher values across the board for all these indicators).\n\nxtabs(~soil_group + taxsuborder, data = val_all)\n\n          taxsuborder\nsoil_group Aqualfs Aquerts Aquolls Udalfs Udolls\n     grp-2       5       0       3     10      6\n     grp-3       0       0       0      6      0\n     grp-4       0       0       1      0      0\n     grp-5       0      11      16      0      0\n     grp-6       0       0      10      0      0\n     grp-7       0       0       0      5      7\n\nxtabs(~taxgrtgroup + soil_group, data = val_all)\n\n              soil_group\ntaxgrtgroup    grp-2 grp-3 grp-4 grp-5 grp-6 grp-7\n  Argiaquolls      3     0     0     0     0     0\n  Calciaquerts     0     0     0     5     0     0\n  Calciaquolls     0     0     0     0    10     0\n  Endoaquolls      0     0     1    16     0     0\n  Epiaqualfs       5     0     0     0     0     0\n  Epiaquerts       0     0     0     6     0     0\n  Hapludalfs      10     6     0     0     0     5\n  Hapludolls       6     0     0     0     0     7"
  },
  {
    "objectID": "28-compare-variation-demo.html#crosstabs-wout-ud",
    "href": "28-compare-variation-demo.html#crosstabs-wout-ud",
    "title": "21  Compare variation explained",
    "section": "21.6 Crosstabs w/out UD",
    "text": "21.6 Crosstabs w/out UD\n\nval_farm &lt;- val_all %&gt;% \n  filter(!str_detect(val_unit_id, \"UD\"))\n\nxtabs(~soil_group + region, data = val_farm)\n\n          region\nsoil_group MW RR ST SW\n     grp-2  9  0  6  3\n     grp-3  0  0  4  0\n     grp-4  0  0  0  1\n     grp-5  7  8  0  4\n     grp-6  0  8  0  0\n     grp-7  0  0  6  4\n\nxtabs(~soil_group + taxsuborder, data = val_farm)\n\n          taxsuborder\nsoil_group Aqualfs Aquerts Aquolls Udalfs Udolls\n     grp-2       3       0       3      8      4\n     grp-3       0       0       0      4      0\n     grp-4       0       0       1      0      0\n     grp-5       0       8      11      0      0\n     grp-6       0       0       8      0      0\n     grp-7       0       0       0      4      6\n\nxtabs(~taxgrtgroup + soil_group, data = val_farm)\n\n              soil_group\ntaxgrtgroup    grp-2 grp-3 grp-4 grp-5 grp-6 grp-7\n  Argiaquolls      3     0     0     0     0     0\n  Calciaquerts     0     0     0     4     0     0\n  Calciaquolls     0     0     0     0     8     0\n  Endoaquolls      0     0     1    11     0     0\n  Epiaqualfs       3     0     0     0     0     0\n  Epiaquerts       0     0     0     4     0     0\n  Hapludalfs       8     4     0     0     0     4\n  Hapludolls       4     0     0     0     0     6"
  },
  {
    "objectID": "28-compare-variation-demo.html#plots",
    "href": "28-compare-variation-demo.html#plots",
    "title": "21  Compare variation explained",
    "section": "21.7 Plots",
    "text": "21.7 Plots\n\n# keep only data points for soil regions where we have &gt;1 validation area \nsub_dat &lt;- val_all %&gt;% \n  filter(k_8 %in% c(2, 3, 5:7))\n\n# creating this so I can test difference between including/excluding the UD sites.\nfarmdat &lt;- sub_dat %&gt;% \n  filter(!str_detect(val_unit_id, \"UD\"))\n\nsub_dat %&gt;% \n  ggplot(aes(x = region, y = poxc_mg_kg)) + \n  geom_boxplot() +\n  geom_quasirandom(aes(color = soil_group)) +\n  theme_bw() +\n  ggtitle(\"Region\")\n\n\n\nsub_dat %&gt;% \n  ggplot(aes(x = soil_group, y = poxc_mg_kg)) +\n  geom_boxplot() + \n  geom_quasirandom() +\n  theme_bw() + \n  ggtitle(\"Cluster/Group\")\n\n\n\nsub_dat %&gt;% \n  ggplot(aes(x = taxsuborder, y = poxc_mg_kg)) +\n  geom_boxplot() + \n  geom_quasirandom(aes(color = soil_group)) +\n  theme_bw() +\n  ggtitle(\"Suborder\")\n\n\n\nsub_dat %&gt;% \n  ggplot(aes(x = taxgrtgroup, y = poxc_mg_kg)) +\n  geom_boxplot() + \n  geom_quasirandom(aes(color = soil_group)) +\n  theme_bw() +\n  ggtitle(\"Great Group\") +\n  theme(axis.text.x = element_text(angle = 15))"
  },
  {
    "objectID": "28-compare-variation-demo.html#function-to-plot-model-checks",
    "href": "28-compare-variation-demo.html#function-to-plot-model-checks",
    "title": "21  Compare variation explained",
    "section": "21.8 Function to plot model checks",
    "text": "21.8 Function to plot model checks\n\ncheck_plots_anova &lt;- function(soil_var, strat, df, log_trans) {\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(strat),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # checking if any clusters are represented by 1 or less data points. Want to drop these\n  # probably not necessary now that we dropped group 4 above,\n  # the summary tables under \"Stratification options\" and\n  # \"Crosstabs\" show that all groups have 3 or more members\n  \n  n_obs_per_group &lt;- dat_no_na %&gt;%\n    count(.data[[strat]])\n  \n  single_obs_groups &lt;- n_obs_per_group %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[strat]])\n  \n  if (length(single_obs_groups) == 0) {\n    dat_subset &lt;- dat_no_na\n    \n  } else{\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[strat]] %in% single_obs_groups))\n    \n  }\n  \n  if (log_trans){\n    \n    f &lt;- paste0(\"log(\", soil_var, \") ~ \", strat)\n    \n  }else{\n    \n    f &lt;- paste0(soil_var, \" ~ \", strat)\n    \n  }\n  \n  mod &lt;- lm(formula = f,\n            data = dat_subset)\n  \n  check_plots &lt;- performance::check_model(mod, check = c(\"normality\", \"homogeneity\", \"linearity\"))  \n  \n  return(list(f_used = f,\n              plots = check_plots))\n  \n}"
  },
  {
    "objectID": "28-compare-variation-demo.html#models-to-check",
    "href": "28-compare-variation-demo.html#models-to-check",
    "title": "21  Compare variation explained",
    "section": "21.9 Models to check",
    "text": "21.9 Models to check\nWorking with these independent variables (stratification options):\n\nsoil_group (k-means);\nregion\ntaxsuborder\ntaxgrtgroup\n\n\n(dep_vars &lt;- c(\"corr_percent_stable_gr2mm\",\n               \"ugC_g_day\",\n               \"poxc_mg_kg\",\n               \"mbc_ug_g_soil\",\n               \"mbn_ug_g_soil\", # include MBN?\n               \"total_living_microbial_biomass_plfa_ng_g\",\n               \"total_bacteria_plfa_ng_g\",\n               \"total_fungi_plfa_ng_g\"\n))\n\n[1] \"corr_percent_stable_gr2mm\"               \n[2] \"ugC_g_day\"                               \n[3] \"poxc_mg_kg\"                              \n[4] \"mbc_ug_g_soil\"                           \n[5] \"mbn_ug_g_soil\"                           \n[6] \"total_living_microbial_biomass_plfa_ng_g\"\n[7] \"total_bacteria_plfa_ng_g\"                \n[8] \"total_fungi_plfa_ng_g\"                   \n\n# determined by inspecting histograms below w/ different\n# transformation options\nlog_trans &lt;- c(FALSE, \n               TRUE, \n               FALSE, \n               TRUE,\n               TRUE,\n               TRUE,\n               TRUE,\n               TRUE)\n\n\n21.9.1 Checking distributions\nTransformations needed:\n\nNone: aggregate stability, POXC\nLog10: total biomass PLFA, total bacteria PLFA, total fungi PLFA, MBC, MBN, PMC\n\n\n21.9.1.1 Aggregate stability\n\nplot_transformations(var = corr_percent_stable_gr2mm,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n21.9.1.2 PLFA Indicators\n\nplot_transformations(var = total_living_microbial_biomass_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\nplot_transformations(var = total_bacteria_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\nplot_transformations(var = total_fungi_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n21.9.1.3 Microbial biomass C & N (CFE)\n\nplot_transformations(var = mbc_ug_g_soil,\n                     df = val_all,\n                     nbins = 15)\n\n\n\nplot_transformations(var = mbn_ug_g_soil,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n21.9.1.4 POXC\n\nplot_transformations(var = poxc_mg_kg,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n21.9.1.5 PMC\n\nplot_transformations(var = ugC_g_day,\n                     df = val_all,\n                     nbins = 10)\n\n\n\n\n\n\n\n21.9.2 K-means soil groups\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"soil_group\",\n    df = val_all\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ soil_group\"\n\n[[1]]$plots\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ soil_group\"\n\n[[2]]$plots\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ soil_group\"\n\n[[3]]$plots\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ soil_group\"\n\n[[4]]$plots\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ soil_group\"\n\n[[5]]$plots\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ soil_group\"\n\n[[6]]$plots\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ soil_group\"\n\n[[7]]$plots\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ soil_group\"\n\n[[8]]$plots\n\n\n\n\n\n\n\n21.9.3 Region\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"region\",\n    df = val_all\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ region\"\n\n[[1]]$plots\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ region\"\n\n[[2]]$plots\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ region\"\n\n[[3]]$plots\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ region\"\n\n[[4]]$plots\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ region\"\n\n[[5]]$plots\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ region\"\n\n[[6]]$plots\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ region\"\n\n[[7]]$plots\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ region\"\n\n[[8]]$plots\n\n\n\n\n\n\n\n21.9.4 Suborder\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"taxsuborder\",\n    df = val_all\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ taxsuborder\"\n\n[[1]]$plots\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ taxsuborder\"\n\n[[2]]$plots\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ taxsuborder\"\n\n[[3]]$plots\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ taxsuborder\"\n\n[[4]]$plots\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ taxsuborder\"\n\n[[5]]$plots\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ taxsuborder\"\n\n[[6]]$plots\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ taxsuborder\"\n\n[[7]]$plots\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ taxsuborder\"\n\n[[8]]$plots\n\n\n\n\n\n\n\n21.9.5 Great Group\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"taxgrtgroup\",\n    df = val_all\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ taxgrtgroup\"\n\n[[1]]$plots\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ taxgrtgroup\"\n\n[[2]]$plots\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ taxgrtgroup\"\n\n[[3]]$plots\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ taxgrtgroup\"\n\n[[4]]$plots\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ taxgrtgroup\"\n\n[[5]]$plots\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ taxgrtgroup\"\n\n[[6]]$plots\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ taxgrtgroup\"\n\n[[7]]$plots\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ taxgrtgroup\"\n\n[[8]]$plots"
  },
  {
    "objectID": "28-compare-variation-demo.html#function-to-test-homogeneity-of-variance",
    "href": "28-compare-variation-demo.html#function-to-test-homogeneity-of-variance",
    "title": "21  Compare variation explained",
    "section": "21.10 Function to test homogeneity of variance",
    "text": "21.10 Function to test homogeneity of variance\nHere I’m running the Levene Test for Equality of Variances as a way to check the homogeneity of variance assumption required for ANOVA. If our groups fail this test, would be better to do Kruskal-Wallis test.\n\nlevene_test_clusts &lt;- function(soil_var, strat, df) {\n  \n  ### prep data \n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(strat),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var)) \n  \n  # can't do tests with only 1 obs in a\n  # group (b/c need variance calc), so want to filter those out\n\n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[strat]])\n\n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[strat]])\n\n  if(length(single_obs_clusters) == 0){\n\n    dat_subset &lt;- dat_no_na\n\n  }else{\n\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[strat]] %in% single_obs_clusters))\n\n  }\n\n  # gets rid of warning from levene_test about factor coercion\n  dat_subset$strat_factor &lt;- as.factor(dat_subset %&gt;% pull(strat))\n\n  ### perform levene test\n\n  lev_df &lt;- levene_test(data = dat_subset,\n                        formula = as.formula(paste0(soil_var, \" ~ \", \"strat_factor\")), center = median) %&gt;%\n    mutate(var = soil_var,\n           strat_id = strat)\n\n  return(lev_df)\n\n}"
  },
  {
    "objectID": "28-compare-variation-demo.html#run-levene-tests",
    "href": "28-compare-variation-demo.html#run-levene-tests",
    "title": "21  Compare variation explained",
    "section": "21.11 Run Levene Tests",
    "text": "21.11 Run Levene Tests\nPlan to use the results of these tests to determine which test is appropriate, ANOVA (parametric, variance homogeneous between groups) or Kruskal-Wallis (non-parametric, variance NOT homogeneous between groups).\nIn either case, what I’m trying to get to is a calculation of eta-squared, which “measures the proportion of the total variance in a dependent variable that is associated with the membership of different groups defined by an independent variable” Richardson, 2011 Educational Research Review\nWhen the p-value for the Levene’s test is &lt;0.05 we reject the null hypothesis that the variances of our groups are homogeneous / equal. This means that traditional ANOVA is inappropriate, and we should use a non-parametric / rank based ANOVA like the Kruskal-Wallis test and associated rank-based effect sizes / pariwise comparisons.\n\nstrat_opts &lt;- c(\"soil_group\", \"region\", \"taxsuborder\",\n                \"taxgrtgroup\")\n\nlev_frame &lt;- tidyr::crossing(dep_vars, strat_opts)\n\nlev_all &lt;- lev_frame %&gt;% \n  mutate(lev = map2(.x = dep_vars, .y = strat_opts, \n                    .f = ~levene_test_clusts(\n                      soil_var = .x, \n                      strat = .y, \n                      df = val_all\n                    )))\n\nlev_long &lt;- list_rbind(lev_all$lev) %&gt;% \n  mutate(test_type = case_when(\n    p &lt; 0.05 ~ \"non-parametric\", \n    p &gt;= 0.05 ~ \"parametric\"\n  ))"
  },
  {
    "objectID": "28-compare-variation-demo.html#effect-sizes-for-variance-explained",
    "href": "28-compare-variation-demo.html#effect-sizes-for-variance-explained",
    "title": "21  Compare variation explained",
    "section": "21.12 Effect sizes for variance explained",
    "text": "21.12 Effect sizes for variance explained\nI was originally planning on calculating eta-squared as my effect size, using either: effectsize::eta_squared() for ANOVA or effectsize::rank_eta_squared(). But in reading the documentation for eta_squared() from {effectsize}, I learned that there are other, less biased options for calculating this value: omega-squared and epsilon-squared.\nI found a very helpful, recent reference, Iacobucci et al., (2023), that explains the differences between eta, epsilon, and omega when used as effect sizes of variance explained. See their paper for the details, but the easiest way to think about it is that using epsilon-squared or omega-squared is essentially like reporting an adjusted R2 value instead of a regular R2 value. It’s better to use epsilon squared because the equation accounts for small sample size, which is relevant in our case.\nBelow are a few examples using our dataset, this is just me figuring out what the different functions outputs look like, and how NAs are handled.\n\nthe non-parametric effect sizes are somewhat easier to code because it’s one function call. For the parametric/ANOVA based ones, actually need to create the lm , run ANOVA, then pass that object to the appropriate effectsize function to get variance explained.\nhad I wanted to use the rstatix::kruskal_effsize , would need to drop NAs beforehand, the function does not do this by default. In contrast, the {effectsize} functions all do this, and give you a warning to let you know they did it (helpful).\n\n\n21.12.1 Examples\n\n## non-parametric (rank) version MBC by region \n\n# learned that kruskal_effsize doesn't drop NAs by default,\n# confirming this with a test dataset.\nno_na &lt;- sub_dat %&gt;% \n  filter(!is.na(mbc_ug_g_soil))\n\n# does not drop NAs by default? get a different answer compared to the\n# effectsize::rank_eta_squred if running this rstatix function on data with NAs\n# in the response var column\nrstatix::kruskal_effsize(data = no_na, \n                         formula = mbc_ug_g_soil ~ region)\n\n\n\n  \n\n\n# default behavior is to drop NAs\neffectsize::rank_eta_squared(data = sub_dat, \n                             x = mbc_ug_g_soil ~ region)\n\n\n\n  \n\n\n# how does epsilon squared compare to eta squared?\neffectsize::rank_epsilon_squared(data = sub_dat, \n                                 x = mbc_ug_g_soil ~ region)\n\n\n\n  \n\n\n\nBelow I test out Welch’s ANOVA followed by calculation of epsilon squared. This works, and after reading more from the {effectsize} documentation here I think I understand how. I can pass the result from oneway.test(var.equal = FALSE) to effectsize::epsilon_squared() and there is an intermediate step performed where the F statistic from oneway.test is converted to the effect size (could be partial eta-squared, omega-squared, or epsilon-squared)\n\n## parametric version MBC by soil_group (k-means)\n\n# what about welch's ANOVA? \ntest_welch2 &lt;- rstatix::welch_anova_test(data = no_na,\n                          formula = mbc_ug_g_soil ~ soil_group)\n\n\n\ntest_welch &lt;- oneway.test(data = sub_dat,\n            formula = mbc_ug_g_soil ~ soil_group,\n           var.equal = FALSE,\n            na.action = \"na.omit\")\n\neffectsize::epsilon_squared(test_welch, partial = FALSE)\n\n`var.equal = FALSE` - effect size is an approximation.\n\n\n\n\n  \n\n\ntest_mod &lt;- lm(formula = mbc_ug_g_soil ~ soil_group, \n               data = sub_dat)\n\nlevene_test(formula = mbc_ug_g_soil ~ soil_group, \n               data = sub_dat)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n\n\n  \n\n\naov_obj &lt;- car::Anova(mod = test_mod, type = 2)\n\neffectsize::eta_squared(model = aov_obj,\n                        # partial b/c one-way test \n                        # we don't have other vars to \n                        # break the results down by\n                        partial = FALSE)\n\n\n\n  \n\n\neffectsize::epsilon_squared(model = aov_obj,\n                        # partial b/c one-way test \n                        # we don't have other vars to \n                        # break the results down by\n                        partial = FALSE)\n\n\n\n  \n\n\n\n\n\n21.12.2 Function to calculate epsilon squared\nCan specify parametric or non-parametric.\n\ncalc_epsilon_squared_all &lt;- function(soil_var, strat, type, df = sub_dat) {\n  \n  df_no_nas &lt;- df %&gt;% \n    drop_na(all_of(soil_var))\n \n  my_formula &lt;- as.formula(paste0(soil_var, \" ~ \", strat))\n  \n  if (type == \"parametric\") {\n    \n    mod_obj &lt;- lm(formula = my_formula,\n                  data = df_no_nas)\n    \n    aov_obj &lt;- car::Anova(mod = mod_obj, type = 2)\n    \n    eps_sq &lt;- effectsize::epsilon_squared(model = aov_obj,\n                        # partial FALSE b/c one-way test \n                        # we don't have other vars to \n                        # break the variance down by\n                        partial = FALSE)\n      \n  } else if (type == \"non-parametric\") {\n    \n    eps_sq &lt;- effectsize::rank_epsilon_squared(data = df_no_nas, \n                                 x = my_formula)\n    \n  } else {\n    \n    eps_sq &lt;- \"error-specify test type\"\n    \n  }\n  \n\n  return(eps_sq)\n  \n}\n\ncalc_epsilon_squared_farms &lt;- function(soil_var, strat, type, df_farm = farmdat){\n  \n  calc_epsilon_squared_all(soil_var, strat, type, df_farm)\n  \n}\n\n\n\n21.12.3 Calculate epsilon squared\nWe used the Levene Tests (above) above to determine of our data passed or failed the homogeneity of variance assumption required for traditional ANOVA. Using the dataframe I generated from running the Levene Tests, I can iterate through all the groups and soil variables and use the epsilon squared function above to calculate the appropriate effect size (parametric or non-parametric).\n\neps_inputs &lt;- lev_long %&gt;% \n  rename(strat = strat_id, \n         soil_var = var, \n         type = test_type) %&gt;% \n  select(soil_var, strat, type) \n\neps_results &lt;- eps_inputs %&gt;% \n  mutate(eps_results = pmap(eps_inputs, calc_epsilon_squared_all))\n\n\neps_farm &lt;- eps_inputs %&gt;% \n  mutate(eps_results = pmap(eps_inputs, calc_epsilon_squared_farms))\n\n\n\n21.12.4 Clean up data\n\nfarm_long &lt;- eps_farm %&gt;% \n  unnest(eps_results) %&gt;% \n  mutate(epsilon_sq = case_when(\n    !is.na(rank_epsilon_squared) ~ rank_epsilon_squared,\n    !is.na(Epsilon2) ~ Epsilon2, \n    TRUE ~ 99999\n  )) %&gt;% \n  select(-c(Epsilon2, rank_epsilon_squared, Parameter, CI, CI_high, CI_low)) %&gt;% \n  mutate(type = case_when(\n    type == \"parametric\" ~ \"p\", \n    type == \"non-parametric\" ~ \"np\"\n  ))\n  \n\neps_long &lt;- eps_results %&gt;% \n  unnest(eps_results) %&gt;% \n  mutate(epsilon_sq = case_when(\n    !is.na(rank_epsilon_squared) ~ rank_epsilon_squared,\n    !is.na(Epsilon2) ~ Epsilon2, \n    TRUE ~ 99999\n  )) %&gt;% \n  select(-c(Epsilon2, rank_epsilon_squared, Parameter, CI, CI_high, CI_low)) %&gt;% \n  mutate(type = case_when(\n    type == \"parametric\" ~ \"p\", \n    type == \"non-parametric\" ~ \"np\"\n  ))\n\neps_wide &lt;- eps_long %&gt;% \n  pivot_wider(values_from = c(\"epsilon_sq\", \"type\"), \n              names_from = \"strat\")\n\n\n\n21.12.5 Plots of variance explained\n\neps_long %&gt;% \n  filter(strat != \"taxgrtgroup\") %&gt;% \n  mutate(soil_var = case_when(\n    soil_var == \"BG_activity\" ~ \"BG Activity\",\n    soil_var == \"NAG_activity\" ~ \"NAG Activity\",\n    soil_var == \"mbc_ug_g_soil\" ~ \"MBC\",\n    soil_var == \"mean_protein_mg_g\" ~ \"ACE Protein\",\n    soil_var == \"P_activity\" ~ \"PASE activity\", \n    soil_var == \"corr_percent_stable_gr2mm\" ~ \"Aggregate Stab.\",\n    soil_var == \"poxc_mg_kg\" ~ \"POXC\", \n    soil_var == \"total_bacteria_plfa_ng_g\" ~ \"Bacteria PLFA (Total)\",\n    soil_var == \"total_fungi_plfa_ng_g\" ~ \"Fungi PLFA\",\n    soil_var == \"total_living_microbial_biomass_plfa_ng_g\" ~ \"Biomass PLFA (Tot.)\", \n    soil_var == \"poxc_mg_kg\" ~ \"POXC\",\n    soil_var == \"ugC_g_day\" ~ \"PMC\"\n  ),\n  strat = case_when(\n    strat == \"region\" ~ \"G-REG\",\n    strat == \"soil_group\" ~ \"KM\",\n    strat == \"taxsuborder\" ~ \"T-SUB\"\n  )) %&gt;% \n  ggplot() +\n  geom_col(aes(x = strat, y = epsilon_sq)) +\n  facet_wrap(vars(soil_var)) + \n  ggtitle(\"W/ UD\")\n\n\n\nfarm_labelled &lt;- farm_long %&gt;% \n  filter(strat != \"taxgrtgroup\") %&gt;% \n  mutate(soil_var = case_when(\n    soil_var == \"BG_activity\" ~ \"BG Activity\",\n    soil_var == \"NAG_activity\" ~ \"NAG Activity\",\n    soil_var == \"mbc_ug_g_soil\" ~ \"MBC\",\n    soil_var == \"mean_protein_mg_g\" ~ \"ACE Protein\",\n    soil_var == \"P_activity\" ~ \"PASE activity\", \n    soil_var == \"corr_percent_stable_gr2mm\" ~ \"Aggregate Stab.\",\n    soil_var == \"poxc_mg_kg\" ~ \"POXC\", \n    soil_var == \"total_bacteria_plfa_ng_g\" ~ \"Bacteria PLFA\",\n    soil_var == \"total_fungi_plfa_ng_g\" ~ \"Fungi PLFA\",\n    soil_var == \"total_living_microbial_biomass_plfa_ng_g\" ~ \"Biomass PLFA (Tot.)\", \n    soil_var == \"poxc_mg_kg\" ~ \"POXC\",\n    soil_var == \"ugC_g_day\" ~ \"PMC\"\n  ),\n  strat = case_when(\n    strat == \"region\" ~ \"GEOG REGION\",\n    strat == \"soil_group\" ~ \"KM GROUP\",\n    strat == \"taxsuborder\" ~ \"SUBORDER\"\n  )) %&gt;%\n  group_by(soil_var) %&gt;%\n  mutate(max_epsilon = max(epsilon_sq),\n         top_strat = case_when(\n           epsilon_sq == max_epsilon ~ \"Y\", \n           TRUE ~ \"N\"\n         ))\n\nfarm_labelled %&gt;% \n  ggplot() +\n  geom_col(aes(x = strat, y = epsilon_sq)) +\n  facet_wrap(vars(soil_var)) +\n  ggtitle(\"Without UD\")\n\n\n\n\n\nreg_best &lt;- farm_labelled %&gt;%\n  filter(strat == \"GEOG REGION\",\n         top_strat == \"Y\") %&gt;% \n  arrange(epsilon_sq)\n\nkm_best &lt;- farm_labelled %&gt;% \n  filter(strat == \"KM GROUP\", \n         top_strat == \"Y\") %&gt;% \n  arrange(epsilon_sq)\n\nsub_best &lt;- farm_labelled %&gt;% \n  filter(strat == \"SUBORDER\", \n         top_strat == \"Y\") %&gt;% \n  arrange(epsilon_sq)\n\nplot_var_order &lt;- c(reg_best$soil_var, km_best$soil_var, \n                    sub_best$soil_var)\n\n\neps_bar_plot &lt;- farm_labelled %&gt;%\n  ggplot() +\n  geom_col(aes(x = soil_var, y = epsilon_sq, fill = strat),\n           position = position_dodge(),\n           width = 0.7) +\n  geom_text(aes(x = 1, y = 0.6, label = \"SUBORDER\"), size = 3) +\n  geom_text(aes(x = 3, y = 0.6, label = \"KM GROUP\"), size = 3) +\n  geom_text(aes(x = 8, y = 0.57, label = \"GEOG REGION\"), size = 3) +\n  scale_x_discrete(limits = rev(plot_var_order)) + \n  scale_y_continuous(breaks = seq(0, 0.7, 0.1)) +\n  geom_vline(aes(xintercept = 1.5)) +\n  geom_vline(aes(xintercept = 4.5)) +\n  geom_text(aes(x = 11, y = 0.32, label = \"Explains most variance?\"),\n            hjust = 0,\n            size = 4) +\n  coord_cartesian(xlim = c(0, 0.7),\n                  clip = 'off') +\n  coord_flip() +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  xlab(\"\") +\n  ylab(\"Epsilon Squared\") +\n  scale_fill_brewer(type = \"qual\", palette = 2) +\n  guides(fill = guide_legend(title = \"Groups\"))\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\nggsave(\"figs/epsilon_sq_barplot.png\", plot = eps_bar_plot, \n       width = 5, height = 5, units = \"in\")\n\n\n\n21.12.6 What is going on with PMC?\nPMC returns 0% variance explained for all three stratification options: K-means, Region, and Taxonomic sub-group. Is this really correct? Let’s double check below with the region stratification to make sure there isn’t something weird with the data.\n\nlm_pmc &lt;- lm(ugC_g_day ~ region, data = sub_dat, na.action = \"na.omit\")\n\naov_pmc &lt;- car::Anova(lm_pmc, type = 2)\n\n# big residuals number... that explains it\naov_pmc\n\n\n\n  \n\n\neffectsize::epsilon_squared(aov_pmc, partial = FALSE)\n\n\n\n  \n\n\n# what about eta-squared?\neffectsize::eta_squared(aov_pmc, partial = FALSE)\n\n\n\n  \n\n\n# what if we drop the UD sites?\n\nfarmdat &lt;- sub_dat %&gt;% filter(!str_detect(val_unit_id, \"UD\"))\n\nlm_farm &lt;- lm(ugC_g_day ~ region, data = farmdat, na.action = \"na.omit\")\n\naov_farm &lt;- car::Anova(lm_farm, type = 2)\n\n# big residuals number... that explains it\naov_farm\n\n\n\n  \n\n\neffectsize::epsilon_squared(aov_farm, partial = FALSE)\n\n\n\n  \n\n\n# what about eta-squared?\neffectsize::eta_squared(aov_farm, partial = FALSE)"
  },
  {
    "objectID": "28-compare-variation-demo.html#anova-models-poxc",
    "href": "28-compare-variation-demo.html#anova-models-poxc",
    "title": "21  Compare variation explained",
    "section": "21.13 ANOVA models POXC",
    "text": "21.13 ANOVA models POXC\nTrying this after reading Mourtzinis et al., 2020 about stratifying producer fields to better explain soybean yield variability\n\n# using poxc as example \n# check for missing values\nnrow(sub_dat %&gt;% filter(is.na(poxc_mg_kg)))\n\n# summarise to mapunit level (val_unit_ids)\npoxc_test &lt;- sub_dat %&gt;% \n  filter(!is.na(poxc_mg_kg)) %&gt;% \n  group_by(val_unit_id, region, k_8) %&gt;% \n  summarise(mean_poxc = mean(poxc_mg_kg),\n            .groups = \"drop\") %&gt;% \n  mutate(k_8 = glue(\"cl_{k_8}\"))\n\nreglm &lt;- lm(mean_poxc ~ region, data = poxc_test)\n#regposlm &lt;- lm(mean_poxc ~ region*position, data = poxc_test)\nclustlm &lt;- lm(mean_poxc ~ k_8, data = poxc_test)\n\nperformance::check_model(reglm, check = c(\"normality\", \"linearity\", \"homogeneity\", \"outliers\"))\n\nanova_reg &lt;- car::Anova(reglm) %&gt;% broom::tidy() %&gt;% column_to_rownames(\"term\")\n\n# anova_regpos &lt;- car::Anova(regposlm) %&gt;% broom::tidy() %&gt;% column_to_rownames(\"term\")\n  \nanova_clust &lt;- car::Anova(clustlm) %&gt;% broom::tidy() %&gt;% \n  column_to_rownames(\"term\")\n\n# what percentage of variation is explained by the \"region\" term\n# vs. the \"cluster\" term vs. the region and cluster terms (and their interaction?)\n\nss_reg &lt;- anova_reg['region', 'sumsq']/sum(anova_reg['sumsq'])*100\n\n# ss_regpos &lt;- (anova_regpos['region', 'sumsq'] + \n#     anova_regpos['position', 'sumsq'] + \n#     anova_regpos['region:position', 'sumsq'] ) /sum(anova_reg['sumsq'])*100\n\nss_clust &lt;- anova_clust['k_8', 'sumsq']/sum(anova_clust['sumsq'])*100\n\n\nss_reg\nss_regpos\nss_clust"
  },
  {
    "objectID": "28-compare-variation-demo.html#anova-models-mbc",
    "href": "28-compare-variation-demo.html#anova-models-mbc",
    "title": "21  Compare variation explained",
    "section": "21.14 ANOVA models MBC",
    "text": "21.14 ANOVA models MBC\n\n# using poxc as example \n# check for missing values\nnrow(sub_dat %&gt;% filter(is.na(mbc_ug_g_soil)))\n\nmbc_test &lt;- sub_dat %&gt;% \n  filter(!is.na(mbc_ug_g_soil)) %&gt;% \n  group_by(val_unit_id, region, k_8, position) %&gt;% \n  summarise(mean_mbc = mean(mbc_ug_g_soil),\n            .groups = \"drop\") %&gt;% \n  mutate(k_8 = glue(\"cl_{k_8}\"))\n\nreglm &lt;- lm(mean_mbc ~ region, data = mbc_test)\nregposlm &lt;- lm(mean_mbc ~ region*position, data = mbc_test)\nclustlm &lt;- lm(mean_mbc ~ k_8, data = mbc_test)\n\nanova_reg &lt;- car::Anova(reglm) %&gt;% broom::tidy() %&gt;% column_to_rownames(\"term\")\n\nanova_regpos &lt;- car::Anova(regposlm) %&gt;% broom::tidy() %&gt;% column_to_rownames(\"term\")\n  \nanova_clust &lt;- car::Anova(clustlm) %&gt;% broom::tidy() %&gt;% \n  column_to_rownames(\"term\")\n\n# what percentage of variation is explained by the \"region\" term\n# vs. the \"cluster\" term vs. the region and cluster terms (and their interaction?)\n\nss_reg &lt;- anova_reg['region', 'sumsq']/sum(anova_reg['sumsq'])*100\n\nss_regpos &lt;- (anova_regpos['region', 'sumsq'] + \n    anova_regpos['position', 'sumsq'] + \n    anova_regpos['region:position', 'sumsq'] ) /sum(anova_reg['sumsq'])*100\n\nss_clust &lt;- anova_clust['k_8', 'sumsq']/sum(anova_clust['sumsq'])*100\n\n\nss_reg\nss_regpos\nss_clust"
  },
  {
    "objectID": "28-compare-variation-demo.html#old-calculate-coefficient-of-variation",
    "href": "28-compare-variation-demo.html#old-calculate-coefficient-of-variation",
    "title": "21  Compare variation explained",
    "section": "21.15 (Old) Calculate coefficient of variation",
    "text": "21.15 (Old) Calculate coefficient of variation\nOld stuff here but wanted to keep track of the cvequality package link in case it’s useful at some other point… My idea was to use coefficient of variation (%) to compare the spread of the points when grouped by geographic region vs. soil group. I found a helpful R package to do this (which includes stats citations): {cvequality}"
  },
  {
    "objectID": "29-welch-pairwise.html#overview",
    "href": "29-welch-pairwise.html#overview",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.1 Overview",
    "text": "22.1 Overview\nDid some more reading on March 15, 2023 and decided a better way to deal with the issue of hetergeneous variance is to do Welch’s ANOVA followed by Games-Howell pairwise comparisons.\nSee Lakens et al. (2019) “Taking Parametric Assumptions Seriously: Arguments for the Use of Welch’s F-test instead of the Classical F-test in One-Way ANOVA” for more info on Welch’s F-test.\nSee Sauder & Demars (2019) plus their supplemental material on OSF for more about the Games-Howell test\nIn R, Wech’s ANOVA can be accomplished with oneway.test(…, var.equal = FALSE) , or {rstatix} has a wrapper function welch_anova_test().\nFor Games-Howell, {rstatix} has games_howell_test() documentation here.\nFrom the documentation (emphasis added):\n\nPerforms Games-Howell test, which is used to compare all possible combinations of group differences when the assumption of homogeneity of variances is violated. This post hoc test provides confidence intervals for the differences between group means and shows whether the differences are statistically significant.\nThe test is based on Welch’s degrees of freedom correction and uses Tukey’s studentized range distribution for computing the p-values. The test compares the difference between each pair of means with appropriate adjustment for the multiple testing. So there is no need to apply additional p-value corrections."
  },
  {
    "objectID": "29-welch-pairwise.html#validation-points-per-cluster",
    "href": "29-welch-pairwise.html#validation-points-per-cluster",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.2 Validation points per cluster",
    "text": "22.2 Validation points per cluster\nHere I am illustrating how many independent validation points we have for each cluster assignment, for the different model options from k = 4, 6, 8.\nNote that clusters with only 1 member won’t be included in pairwise comparison b/c not possible to calculate variance with only 1 member…"
  },
  {
    "objectID": "29-welch-pairwise.html#soil-properties-for-pairwise-comparisons",
    "href": "29-welch-pairwise.html#soil-properties-for-pairwise-comparisons",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.3 Soil properties for pairwise comparisons",
    "text": "22.3 Soil properties for pairwise comparisons\nBecause our validation data points are coming from different projects / datasets, we don’t have exactly the same variables from each project. This is a reminder of which variables exist in the three sources we used for validation points:\n\nKSSL : clay, bulk density, lep, awc, ec, cec, pH, carbonates, organic matter, (est org C)\nCIG: clay, bulk density, pH, carbonates, organic matter, (est org C)\nSHI: clay, bulk density, pH, organic matter\n\nIn summary: all three datasets include bulk density, pH, organic matter, and clay, and KSSL and CIG include carbonates. So I think it makes sense to focus on plotting and doing pairwise comparisons with these variables specifically"
  },
  {
    "objectID": "29-welch-pairwise.html#example-pairwise-om-k6",
    "href": "29-welch-pairwise.html#example-pairwise-om-k6",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.4 Example pairwise: OM, k=6",
    "text": "22.4 Example pairwise: OM, k=6\nWorking out the steps I need to include in a function to do the pairwise comparisons.\n\n# test case k=6 version and organic matter\n\ntest_dat &lt;- val_dat %&gt;% \n  select(val_unit_id, k_6, om_loi, claytotal, source) %&gt;% \n  drop_na(om_loi)\n\n# note only 1 observation for clust_1, need to drop it\n# b/c can't calculate variance for the pairwise comparison w/ only 1 obs.\ntest_dat %&gt;% count(k_6)\n\n\n\n  \n\n\ntest_dat_mult &lt;- test_dat %&gt;% filter(k_6 != \"clust_1\")\n\n# plot to see distributions \ntest_dat_mult %&gt;% \n  ggplot(aes(x = k_6, y = om_loi)) + \n  geom_boxplot() + \n  geom_point() + \n  theme_bw()\n\n\n\ntest_lm &lt;- lm(formula = om_loi ~ k_6,\n   data = test_dat_mult)\n\n# look at some diagnostic plots for our model \n# note homogeneity of variance looks sketchy\nperformance::check_model(test_lm, check = c(\"normality\", \"homogeneity\", \"linearity\")) \n\n\n\n# runs Bartlett Test for homogeneity of variance\nperformance::check_homogeneity(test_lm) \n\nWarning: Variances differ between groups (Bartlett Test, p = 0.000).\n\n\nThis shows how I would do Games-Howell pairwise comparisons\n\ngh_om &lt;- games_howell_test(formula = om_loi ~ k_6,\n                  data = test_dat_mult)\n\ngh_om\n\n\n\n  \n\n\n# now we add a column \"comparison\" needed in order for \n# biostat::cld to make our \"compact letter display\" (letters\n# showing where we can/can't reject null that means are the same)\n# pairwise comps w/ Dunn's test \n\ngh_comp &lt;- gh_om %&gt;% \n  mutate(comparison = glue(\"{group1}-{group2}\"))\n\n\nbiostat::make_cld(p.adj ~ comparison, data = gh_comp)"
  },
  {
    "objectID": "29-welch-pairwise.html#function-to-plot-model-checks",
    "href": "29-welch-pairwise.html#function-to-plot-model-checks",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.5 Function to plot model checks",
    "text": "22.5 Function to plot model checks\nI want to make some plots to check the assumptions of normally distributed residuals and homogeneity of variance. In chapter 27, where I show the non-parametric pairwise tests, I also have a section for running Levene’s test to check homogeneity of variance. However, since Welch’s ANOVA and Games-Howell make no assumptions about homogeneity variance, I’m not worried about running Levene’s test here.\n\n# working with these variables for the model checks\n# independent:  k= 4, 6, 8\n# dependent:  vars = om_loi, dbthirdbar,  ph1to1h2o, caco3, claytotal\n\ncheck_plots_anova &lt;- function(soil_var, k_opt, df, log_trans_adj = FALSE) {\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # checking if any clusters are represented by 1 or less data points. Want to drop these\n  \n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n  \n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[k_opt]])\n  \n  if (length(single_obs_clusters) == 0) {\n    dat_subset &lt;- dat_no_na\n    \n  } else{\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n    \n  }\n  \n  if(log_trans_adj){\n    \n    # note this is for CaCO3, need to add 1 to avoid\n    # zero values\n    f &lt;- paste0(\"log10(\", soil_var, \"+1)~\", k_opt)\n    \n  }else{\n    \n    f &lt;- paste0(soil_var, \" ~ \", k_opt)\n    \n  }\n  \n  mod &lt;- lm(formula = f,\n            data = dat_subset)\n  \n  check_plots &lt;- performance::check_model(mod, check = c(\"normality\", \"homogeneity\", \"linearity\"))  \n  \n  return(list(var = glue(\"{soil_var}\"),\n              plots = check_plots))\n  \n}"
  },
  {
    "objectID": "29-welch-pairwise.html#k4-model-checks",
    "href": "29-welch-pairwise.html#k4-model-checks",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.6 k=4 model checks",
    "text": "22.6 k=4 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\n# note we need to log10 transform here (+1) to meet the assumption of normally distributed residuals\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_4\", \n                  df = val_dat,\n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots"
  },
  {
    "objectID": "29-welch-pairwise.html#k6-model-checks",
    "href": "29-welch-pairwise.html#k6-model-checks",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.7 k=6 model checks",
    "text": "22.7 k=6 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_6\", \n                  df = val_dat,\n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots"
  },
  {
    "objectID": "29-welch-pairwise.html#k8-model-checks",
    "href": "29-welch-pairwise.html#k8-model-checks",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.8 k=8 model checks",
    "text": "22.8 k=8 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_8\", \n                  df = val_dat, \n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots"
  },
  {
    "objectID": "29-welch-pairwise.html#function-for-games-howell-pairwise-comparisons",
    "href": "29-welch-pairwise.html#function-for-games-howell-pairwise-comparisons",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.9 Function for Games-Howell pairwise comparisons",
    "text": "22.9 Function for Games-Howell pairwise comparisons\nArguments: soil property and k option (model version / number of clusters) and validation dataframe.\nReturns: all pairwise comparisons in a dataframe\n\ncompare_clust_pairwise &lt;- function(soil_var, k_opt, df = val_dat) {\n  ### prep data\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # can't do pairwise t-tests with only 1 obs in a\n  # group, so need to filter those out\n  \n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n  \n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[k_opt]])\n  \n  if (length(single_obs_clusters) == 0) {\n    dat_subset &lt;- dat_no_na\n    \n  } else{\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n    \n  }\n  \n  ### perform pairwise comparison\n  \n    f &lt;- as.formula(paste0(soil_var, \" ~ \", k_opt))\n  \n  pairs_df &lt;- games_howell_test(data = dat_subset,\n                                formula = f) %&gt;%\n    mutate(comparison = glue(\"{group1} - {group2}\"))\n  \n  cld_df &lt;- biostat::make_cld(p.adj ~ comparison,\n                              data = pairs_df)\n  \n  ### results\n  \n  results_list &lt;- list(pairs_df = pairs_df,\n                       cld = cld_df)\n  \n  return(results_list)\n  \n}"
  },
  {
    "objectID": "29-welch-pairwise.html#example-function-output",
    "href": "29-welch-pairwise.html#example-function-output",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.10 Example function output",
    "text": "22.10 Example function output\nThis is what the pairwise function I wrote above returns:\n\nph_test &lt;- compare_clust_pairwise(soil_var = \"ph1to1h2o\",\n                       k_opt = \"k_8\",\n                       df = val_dat) \n\nph_test\n\n$pairs_df\n# A tibble: 21 × 9\n   .y.       group1  group2  estimate conf.low conf.high    p.adj p.adj.signif\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n 1 ph1to1h2o clust_1 clust_2 -0.119     -1.66      1.42  1   e+ 0 ns          \n 2 ph1to1h2o clust_1 clust_3  0.00222   -1.50      1.50  1   e+ 0 ns          \n 3 ph1to1h2o clust_1 clust_4  1.16      -0.399     2.71  1.32e- 1 ns          \n 4 ph1to1h2o clust_1 clust_5  0.566     -0.926     2.06  6.68e- 1 ns          \n 5 ph1to1h2o clust_1 clust_6  1.39      -0.165     2.94  7.4 e- 2 ns          \n 6 ph1to1h2o clust_1 clust_7  0.625     -0.865     2.12  5.87e- 1 ns          \n 7 ph1to1h2o clust_2 clust_3  0.122     -0.451     0.694 9.81e- 1 ns          \n 8 ph1to1h2o clust_2 clust_4  1.28       0.953     1.60  0        ****        \n 9 ph1to1h2o clust_2 clust_5  0.685      0.220     1.15  6.27e- 4 ***         \n10 ph1to1h2o clust_2 clust_6  1.51       1.18      1.84  9.92e-13 ****        \n# ℹ 11 more rows\n# ℹ 1 more variable: comparison &lt;glue&gt;\n\n$cld\n    group cld spaced_cld\n1 clust_1 abc        abc\n2 clust_2   a        a__\n3 clust_3  ab        ab_\n4 clust_4   c        __c\n5 clust_5   b        _b_\n6 clust_6   c        __c\n7 clust_7   b        _b_\n\ncomps &lt;- ph_test[[\"pairs_df\"]]\n\ncomps %&gt;% \n  select(group1, group2, p.adj) %&gt;% \n  filter(p.adj &lt; 0.05)"
  },
  {
    "objectID": "29-welch-pairwise.html#run-pairwise-comparisons",
    "href": "29-welch-pairwise.html#run-pairwise-comparisons",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.11 Run pairwise comparisons",
    "text": "22.11 Run pairwise comparisons\nHere I create a dataframe to hold the results of the pairwise comparisons, then use map2() to iterate over the variables and cluster sizes, running all the tests.\n\n# transform CaCO3 in the dataframe beforehand\nval_dat_trans &lt;- val_dat %&gt;% \n  mutate(caco3 = log10(caco3+1)) \n\n# dependent vars\nsoil_var &lt;- c(\"claytotal\", \"ph1to1h2o\", \"om_loi\", \"caco3\", \"dbthirdbar\")\n\n# independent vars all possible values of k (number of clusters)\nk_opt &lt;- as.character(glue(\"k_{c(4, 5, 6, 7, 8, 9, 10, 11)}\"))\n\n# create df with all combinations of var_names x clusters\nmod_template &lt;- tidyr::crossing(soil_var, k_opt)\n\n\n# run the pairwise comparisons for each var and cluster size\ndiffs_df &lt;- mod_template %&gt;%\n  mutate(comps_all = map2(\n    .x = soil_var,\n    .y = k_opt,\n    .f = compare_clust_pairwise\n  ))\n\n\n\n# unnest once to get the pairs_df and cld_df dfs \n# as their own columns\ndiffs_unnest &lt;- diffs_df %&gt;% \n  unnest(comps_all) %&gt;% \n  mutate(obj_names = names(comps_all))\n\n# want to save pairs_df and cld_df dat separately, so filtering\n# and then unnesting again to get rectangular data \npairs_dat &lt;- diffs_unnest %&gt;% \n  filter(obj_names == \"pairs_df\") %&gt;% \n  unnest(comps_all) %&gt;% \n  select(-obj_names)\n\nwrite_csv(pairs_dat, \"data/pca_welch_pairwise_results_all.csv\")\n\ncld_dat &lt;- diffs_unnest %&gt;% \n  filter(obj_names == \"cld\") %&gt;% \n  unnest(comps_all) %&gt;% \n  select(-obj_names)\n\nwrite_csv(cld_dat, \"data/cld_display_welch.csv\")\n\nNow need to count how many of the tests have an adjusted p-value &lt; 0.05. All of the p-values are already adjusted (see documentation for games_howell_test()\n\ncount_sig_comps &lt;- function(df){\n  \n  df %&gt;% \n    filter(p.adj&lt;0.05) %&gt;% \n    nrow()\n  \n}\n\nsig_diffs_df &lt;- pairs_dat %&gt;%\n  select(k_opt, soil_var, group1, group2, p.adj) %&gt;% \n  group_by(k_opt, soil_var) %&gt;%\n  nest(data = c(group1, group2, p.adj)) %&gt;%\n  mutate(n_sig_comps = map_int(data, count_sig_comps)) %&gt;%\n  mutate(\n    num_regions = as.numeric(str_extract(k_opt, \"[:digit:]+\")),\n    possible_comps = (num_regions * (num_regions - 1)) / 2,\n    alpha_comps = round(possible_comps * 0.05, digits = 0)\n  ) %&gt;%\n  select(soil_var,\n         k_opt,\n         num_regions,\n         data,\n         n_sig_comps,\n         possible_comps,\n         alpha_comps)\n\nsig_diffs_summary &lt;- sig_diffs_df %&gt;% \n  select(soil_var,\n         k_opt,\n         num_regions,\n         n_sig_comps,\n         possible_comps,\n         alpha_comps)\n\nwrite_csv(sig_diffs_summary, \"data/pca_welch_pairwise_comparisons_summary.csv\")"
  },
  {
    "objectID": "29-welch-pairwise.html#plot-comparisons",
    "href": "29-welch-pairwise.html#plot-comparisons",
    "title": "22  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "22.12 Plot comparisons",
    "text": "22.12 Plot comparisons\n\n22.12.1 All clusters\n\n\n\n\n\nFor context, this plot shows a black line for the total number of possible contrasts. Note that because we are showing each soil property variable separately, the “total possible” line illustrates the total number of possible comparisons for a single variable\n\n\n\n\n\nTwo other ways to contextualize the number of significant contrasts: 1) with a table, and 2) with a plot showing how the % significant contrasts (as a function of total possible) changes as the number of clusters goes up."
  },
  {
    "objectID": "30-welch-compare-variation.html#overview",
    "href": "30-welch-compare-variation.html#overview",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.1 Overview",
    "text": "23.1 Overview\nThis chapter is similar to 28 (compare variation demo), but here am using Welch ANOVA followed by calculation of epsilon squared as an effect size (which we can do because {effectsize} uses the F stats from the Welch’s ANOVA to estimate epsilon squared.\nThe goal is to use selected soil health indicators from the CIG dataset to compare the amount of variation explained when grouping the data by geographic region (per the CIG design) vs. taxaonomy vs. soil health group (generated by k-means clustering).\nThis is meant to be a demonstration of how the soil health groups generated by k-means might be useful in helping to set benchmarks and expectations for soil health indicators measured on different types of soil. The logic is that by reducing the within-group variation through stratification by group or region, we make it easier to detect changes resulting from management differences."
  },
  {
    "objectID": "30-welch-compare-variation.html#models",
    "href": "30-welch-compare-variation.html#models",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.2 Models",
    "text": "23.2 Models\nResponse variables: soil health indicators (perhaps the NRCS tech note list?). Should we use just 1 year of data to avoid pulling in year-to-year variability? Could choose 2019, because this is the year for which we have PLFA data, and PLFA is on the NRCS list.\nVariables from the NRCS Soil Health Tech Note recommended methods:\n\nAggregate stability\nPMC\n(skip - known weirdness in CIG data from different pHs/discussed with Nic). enzyme activities (BG, NAG, PASE) we have cellobiohydrolase data too, but skipping it b/c we have more missing data for this one (supply chain issues) and it’s not on the NRCS list, which I’m using as my justification for choosing these ones\nPOXC\n(skip - weird thing with RRV poor extraction in CIG data) ACE Protein\nPLFA\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.2\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(glue)\nlibrary(ggbeeswarm)\nlibrary(lmerTest)\n\nLoading required package: lme4\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nAttaching package: 'lmerTest'\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(rstatix)\n\nWarning: package 'rstatix' was built under R version 4.2.2\n\n\n\nAttaching package: 'rstatix'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.2.3"
  },
  {
    "objectID": "30-welch-compare-variation.html#data",
    "href": "30-welch-compare-variation.html#data",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.3 Data",
    "text": "23.3 Data\nAs part of the validation process for my k-means analysis, I determined the soil health group assigned to each CIG sampling area at the mapunit level (hillslope within management condition).\n\n23.3.1 Load data\n\n# this file created in \"extract_raster_values_at_validation_points.R\"\n# it is at the \"sample\" level\ncig_comps &lt;- read_csv(\"data/cig_incl_components_table.csv\") %&gt;% \n  # for now, just using 2019 data so we avoid the year-to-year variability we\n  # know is there, and b/c we have PLFA data for 2019 only\n  filter(sample_id %in% c(1:243))\n\n\n# this is for disambiguating which MUKEY should be assigned\n# to each CIG val_unit_id (mapunit), because sometimes the\n# GPS points landed in multiple mapunits\n# the code that handles this is in \"extract_raster_values_at_validation_points.R\ncig_mukey_votes &lt;- read_csv(\"data/cig_mukey_votes_for_validation.csv\") \n\n# cluster assigned to each CIG point. this dataset is created in \"prep_validation_data_for_pca_version.R\"\n# it is at the \"validation unit\" level (summarized to hillslope within management)\ncig_clus &lt;- read_csv(\"data/validation_data_pca_clusters_and_soil_props.csv\") %&gt;% \n  filter(str_detect(val_unit_id, \"CV\") |\n           str_detect(val_unit_id, \"SH\") |\n         str_detect(val_unit_id, \"UD\")) %&gt;%\n  mutate(region = str_extract(val_unit_id, \"[:alpha:]{2}\"),\n         soil_group = glue(\"grp-{k_8}\")) %&gt;% \n  select(val_unit_id, soil_group, k_8, region)\n\n# currently from csv saved by \"heatmap_validation_counties.R\", \n# but need to move this to a more logical place \ncig_mlras &lt;- read_csv(\"data/cig_mlra_by_val_unit_id.csv\") \n\ncig_val &lt;- left_join(cig_clus, cig_mlras, by = \"val_unit_id\") %&gt;%\n  # need to fix this NA created by the join, it happened\n  # because this RR4-CV site had a label change from A to B\n  # during the harmonization I did to align soil textures\n  mutate(\n    MLRA_NAME = case_when(\n      val_unit_id == \"RR4-CV-B\" ~ \"Red River Valley of the North\",\n      TRUE ~ MLRA_NAME\n    ),\n    mlra_short = case_when(\n      str_detect(MLRA_NAME, \"Eastern Iowa\") ~ \"E IA MN Till\",\n      str_detect(MLRA_NAME, \"Red River\") ~ \"Red River\",\n      str_detect(MLRA_NAME, \"Rolling Till Prairie\") ~ \"Rol Till Pr\",\n      str_detect(MLRA_NAME, \"Central Min\") ~ \"C MN Sandy\",\n      str_detect(MLRA_NAME, \"Gray Drift\") ~ \"N MN Gray\",\n      str_detect(MLRA_NAME, \"Central Iowa\") ~ \"C IA MN Till\",\n      TRUE ~ \"Fix me\"\n    )\n  )\n\n# want all the CIG lab observations (not averaged to the plot level)\nlab &lt;- read_csv(\"../CIG/cig-main/cig_lab_data_all_20230301.csv\") %&gt;%\n  mutate(val_unit_id = glue(\"{site}-{treatment}-{position}\")) %&gt;%\n  select(\n    sample_id,\n    val_unit_id,\n    region,\n    position,\n    corr_percent_stable_gr2mm,\n    ugC_g_day,\n    BG_activity,\n    NAG_activity,\n    P_activity,\n    poxc_mg_kg,\n    mean_protein_mg_g,\n    mbc_ug_g_soil,\n    mbn_ug_g_soil,\n    total_living_microbial_biomass_plfa_ng_g,\n    total_bacteria_plfa_ng_g,\n    total_fungi_plfa_ng_g,\n    total_bacteria_percent_of_tot_biomass,\n    total_fungi_percent_of_tot_biomass\n  ) %&gt;% \n  filter(sample_id %in% c(1:243))\n\n# missing data due to sample contamination in 2019 samples\n# that were run for PLFA. Pulling MBN values from 2020\n# for this validation site only after confirming that the values \n# are in a similar range for this soil type, hillslope position,\n# and region\nmbn_mw4_cva &lt;-\n  read_csv(\"../CIG/cig-main/cig_lab_data_all_20230301.csv\") %&gt;%\n  mutate(val_unit_id = glue(\"{site}-{treatment}-{position}\")) %&gt;%\n  filter(sample_id %in% c(474:476)) %&gt;%\n  pull(mbn_ug_g_soil) %&gt;%\n  mean(., na.rm = TRUE)\n\n\n\n23.3.2 Summarize to mapunit level\nRecall that the lab data and the component data need to be summarized to the “mapunit” level. This involves averaging the soil properties and determining the majority taxonomic classification for each MUKEY based on the included components (recall that most MUKEYs have just 1 contributing component, so we just need to account for those that have &gt;1 contributing component to determine, for each level of taxonomy I look at, what the majority assignment should be).\nStarting with the taxonomy data:\n\n# how many unique mukeys are we working with?\n(cig_mukeys &lt;- cig_comps %&gt;%\n    pull(mukey) %&gt;%\n    unique() %&gt;%\n    length())\n\n[1] 37\n\n# Q: which mukeys have multiple components contributing data?\n# A: 16 unique mukeys\ncig_comps %&gt;% \n  select(mukey, cokey, taxclname:taxpartsize) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey) %&gt;% \n  count() %&gt;% \n  filter(n&gt;1)\n\n\n\n  \n\n\n# goal: assign each mukey a representative suborder\n# based on the suborders of the components within it.\n# should get back object w/ length 37 (# of unique mukeys)\ncig_comps %&gt;% \n  select(mukey, cokey, comppct_r, taxsuborder) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey, taxsuborder) %&gt;% \n  summarise(tot_percent = sum(comppct_r), .groups = \"drop\") %&gt;% \n  group_by(mukey) %&gt;% \n  slice_max(tot_percent)\n\n\n\n  \n\n\n\nFunction for determining representative taxonomic level . Using similar code to the example with suborder above, this function allows me to summarize the representative taxonomic level (order, suborder, etc.) based on % area.\n\n# calculate representative taxonomy\ncalc_rep_tax &lt;- function(tax_level, comp_df){\n  \n  comp_df %&gt;% \n  select(mukey, cokey, comppct_r, {{tax_level}}) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey, {{tax_level}}) %&gt;% \n  summarise(tot_percent = sum(comppct_r), .groups = \"drop\") %&gt;% \n  group_by(mukey) %&gt;% \n  slice_max(tot_percent) %&gt;% \n    select(-tot_percent)\n  \n}\n\n\norder &lt;- calc_rep_tax(taxorder, cig_comps)\nsuborder &lt;- calc_rep_tax(taxsuborder, cig_comps)\ngrt_grp &lt;- calc_rep_tax(taxgrtgroup, cig_comps)\nsub_grp &lt;- calc_rep_tax(taxsubgrp, cig_comps)\nfamily &lt;- calc_rep_tax(taxclname, cig_comps)\n\ntax_joined &lt;- list(order, suborder, grt_grp, sub_grp, family) %&gt;% \n  purrr::reduce(., left_join, by = \"mukey\")\n\ntax_val_key &lt;- cig_comps %&gt;% \n  select(val_unit_id, mukey) %&gt;% \n  distinct()\n\nNow time to summarize the lab data to the mapunit level, this is easier than the taxonomy because we can just take the average. Recall that we are working with 2019 data only.\n\nlab_summary &lt;- lab %&gt;% \n  select(-sample_id) %&gt;% \n  group_by(val_unit_id) %&gt;% \n  summarise(across(where(is.numeric),\n                   ~mean(.x, na.rm = TRUE)))\n\n\n\n23.3.3 Join taxonomic and lab info with validation dataset\nHere, we create a dataset with one row for each validation unit in the CIG dataset. It includes information about representative taxonomic classification at different levels (order, suborder, great group) and also representative values for the soil health indicators we want to analyze further below when comparing variance explained by different stratification options (region, taxonomy, k-means group).\n\nval_mukey &lt;- left_join(cig_val, cig_mukey_votes, by = \"val_unit_id\") %&gt;% \n  select(-c(n, max_vote))\n\nval_mukey_lab &lt;- left_join(val_mukey, lab_summary, by = \"val_unit_id\")\n\nval_all &lt;- left_join(val_mukey_lab, tax_joined, by = \"mukey\") %&gt;%\n # on list from old key, this id no longer exists\n   filter(val_unit_id != \"RR4-CV-B\") %&gt;% \n  mutate(mbn_ug_g_soil = case_when(\n    # dealing with missing data, see note at end of \n    # \"load data\" code block\n    val_unit_id == \"MW4-CV-A\" ~ mbn_mw4_cva,\n    TRUE ~ mbn_ug_g_soil\n  ))"
  },
  {
    "objectID": "30-welch-compare-variation.html#response-variables---exploratory-plots",
    "href": "30-welch-compare-variation.html#response-variables---exploratory-plots",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.4 Response variables - exploratory plots",
    "text": "23.4 Response variables - exploratory plots\n\nrvars &lt;- c(\"corr_percent_stable_gr2mm\",\n               \"ugC_g_day\",\n               \"poxc_mg_kg\",\n               \"mbc_ug_g_soil\",\n               \"mbn_ug_g_soil\", # include MBN?\n               \"total_living_microbial_biomass_plfa_ng_g\",\n               \"total_bacteria_plfa_ng_g\",\n               \"total_fungi_plfa_ng_g\"\n)\n\n\nval_long &lt;- val_all %&gt;% \n  select(val_unit_id, all_of(rvars)) %&gt;% \n  pivot_longer(-val_unit_id) %&gt;% \n  mutate( name = case_when(\n    name == \"corr_percent_stable_gr2mm\" ~ \"agg_stab\",\n    name == \"ugC_g_day\" ~ \"pmc_ugCgday\",\n    name == \"total_living_microbial_biomass_plfa_ng_g\" ~ \"PLFA total\",\n    name == \"total_bacteria_plfa_ng_g\" ~ \"PLFA bacteria\",\n    name == \"total_fungi_plfa_ng_g\" ~ \"PLFA fungi\",\n    TRUE ~ name\n    \n  ))\n\nval_long %&gt;% \n  ggplot() + \n  geom_histogram(aes(value), bins = 20) +\n  facet_wrap(vars(name), scales = \"free\") +\n  ggtitle(\"Distribution (not transformed)\")\n\n\n\nval_all %&gt;% \n  select(val_unit_id, contains(\"percent_of\")) %&gt;% \n  pivot_longer(-val_unit_id) %&gt;% \n  ggplot() +\n  geom_histogram(aes(value), bins = 20) + \n  facet_wrap(vars(name))\n\n\n\nval_all %&gt;% \n  select(val_unit_id, contains(\"percent_of\")) %&gt;% \n  pivot_longer(-val_unit_id) %&gt;% \n  group_by(name) %&gt;% \n  summarise(min = min(value), \n            max = max(value))"
  },
  {
    "objectID": "30-welch-compare-variation.html#stratification-options",
    "href": "30-welch-compare-variation.html#stratification-options",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.5 Stratification options",
    "text": "23.5 Stratification options\nWe could stratify the CIG points based on several different levels of Soil Taxonomy. How do the CIG sampling points break down in terms of number per cluster, and number per taxonomic level(s) (order, suborder, etc)?\nFor k-means clusters/groups: here we see that there is only 1 representative from group 4, so probably won’t be able to use that group for any calculations. But the others are workable.\nFor the different taxonomic levels, suborder seems workable, it has &gt;= 5 validation units in any given level. I think order is too broad, and by the time we get to sub-group, we have two subgroups with just 1 validation unit each, so would have to drop those.\n\nval_all %&gt;% \n  count(k_8)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(mlra_short)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(taxorder)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(taxsuborder) \n\n\n\n  \n\n\nval_all %&gt;% \n  count(taxgrtgroup)\n\n\n\n  \n\n\nval_all %&gt;% \n  count(taxsubgrp)\n\n\n\n  \n\n\nval_all %&gt;% count(taxclname)\n\n\n\n  \n\n\nxtabs(~region+taxorder, data = val_all)\n\n      taxorder\nregion Alfisols Mollisols Vertisols\n    MW       13        11         0\n    RR        0        10        11\n    ST       13         7         0\n    SW        0        15         0"
  },
  {
    "objectID": "30-welch-compare-variation.html#crosstabs-w-ud",
    "href": "30-welch-compare-variation.html#crosstabs-w-ud",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.6 Crosstabs w/ UD",
    "text": "23.6 Crosstabs w/ UD\nWas originally thinking about whether we should include or exclude UD (undisturbed/unfarmed sites) because we know they are a major source of variance (they have much higher values across the board for all these indicators).\n\nxtabs(~soil_group + taxsuborder, data = val_all)\n\n          taxsuborder\nsoil_group Aqualfs Aquerts Aquolls Udalfs Udolls\n     grp-2       5       0       3     10      6\n     grp-3       0       0       0      6      0\n     grp-4       0       0       1      0      0\n     grp-5       0      11      16      0      0\n     grp-6       0       0      10      0      0\n     grp-7       0       0       0      5      7\n\nxtabs(~taxgrtgroup + soil_group, data = val_all)\n\n              soil_group\ntaxgrtgroup    grp-2 grp-3 grp-4 grp-5 grp-6 grp-7\n  Argiaquolls      3     0     0     0     0     0\n  Calciaquerts     0     0     0     5     0     0\n  Calciaquolls     0     0     0     0    10     0\n  Endoaquolls      0     0     1    16     0     0\n  Epiaqualfs       5     0     0     0     0     0\n  Epiaquerts       0     0     0     6     0     0\n  Hapludalfs      10     6     0     0     0     5\n  Hapludolls       6     0     0     0     0     7\n\nxtabs(~mlra_short + soil_group, data = val_all)\n\n              soil_group\nmlra_short     grp-2 grp-3 grp-4 grp-5 grp-6 grp-7\n  C IA MN Till     5     0     1     5     0     4\n  C MN Sandy       2     0     0     0     0     3\n  E IA MN Till    13     0     0    11     0     0\n  N MN Gray        0     5     0     0     0     0\n  Red River        0     0     0    11    10     0\n  Rol Till Pr      4     1     0     0     0     5"
  },
  {
    "objectID": "30-welch-compare-variation.html#crosstabs-wout-ud",
    "href": "30-welch-compare-variation.html#crosstabs-wout-ud",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.7 Crosstabs w/out UD",
    "text": "23.7 Crosstabs w/out UD\n\nval_farm &lt;- val_all %&gt;% \n  filter(!str_detect(val_unit_id, \"UD\"))\n\nxtabs(~soil_group + region, data = val_farm)\n\n          region\nsoil_group MW RR ST SW\n     grp-2  9  0  6  3\n     grp-3  0  0  4  0\n     grp-4  0  0  0  1\n     grp-5  7  8  0  4\n     grp-6  0  8  0  0\n     grp-7  0  0  6  4\n\nxtabs(~soil_group + taxsuborder, data = val_farm)\n\n          taxsuborder\nsoil_group Aqualfs Aquerts Aquolls Udalfs Udolls\n     grp-2       3       0       3      8      4\n     grp-3       0       0       0      4      0\n     grp-4       0       0       1      0      0\n     grp-5       0       8      11      0      0\n     grp-6       0       0       8      0      0\n     grp-7       0       0       0      4      6\n\nxtabs(~taxgrtgroup + soil_group, data = val_farm)\n\n              soil_group\ntaxgrtgroup    grp-2 grp-3 grp-4 grp-5 grp-6 grp-7\n  Argiaquolls      3     0     0     0     0     0\n  Calciaquerts     0     0     0     4     0     0\n  Calciaquolls     0     0     0     0     8     0\n  Endoaquolls      0     0     1    11     0     0\n  Epiaqualfs       3     0     0     0     0     0\n  Epiaquerts       0     0     0     4     0     0\n  Hapludalfs       8     4     0     0     0     4\n  Hapludolls       4     0     0     0     0     6"
  },
  {
    "objectID": "30-welch-compare-variation.html#plots",
    "href": "30-welch-compare-variation.html#plots",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.8 Plots",
    "text": "23.8 Plots\n\n# keep only data points for soil regions where we have &gt;1 validation area (need to drop group 4)\nsub_dat &lt;- val_all %&gt;% \n  filter(k_8 %in% c(2, 3, 5:7))\n\n# creating this so I can test difference between including/excluding the UD sites.\nfarmdat &lt;- sub_dat %&gt;% \n  filter(!str_detect(val_unit_id, \"UD\"))\n\nsub_dat %&gt;% \n  ggplot(aes(x = mlra_short, y = poxc_mg_kg)) + \n  geom_boxplot() +\n  geom_quasirandom(aes(color = soil_group)) +\n  theme_bw() +\n  ggtitle(\"MLRA\")\n\n\n\nsub_dat %&gt;% \n  ggplot(aes(x = soil_group, y = poxc_mg_kg)) +\n  geom_boxplot() + \n  geom_quasirandom() +\n  theme_bw() + \n  ggtitle(\"Cluster/Group\")\n\n\n\nsub_dat %&gt;% \n  ggplot(aes(x = taxsuborder, y = poxc_mg_kg)) +\n  geom_boxplot() + \n  geom_quasirandom(aes(color = soil_group)) +\n  theme_bw() +\n  ggtitle(\"Suborder\")\n\n\n\nsub_dat %&gt;% \n  ggplot(aes(x = taxgrtgroup, y = poxc_mg_kg)) +\n  geom_boxplot() + \n  geom_quasirandom(aes(color = soil_group)) +\n  theme_bw() +\n  ggtitle(\"Great Group\") +\n  theme(axis.text.x = element_text(angle = 15))\n\n\n\n\n\n23.8.1 Checking distributions\nTransformations needed (based on inspecting the plots below):\n\nNone: aggregate stability, POXC\nLog10: total biomass PLFA, total bacteria PLFA, total fungi PLFA, MBC, MBN, PMC\n\n\n23.8.1.1 Aggregate stability\n\nplot_transformations(var = corr_percent_stable_gr2mm,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n23.8.1.2 PLFA Indicators\n\nplot_transformations(var = total_living_microbial_biomass_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\nplot_transformations(var = total_bacteria_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\nplot_transformations(var = total_fungi_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n23.8.1.3 Microbial biomass C & N (CFE)\n\nplot_transformations(var = mbc_ug_g_soil,\n                     df = val_all,\n                     nbins = 15)\n\n\n\nplot_transformations(var = mbn_ug_g_soil,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n23.8.1.4 POXC\n\nplot_transformations(var = poxc_mg_kg,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n23.8.1.5 PMC\n\nplot_transformations(var = ugC_g_day,\n                     df = val_all,\n                     nbins = 10)"
  },
  {
    "objectID": "30-welch-compare-variation.html#function-to-plot-model-checks",
    "href": "30-welch-compare-variation.html#function-to-plot-model-checks",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.9 Function to plot model checks",
    "text": "23.9 Function to plot model checks\n\ncheck_plots_anova &lt;- function(soil_var, strat, df, log_trans) {\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(strat),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # checking if any clusters are represented by 1 or less data points. Want to drop these cuz can't calc variance with only 1\n\n  n_obs_per_group &lt;- dat_no_na %&gt;%\n    count(.data[[strat]])\n  \n  single_obs_groups &lt;- n_obs_per_group %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[strat]])\n  \n  if (length(single_obs_groups) == 0) {\n    dat_subset &lt;- dat_no_na\n    \n  } else{\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[strat]] %in% single_obs_groups))\n    \n  }\n  \n  if (log_trans){\n    \n    f &lt;- paste0(\"log(\", soil_var, \") ~ \", strat)\n    \n  }else{\n    \n    f &lt;- paste0(soil_var, \" ~ \", strat)\n    \n  }\n  \n  mod &lt;- lm(formula = f,\n            data = dat_subset)\n  \n  check_plots &lt;- performance::check_model(mod, check = c(\"normality\", \"homogeneity\", \"linearity\"))  \n  \n  return(list(f_used = f,\n              plots = check_plots))\n  \n}"
  },
  {
    "objectID": "30-welch-compare-variation.html#models-to-check",
    "href": "30-welch-compare-variation.html#models-to-check",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.10 Models to check",
    "text": "23.10 Models to check\nWorking with these independent variables (stratification options):\n\nsoil_group (k-means);\nmlra_short\ntaxsuborder\ntaxgrtgroup\n\n\n(dep_vars &lt;- c(\"corr_percent_stable_gr2mm\",\n               \"ugC_g_day\",\n               \"poxc_mg_kg\",\n               \"mbc_ug_g_soil\",\n               \"mbn_ug_g_soil\", # include MBN?\n               \"total_living_microbial_biomass_plfa_ng_g\",\n               \"total_bacteria_plfa_ng_g\",\n               \"total_fungi_plfa_ng_g\"\n))\n\n[1] \"corr_percent_stable_gr2mm\"               \n[2] \"ugC_g_day\"                               \n[3] \"poxc_mg_kg\"                              \n[4] \"mbc_ug_g_soil\"                           \n[5] \"mbn_ug_g_soil\"                           \n[6] \"total_living_microbial_biomass_plfa_ng_g\"\n[7] \"total_bacteria_plfa_ng_g\"                \n[8] \"total_fungi_plfa_ng_g\"                   \n\n# determined by inspecting histograms above w/ different\n# transformation options\nlog_trans &lt;- c(FALSE, \n               TRUE, \n               FALSE, \n               TRUE,\n               TRUE,\n               TRUE,\n               TRUE,\n               TRUE)\n\n\n23.10.1 K-means soil groups\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"soil_group\",\n    df = val_all\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ soil_group\"\n\n[[1]]$plots\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ soil_group\"\n\n[[2]]$plots\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ soil_group\"\n\n[[3]]$plots\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ soil_group\"\n\n[[4]]$plots\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ soil_group\"\n\n[[5]]$plots\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ soil_group\"\n\n[[6]]$plots\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ soil_group\"\n\n[[7]]$plots\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ soil_group\"\n\n[[8]]$plots\n\n\n\n\n\n\n\n23.10.2 Region\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"region\",\n    df = val_all\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ region\"\n\n[[1]]$plots\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ region\"\n\n[[2]]$plots\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ region\"\n\n[[3]]$plots\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ region\"\n\n[[4]]$plots\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ region\"\n\n[[5]]$plots\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ region\"\n\n[[6]]$plots\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ region\"\n\n[[7]]$plots\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ region\"\n\n[[8]]$plots\n\n\n\n\n\n\n\n23.10.3 Suborder\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"taxsuborder\",\n    df = val_all\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ taxsuborder\"\n\n[[1]]$plots\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ taxsuborder\"\n\n[[2]]$plots\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ taxsuborder\"\n\n[[3]]$plots\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ taxsuborder\"\n\n[[4]]$plots\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ taxsuborder\"\n\n[[5]]$plots\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ taxsuborder\"\n\n[[6]]$plots\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ taxsuborder\"\n\n[[7]]$plots\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ taxsuborder\"\n\n[[8]]$plots"
  },
  {
    "objectID": "30-welch-compare-variation.html#effect-sizes-for-variance-explained",
    "href": "30-welch-compare-variation.html#effect-sizes-for-variance-explained",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.11 Effect sizes for variance explained",
    "text": "23.11 Effect sizes for variance explained\nAdd updated references from reading notes on 2023-03-16 re: calculating effect sizes from F statistics (which is what {effectsize} is doing for us behind the scenes when we pass the Welch’s ANOVA results to epsilon_squared()\nI was originally planning on calculating eta-squared as my effect size, using either: effectsize::eta_squared() for ANOVA or effectsize::rank_eta_squared(). But in reading the documentation for eta_squared() from {effectsize}, I learned that there are other, less biased options for calculating this value: omega-squared and epsilon-squared.\nI found a very helpful, recent reference, Iacobucci et al., (2023), that explains the differences between eta, epsilon, and omega when used as effect sizes of variance explained. See their paper for the details, but the easiest way to think about it is that using epsilon-squared or omega-squared is essentially like reporting an adjusted R2 value instead of a regular R2 value. It’s better to use epsilon squared because the equation accounts for small sample size, which is relevant in our case.\nBelow is an example with MBC, this is just me figuring out what the different functions outputs look like, and how NAs are handled.\n\n23.11.1 Example\n\n## welch's ANOVA - MBC by soil_group (k-means)\n\ntest_welch &lt;- oneway.test(data = sub_dat,\n            formula = mbc_ug_g_soil ~ soil_group,\n           var.equal = FALSE,\n            na.action = \"na.omit\")\n\ntest_welch\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  mbc_ug_g_soil and soil_group\nF = 5.5237, num df = 4.000, denom df = 22.848, p-value = 0.002911\n\neffectsize::epsilon_squared(test_welch, partial = FALSE)\n\n`var.equal = FALSE` - effect size is an approximation.\n\n\n\n\n  \n\n\n\nBelow I test out Welch’s ANOVA followed by calculation of epsilon squared. This works, and after reading more from the {effectsize} documentation here I think I understand how. I can pass the result from oneway.test(var.equal = FALSE) to effectsize::epsilon_squared() and there is an intermediate step performed where the F statistic from oneway.test is converted to the effect size (could be partial eta-squared, omega-squared, or epsilon-squared)\n\n\n23.11.2 Setup dataframe\nTransformations needed:\n\nNone: aggregate stability, POXC\nLog10: total biomass PLFA, total bacteria PLFA, total fungi PLFA, MBC, MBN, PMC\n\n\nstrat_opts &lt;- c(\"soil_group\", \"mlra_short\", \"taxsuborder\",\n                \"taxgrtgroup\")\n\nmod_frame &lt;- tidyr::crossing(dep_vars, strat_opts)\n\n# apply the transformations I decided on above after looking at \n# \"checking distributions\" plots\ndat_trans &lt;- sub_dat %&gt;% \n  mutate(across(.cols = c(total_living_microbial_biomass_plfa_ng_g,\n                          total_bacteria_plfa_ng_g,\n                          total_fungi_plfa_ng_g,\n                          mbc_ug_g_soil,\n                          mbn_ug_g_soil,\n                          ugC_g_day), \n                .fns = log10\n                ))\n# creating a farm dataset so I can compare results \n# for including vs. excluding UD sites\ndat_farm &lt;- dat_trans %&gt;% \n  filter(!str_detect(val_unit_id, \"UD\"))\n\n\n\n23.11.3 Function run ANOVA & calculate epsilon squared\n\ncalc_welch_eps_sq &lt;- function(soil_var,\n                              strat,\n                              df = dat_trans) {\n  \n  my_formula &lt;- as.formula(paste0(soil_var, \" ~ \", strat))\n  \n  \n  mod_obj &lt;- oneway.test(\n    formula = my_formula,\n    data = df,\n    var.equal = FALSE,\n    na.action = \"na.omit\"\n  )\n\n    eps_sq &lt;- effectsize::epsilon_squared(model = mod_obj,\n                        # partial FALSE b/c one-way test\n                        # we don't have other vars to\n                        # break the variance down by\n                        partial = FALSE)\n\n  return(eps_sq)\n\n}\n\n\n\n23.11.4 Calculate epsilon squared\n\n# quieted the message \"`var.equal = FALSE` - effect size is an approximation.\", it's a reminder that this effect size is an apporximation because we are estimating it based on the f statistic from our Welch's ANOVA\n\neps_inputs &lt;- mod_frame %&gt;% \n  rename(strat = strat_opts, \n         soil_var = dep_vars) %&gt;% \n  select(soil_var, strat) \n\n# returns bunch of reminders about how the eff size is \n# an approximation (b/c we are calculating it from F stat,\n# not directly from SS)\neps_results &lt;- eps_inputs %&gt;%\n  mutate(eps_results = map2(\n    .x = soil_var,\n    .y = strat,\n    .f = ~ calc_welch_eps_sq(\n      soil_var = .x,\n      strat = .y,\n      df = dat_trans\n    )\n  ))\n\n\neps_farm &lt;- eps_inputs %&gt;%\n  mutate(eps_results = map2(\n    .x = soil_var,\n    .y = strat,\n    .f = ~ calc_welch_eps_sq(\n      soil_var = .x,\n      strat = .y,\n      df = dat_farm\n    )\n  ))\n\n\n\n23.11.5 Clean up data\n\neps_long &lt;- eps_results %&gt;% \n  unnest(eps_results) %&gt;% \n  rename(epsilon_sq = Epsilon2)\n\nfarm_long &lt;- eps_farm %&gt;% \n  unnest(eps_results) %&gt;% \n  rename(epsilon_sq = Epsilon2)\n\n# eps_wide &lt;- eps_long %&gt;% \n#   pivot_wider(values_from = c(\"epsilon_sq\", \"type\"), \n#               names_from = \"strat\")\n\n\n\n23.11.6 Table of variance explained\n\neps_long %&gt;% \n  filter(strat != \"taxgrtgroup\") %&gt;% \n  mutate(soil_var = case_when(\n    str_detect(soil_var, \"stable\") ~ \"Agg Stab\",\n    str_detect(soil_var, \"mbc\") ~ \"MBC\",\n    str_detect(soil_var, \"mbn\") ~ \"MBN\",\n    str_detect(soil_var, \"poxc\") ~ \"POXC\",\n    str_detect(soil_var, \"total_bact\") ~ \"Bact PLFA\",\n    str_detect(soil_var, \"total_fungi\") ~ \"Fungi PLFA\",\n    str_detect(soil_var, \"total_living\") ~ \"Total PLFA\",\n    str_detect(soil_var, \"ugC\") ~ \"PMC\",\n  )) %&gt;% \n  select(soil_var, strat, epsilon_sq) %&gt;% \n  pivot_wider(names_from = \"strat\",\n              values_from = \"epsilon_sq\") %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, digits = 2)))\n\n\n\n  \n\n\n\n\n\n23.11.7 Plots of variance explained\n\nall_pts_labelled &lt;- eps_long %&gt;% \n  filter(strat != \"taxgrtgroup\") %&gt;% \n  mutate(soil_var = case_when(\n    soil_var == \"mbc_ug_g_soil\" ~ \"MBC\",\n    soil_var == \"mbn_ug_g_soil\" ~ \"MBN\", \n    soil_var == \"corr_percent_stable_gr2mm\" ~ \"Aggregate Stab.\",\n    soil_var == \"poxc_mg_kg\" ~ \"POXC\", \n    soil_var == \"total_bacteria_plfa_ng_g\" ~ \"Bacteria PLFA\",\n    soil_var == \"total_fungi_plfa_ng_g\" ~ \"Fungi PLFA\",\n    soil_var == \"total_living_microbial_biomass_plfa_ng_g\" ~ \"Total Biomass PLFA\", \n    soil_var == \"poxc_mg_kg\" ~ \"POXC\",\n    soil_var == \"ugC_g_day\" ~ \"PMC\"\n  ),\n  strat = case_when(\n    strat == \"mlra_short\" ~ \"MLRA\",\n    strat == \"soil_group\" ~ \"KM CLUSTER\",\n    strat == \"taxsuborder\" ~ \"SUBORDER\"\n  )) %&gt;%\n  group_by(soil_var) %&gt;%\n  mutate(max_epsilon = max(epsilon_sq),\n         top_strat = case_when(\n           epsilon_sq == max_epsilon ~ \"Y\",\n           TRUE ~ \"N\"\n         ), \n  # round for nice labels \n         epsilon_sq_lab = round(epsilon_sq, digits = 2))\n\n\n# saving in case I want to compare farm (no UD) vs. w/ UD results\n\nfarm_labelled &lt;- farm_long %&gt;%\n  filter(strat != \"taxgrtgroup\") %&gt;%\n  mutate(soil_var = case_when(\n    soil_var == \"mbc_ug_g_soil\" ~ \"MBC\",\n    soil_var == \"mbn_ug_g_soil\" ~ \"MBN\",\n    soil_var == \"corr_percent_stable_gr2mm\" ~ \"Aggregate Stab.\",\n    soil_var == \"poxc_mg_kg\" ~ \"POXC\",\n    soil_var == \"total_bacteria_plfa_ng_g\" ~ \"Bacteria PLFA\",\n    soil_var == \"total_fungi_plfa_ng_g\" ~ \"Fungi PLFA\",\n    soil_var == \"total_living_microbial_biomass_plfa_ng_g\" ~ \"Total Biomass PLFA\",\n    soil_var == \"poxc_mg_kg\" ~ \"POXC\",\n    soil_var == \"ugC_g_day\" ~ \"PMC\"\n  ),\n  strat = case_when(\n    strat == \"mlra_short\" ~ \"MLRA\",\n    strat == \"soil_group\" ~ \"KM CLUSTER\",\n    strat == \"taxsuborder\" ~ \"SUBORDER\"\n  )) %&gt;%\n  group_by(soil_var) %&gt;%\n  mutate(max_epsilon = max(epsilon_sq),\n         top_strat = case_when(\n           epsilon_sq == max_epsilon ~ \"Y\",\n           TRUE ~ \"N\"\n         ), \n  # round for nice labels \n         epsilon_sq_lab = round(epsilon_sq, digits = 2))"
  },
  {
    "objectID": "30-welch-compare-variation.html#with-ud-variance-explained-plot",
    "href": "30-welch-compare-variation.html#with-ud-variance-explained-plot",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.12 (with UD) variance explained plot",
    "text": "23.12 (with UD) variance explained plot\n\nplot_var_order &lt;-\n  c(\n    \"POXC\",\n  #  \"Total Biomass PLFA\",\n    \"Bacteria PLFA\",\n    \"MBC\",\n    \"MBN\",\n    \"Fungi PLFA\",\n    \"Aggregate Stab.\",\n    \"PMC\"\n  )\n\n\neps_bar_plot &lt;- all_pts_labelled %&gt;%\n  filter(soil_var != \"Total Biomass PLFA\") %&gt;% \n  ggplot() +\n  geom_col(aes(x = soil_var, y = epsilon_sq_lab, fill = strat),\n           position = position_dodge(),\n           width = 0.7, \n           color = \"black\") +\n  # geom_text(aes(x = 8.25, y = 0.85, label = \"A.\"), size = 3) +\n  #   geom_text(aes(x = 6.25, y = 0.85, label = \"B.\"), size = 3) +\n  #   geom_text(aes(x = 4.25, y = 0.85, label = \"C.\"), size = 3) +\n  #   geom_text(aes(x = 1.25, y = 0.85, label = \"D.\"), size = 3) +\n  scale_x_discrete(limits = rev(plot_var_order)) +\n  scale_y_continuous(breaks = seq(0, 0.7, 0.1), limits = c(0, 0.7)) +\n#   geom_vline(aes(xintercept = 1.5)) +\n#   geom_vline(aes(xintercept = 4.5)) +\n#   geom_vline(aes(xintercept = 6.5)) +\n  coord_cartesian(xlim = c(0, 0.7),\n                  clip = 'off') +\n  coord_flip() +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 1),\n    legend.text = element_text(size = 8),\n    legend.justification = c(1, 1)\n  ) +\n  xlab(\"\") +\n  ylab(\"Epsilon Squared\") +\n  #labs(caption = \"With undisturbed (Welch's ANOVA)\") +\n  scale_fill_manual(values = c(\"#1b9e77\", \"#d95f02\", \"#7570b3\")) +\n  guides(fill = guide_legend(title = \"Grouping Method\")) \n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\nggsave(\"figs/epsilon_sq_barplot_ud.png\", plot = eps_bar_plot, \n       width = 5, height = 5, units = \"in\")"
  },
  {
    "objectID": "30-welch-compare-variation.html#no-ud-variance-explained-table-plot",
    "href": "30-welch-compare-variation.html#no-ud-variance-explained-table-plot",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.13 (No UD) variance explained table & plot",
    "text": "23.13 (No UD) variance explained table & plot\n\nfarm_long %&gt;% \n  filter(strat != \"taxgrtgroup\") %&gt;% \n  mutate(soil_var = case_when(\n    str_detect(soil_var, \"stable\") ~ \"Agg Stab\",\n    str_detect(soil_var, \"mbc\") ~ \"MBC\",\n    str_detect(soil_var, \"mbn\") ~ \"MBN\",\n    str_detect(soil_var, \"poxc\") ~ \"POXC\",\n    str_detect(soil_var, \"total_bact\") ~ \"Bact PLFA\",\n    str_detect(soil_var, \"total_fungi\") ~ \"Fungi PLFA\",\n    str_detect(soil_var, \"total_living\") ~ \"Total PLFA\",\n    str_detect(soil_var, \"ugC\") ~ \"PMC\",\n  )) %&gt;% \n  select(soil_var, strat, epsilon_sq) %&gt;% \n  pivot_wider(names_from = \"strat\",\n              values_from = \"epsilon_sq\") %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, digits = 2)))\n\n\n\n  \n\n\n\n\n# decided to use the same order as the above plot that also includes the UD data, to make comparison between them easier\n\n# f_plot_var_order &lt;- c(\"POXC\", \"Total Biomass PLFA\",\"Aggregate Stab.\", \"Bacteria PLFA\", \"MBC\", \"MBN\", \"Fungi PLFA\", \"PMC\")\n\n\nfarm_bar_plot &lt;- farm_labelled %&gt;%\n  filter(soil_var != \"Total Biomass PLFA\") %&gt;% \n  ggplot() +\n  geom_col(aes(x = soil_var, y = epsilon_sq_lab, fill = strat),\n           position = position_dodge(),\n           width = 0.7,\n           color = \"black\") +\n  # geom_text(aes(x = 1, y = 0.9, label = \"D.\"), size = 3) +\n  # geom_text(aes(x = 3, y = 0.9, label = \"C.\"), size = 3) +\n  # geom_text(aes(x = 7.5, y = 0.9, label = \"A.\"), size = 3) +\n  #   geom_text(aes(x = 5.5, y = 0.9, label = \"B.\"), size = 3) +\n  scale_x_discrete(limits = rev(plot_var_order)) + \n  scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1.05)) +\n # geom_vline(aes(xintercept = 1.5)) +\n #  geom_vline(aes(xintercept = 4.5)) +\n #  geom_vline(aes(xintercept = 6.5)) +\n  coord_flip() +\n  theme_bw() +\ntheme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 1),\n    legend.text = element_text(size = 8),\n    legend.justification = c(1, 1)\n  ) +\n  xlab(\"\") +\n  ylab(\"Epsilon Squared\") +\n  # labs(caption = \"Just farm data/no undisturbed (Welch's ANOVA)\") +\n  scale_fill_manual(values = c(\"#1b9e77\", \"#d95f02\", \"#7570b3\")) +\n  guides(fill = guide_legend(title = \"Grouping Method\"))\n\nggsave(\n  \"figs/epsilon_sq_barplot_farms.png\",\n  plot = farm_bar_plot,\n  width = 5,\n  height = 5,\n  units = \"in\"\n)"
  },
  {
    "objectID": "30-welch-compare-variation.html#what-is-going-on-with-pmc",
    "href": "30-welch-compare-variation.html#what-is-going-on-with-pmc",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.14 What is going on with PMC?",
    "text": "23.14 What is going on with PMC?\nPMC returns 0% variance explained for all three stratification options: K-means, Region, and Taxonomic sub-group. Is this really correct? Let’s double check below with the region stratification to make sure there isn’t something weird with the data.\n\nlm_pmc &lt;- lm(ugC_g_day ~ region, data = sub_dat, na.action = \"na.omit\")\n\naov_pmc &lt;- car::Anova(lm_pmc, type = 2)\n\n# big residuals number... that explains it\naov_pmc\n\n\n\n  \n\n\neffectsize::epsilon_squared(aov_pmc, partial = FALSE)\n\n\n\n  \n\n\n# what about eta-squared?\neffectsize::eta_squared(aov_pmc, partial = FALSE)\n\n\n\n  \n\n\n# what if we drop the UD sites?\n\nfarmdat &lt;- sub_dat %&gt;% filter(!str_detect(val_unit_id, \"UD\"))\n\nlm_farm &lt;- lm(ugC_g_day ~ region, data = farmdat, na.action = \"na.omit\")\n\naov_farm &lt;- car::Anova(lm_farm, type = 2)\n\n# big residuals number... that explains it\naov_farm\n\n\n\n  \n\n\neffectsize::epsilon_squared(aov_farm, partial = FALSE)\n\n\n\n  \n\n\n# what about eta-squared?\neffectsize::eta_squared(aov_farm, partial = FALSE)"
  },
  {
    "objectID": "30-welch-compare-variation.html#anova-models-poxc",
    "href": "30-welch-compare-variation.html#anova-models-poxc",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.15 ANOVA models POXC",
    "text": "23.15 ANOVA models POXC\nTrying this after reading Mourtzinis et al., 2020 about stratifying producer fields to better explain soybean yield variability\n\n# using poxc as example \n# check for missing values\nnrow(sub_dat %&gt;% filter(is.na(poxc_mg_kg)))\n\n# summarise to mapunit level (val_unit_ids)\npoxc_test &lt;- sub_dat %&gt;% \n  filter(!is.na(poxc_mg_kg)) %&gt;% \n  group_by(val_unit_id, region, k_8) %&gt;% \n  summarise(mean_poxc = mean(poxc_mg_kg),\n            .groups = \"drop\") %&gt;% \n  mutate(k_8 = glue(\"cl_{k_8}\"))\n\nreglm &lt;- lm(mean_poxc ~ region, data = poxc_test)\n#regposlm &lt;- lm(mean_poxc ~ region*position, data = poxc_test)\nclustlm &lt;- lm(mean_poxc ~ k_8, data = poxc_test)\n\nperformance::check_model(reglm, check = c(\"normality\", \"linearity\", \"homogeneity\", \"outliers\"))\n\nanova_reg &lt;- car::Anova(reglm) %&gt;% broom::tidy() %&gt;% column_to_rownames(\"term\")\n\n# anova_regpos &lt;- car::Anova(regposlm) %&gt;% broom::tidy() %&gt;% column_to_rownames(\"term\")\n  \nanova_clust &lt;- car::Anova(clustlm) %&gt;% broom::tidy() %&gt;% \n  column_to_rownames(\"term\")\n\n# what percentage of variation is explained by the \"region\" term\n# vs. the \"cluster\" term vs. the region and cluster terms (and their interaction?)\n\nss_reg &lt;- anova_reg['region', 'sumsq']/sum(anova_reg['sumsq'])*100\n\n# ss_regpos &lt;- (anova_regpos['region', 'sumsq'] + \n#     anova_regpos['position', 'sumsq'] + \n#     anova_regpos['region:position', 'sumsq'] ) /sum(anova_reg['sumsq'])*100\n\nss_clust &lt;- anova_clust['k_8', 'sumsq']/sum(anova_clust['sumsq'])*100\n\n\nss_reg\nss_regpos\nss_clust"
  },
  {
    "objectID": "30-welch-compare-variation.html#anova-models-mbc",
    "href": "30-welch-compare-variation.html#anova-models-mbc",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.16 ANOVA models MBC",
    "text": "23.16 ANOVA models MBC\n\n# using poxc as example \n# check for missing values\nnrow(sub_dat %&gt;% filter(is.na(mbc_ug_g_soil)))\n\nmbc_test &lt;- sub_dat %&gt;% \n  filter(!is.na(mbc_ug_g_soil)) %&gt;% \n  group_by(val_unit_id, region, k_8, position) %&gt;% \n  summarise(mean_mbc = mean(mbc_ug_g_soil),\n            .groups = \"drop\") %&gt;% \n  mutate(k_8 = glue(\"cl_{k_8}\"))\n\nreglm &lt;- lm(mean_mbc ~ region, data = mbc_test)\nregposlm &lt;- lm(mean_mbc ~ region*position, data = mbc_test)\nclustlm &lt;- lm(mean_mbc ~ k_8, data = mbc_test)\n\nanova_reg &lt;- car::Anova(reglm) %&gt;% broom::tidy() %&gt;% column_to_rownames(\"term\")\n\nanova_regpos &lt;- car::Anova(regposlm) %&gt;% broom::tidy() %&gt;% column_to_rownames(\"term\")\n  \nanova_clust &lt;- car::Anova(clustlm) %&gt;% broom::tidy() %&gt;% \n  column_to_rownames(\"term\")\n\n# what percentage of variation is explained by the \"region\" term\n# vs. the \"cluster\" term vs. the region and cluster terms (and their interaction?)\n\nss_reg &lt;- anova_reg['region', 'sumsq']/sum(anova_reg['sumsq'])*100\n\nss_regpos &lt;- (anova_regpos['region', 'sumsq'] + \n    anova_regpos['position', 'sumsq'] + \n    anova_regpos['region:position', 'sumsq'] ) /sum(anova_reg['sumsq'])*100\n\nss_clust &lt;- anova_clust['k_8', 'sumsq']/sum(anova_clust['sumsq'])*100\n\n\nss_reg\nss_regpos\nss_clust"
  },
  {
    "objectID": "30-welch-compare-variation.html#old-calculate-coefficient-of-variation",
    "href": "30-welch-compare-variation.html#old-calculate-coefficient-of-variation",
    "title": "23  Welch ANOVA - compare variation explained",
    "section": "23.17 (Old) Calculate coefficient of variation",
    "text": "23.17 (Old) Calculate coefficient of variation\nOld stuff here but wanted to keep track of the cvequality package link in case it’s useful at some other point… My idea was to use coefficient of variation (%) to compare the spread of the points when grouped by geographic region vs. soil group. I found a helpful R package to do this (which includes stats citations): {cvequality}"
  }
]