[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Minnesota Topsoil Classification for Soil Health Assessment",
    "section": "",
    "text": "Overview\nThis is an analysis notebook with code and notes on the analysis presented in Blair et al. (2024) manuscript “A data-driven topsoil classification to support soil health assessment in Minnesota, USA”\nManuscript authors: Hava K Blair, Jessica M. Gutknecht, Anna M. Cates, A. Marcelle Lewandowski, Nicolas A. Jelinski",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "02-data_sources.html",
    "href": "02-data_sources.html",
    "title": "1  Data Sources",
    "section": "",
    "text": "1.1 Define Area of Interest",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "02-data_sources.html#define-area-of-interest",
    "href": "02-data_sources.html#define-area-of-interest",
    "title": "1  Data Sources",
    "section": "",
    "text": "Union of MN county boundaries and MLRA boundaries\n\nAn idea for subsetting later - bring in Cropland Data Layer. Exclude pixels with impervious &gt;50%, or some other %?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "02-data_sources.html#major-land-resource-areas",
    "href": "02-data_sources.html#major-land-resource-areas",
    "title": "1  Data Sources",
    "section": "1.2 Major Land Resource Areas",
    "text": "1.2 Major Land Resource Areas\n\nAccess the USDA’s Geospatial Data Gateway.\n\nClick “Order by State” link, select Minnesota\n\nSelect Major Land Resource Areas map layers\nFormat: ESRI Shape (not sure about this, other options are ESRI File GeoDatabase, separate ESRI shapefiles, separate ESRI GeoDatabase Feature Classes)\n\nThe code below is reproduced from create_aoi_shapefile.R.\n\n# USA MLRAs\nmlras &lt;- st_read(\"data/mlra/mlra_v42.shp\")\n\nReading layer `mlra_v42' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\mlra\\mlra_v42.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5306 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -179.1334 ymin: -14.38165 xmax: 179.7882 ymax: 71.39805\nGeodetic CRS:  NAD83\n\n# State boundary\nmn &lt;- st_read(\"_qgis_files/shp_mn_state_boundary/mn_state_boundary.shp\")\n\nReading layer `mn_state_boundary' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\_qgis_files\\shp_mn_state_boundary\\mn_state_boundary.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -97.23921 ymin: 43.49936 xmax: -89.49174 ymax: 49.38436\nGeodetic CRS:  NAD83\n\n# confirm CRS the same\nst_crs(mn) == st_crs(mlras)\n\n[1] TRUE\n\n# keep the intersection of MLRAs and MN boundary\nmn_mlras &lt;- st_intersection(mn, mlras)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# check out the result\nplot(mn_mlras[\"MLRARSYM\"])\n\n\n\n\n\n\n\n\nNow that I have the MLRA-MN intersection, need to subset to only the target MLRAs (selected b/c we know we will have validation data broadly in these regions, and because they are domianted by agriculture, unlike the NE part of the state)\n\n# keep only target MLRAs (agricultural regions of the state)\n# clipped to MN boundary\n\nkeep_mlrarsym &lt;- c(\"56\",# Red River Valley of the North\n                \"102A\", # Rolling Till Prairie\n                \"91A\", # Central MN Sandy Outwash\n                \"57\", # Northern Minnesota Gray Drift\n                \"103\", # Central IA and MN Till Prairies\n                \"104\", # Eastern IA and MN Till Prairies\n                \"105\") # Northern Mississippi Valley Loess Hills \n\nmn_targets &lt;- mn_mlras %&gt;% \n  rownames_to_column(var = \"rowid\") %&gt;% \n  filter(MLRARSYM %in% keep_mlrarsym,\n         # drops the northern portion of N MN Gray Drift \n         # which was excluded b/c lack of validation pts \n         rowid != \"1.4\") \n\nmn_targets %&gt;% ggplot() + \n  geom_sf(aes(fill = MLRARSYM))\n\n\n\n\n\n\n\n# might want to make a map that shows the full extent of the MLRAs in MN, \n# extending out to neighborhing states. \nmn_mlras_extend &lt;- mlras %&gt;% \n  filter(MLRARSYM %in% keep_mlrarsym) %&gt;% \n  # drops the northern portion of N MN Gray Drift \n  # which was excluded b/c lack of validation pts \n  slice(-2)\n\nggplot(data = mn_mlras_extend) +\n  geom_sf(aes(fill = MLRARSYM)) +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "02-data_sources.html#minnesota-county-boundaries",
    "href": "02-data_sources.html#minnesota-county-boundaries",
    "title": "1  Data Sources",
    "section": "1.3 Minnesota County Boundaries",
    "text": "1.3 Minnesota County Boundaries\nDownloaded the MDNR’s version of county boundaries from MN Geospatial",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "02-data_sources.html#minnesota-state-boundary",
    "href": "02-data_sources.html#minnesota-state-boundary",
    "title": "1  Data Sources",
    "section": "1.4 Minnesota State Boundary",
    "text": "1.4 Minnesota State Boundary\nMN State Boundary downloaded from US Census Cartographic Boundary Files\nhttps://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html\nFile is the States 2018 500K shapefile",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "02-data_sources.html#national-cultivated-layer",
    "href": "02-data_sources.html#national-cultivated-layer",
    "title": "1  Data Sources",
    "section": "1.5 2020 National Cultivated Layer",
    "text": "1.5 2020 National Cultivated Layer\n\nAccess the data on the Cropland “Research & Science” page\n\nDownload zip file of 2020 National Cultivated Layer\n\nI think we want the “Cultivated Layer”, which is based on most recent 5 years of data, updated annually. Pixels are included in this layer if it was identified as cultivated in at least 2 of the last 5 years.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "02-data_sources.html#soils-data",
    "href": "02-data_sources.html#soils-data",
    "title": "1  Data Sources",
    "section": "1.6 Soils Data",
    "text": "1.6 Soils Data\n\n1.6.1 gSSURGO data\n\nNavigate to the Geospatial Data Gateway’s “Direct DAta/NAIP Download” list\n\nClick link for Soils Geographic Databases\nOctober 2021 gSSURGO by State\n\nDownload ZIP file for MN\n\nCitation for gSSURGO data:\nSoil Survey Staff. Gridded Soil Survey Geographic (gSSURGO) Database for Minnesota. United States Department of Agriculture, Natural Resources Conservation Service. Available online at https://gdg.sc.egov.usda.gov/. November, 22, 2021 (2021 official release).\n\n# inspect layers in .gdb\nsf::st_layers(\"./data/gSSURGO_MN/gSSURGO_MN.gdb\")\n\nDriver: OpenFileGDB \nAvailable layers:\n                   layer_name     geometry_type features fields\n1                    chaashto                NA   223330      4\n2               chconsistence                NA     2794     10\n3               chdesgnsuffix                NA    62846      3\n4                     chfrags                NA   321265     12\n5                    chorizon                NA   130493    171\n6                     chpores                NA       16      9\n7                    chstruct                NA   110021      7\n8                 chstructgrp                NA    95901      4\n9                      chtext                NA        0      7\n10                  chtexture                NA   318704      4\n11               chtexturegrp                NA   313739      6\n12               chtexturemod                NA    50000      3\n13                  chunified                NA   240027      4\n14              cocanopycover                NA       50      6\n15                  cocropyld                NA    34146     12\n16             codiagfeatures                NA    31760     12\n17                 coecoclass                NA    52483      6\n18                  coeplants                NA    25544      7\n19               coerosionacc                NA     5743      4\n20                  coforprod                NA     2726     12\n21                 coforprodo                NA        0     10\n22               cogeomordesc                NA    83667      8\n23           cohydriccriteria                NA    25091      3\n24                   cointerp                NA  5698886     13\n25                    comonth                NA   421728     17\n26                  component                NA    42529    109\n27                       copm                NA    49990      7\n28                    copmgrp                NA    34883      4\n29               copwindbreak                NA   161238      8\n30             corestrictions                NA     4369     13\n31                cosoilmoist                NA   666791      9\n32                 cosoiltemp                NA       96      9\n33                cosurffrags                NA    10955     15\n34              cosurfmorphgc                NA    17984      6\n35             cosurfmorphhpp                NA    28248      3\n36              cosurfmorphmr                NA      460      3\n37              cosurfmorphss                NA    42228      4\n38                 cotaxfmmin                NA    33654      3\n39               cotaxmoistcl                NA    20332      3\n40                     cotext                NA    42529      7\n41               cotreestomng                NA    32192      5\n42                cotxfmother                NA    31091      3\n43               distinterpmd                NA    11868      8\n44               distlegendmd                NA       92     11\n45                     distmd                NA       92      4\n46                   featdesc                NA      801      6\n47                  laoverlap                NA      512      6\n48                     legend                NA       92     14\n49                 legendtext                NA        0      7\n50                    mapunit                NA    10688     24\n51                      month                NA       12      2\n52                   muaggatt                NA    10688     40\n53                 muaoverlap                NA    35224      4\n54                  mucropyld                NA    23355     10\n55                     mutext                NA    12230      7\n56                  sacatalog                NA       92     11\n57                   sainterp                NA    11868      9\n58               sdvalgorithm                NA        8      4\n59               sdvattribute                NA      211     53\n60                  sdvfolder                NA       20      6\n61         sdvfolderattribute                NA      213      2\n62               mdstatdomdet                NA     6930      5\n63               mdstatdommas                NA      123      2\n64               mdstatidxdet                NA      172      4\n65               mdstatidxmas                NA      149      3\n66             mdstatrshipdet                NA       66      5\n67             mdstatrshipmas                NA       63      5\n68              mdstattabcols                NA      865     14\n69                 mdstattabs                NA       75      5\n70                   FEATLINE Multi Line String    60902      5\n71                  FEATPOINT             Point   254435      4\n72                     MULINE Multi Line String        0      5\n73                    MUPOINT             Point        0      4\n74                  SAPOLYGON     Multi Polygon       96      5\n75                  MUPOLYGON     Multi Polygon  2123552      6\n76                      Valu1                NA    10688     58\n77          MapunitRaster_10m     Multi Polygon        1      3\n78      VAT_MapunitRaster_10m                NA    10688      3\n79 fras_aux_MapunitRaster_10m                NA        1      3\n80 fras_blk_MapunitRaster_10m                NA   180371      6\n81 fras_bnd_MapunitRaster_10m                NA        1     18\n82 fras_ras_MapunitRaster_10m                NA        1      3\n               crs_name\n1                  &lt;NA&gt;\n2                  &lt;NA&gt;\n3                  &lt;NA&gt;\n4                  &lt;NA&gt;\n5                  &lt;NA&gt;\n6                  &lt;NA&gt;\n7                  &lt;NA&gt;\n8                  &lt;NA&gt;\n9                  &lt;NA&gt;\n10                 &lt;NA&gt;\n11                 &lt;NA&gt;\n12                 &lt;NA&gt;\n13                 &lt;NA&gt;\n14                 &lt;NA&gt;\n15                 &lt;NA&gt;\n16                 &lt;NA&gt;\n17                 &lt;NA&gt;\n18                 &lt;NA&gt;\n19                 &lt;NA&gt;\n20                 &lt;NA&gt;\n21                 &lt;NA&gt;\n22                 &lt;NA&gt;\n23                 &lt;NA&gt;\n24                 &lt;NA&gt;\n25                 &lt;NA&gt;\n26                 &lt;NA&gt;\n27                 &lt;NA&gt;\n28                 &lt;NA&gt;\n29                 &lt;NA&gt;\n30                 &lt;NA&gt;\n31                 &lt;NA&gt;\n32                 &lt;NA&gt;\n33                 &lt;NA&gt;\n34                 &lt;NA&gt;\n35                 &lt;NA&gt;\n36                 &lt;NA&gt;\n37                 &lt;NA&gt;\n38                 &lt;NA&gt;\n39                 &lt;NA&gt;\n40                 &lt;NA&gt;\n41                 &lt;NA&gt;\n42                 &lt;NA&gt;\n43                 &lt;NA&gt;\n44                 &lt;NA&gt;\n45                 &lt;NA&gt;\n46                 &lt;NA&gt;\n47                 &lt;NA&gt;\n48                 &lt;NA&gt;\n49                 &lt;NA&gt;\n50                 &lt;NA&gt;\n51                 &lt;NA&gt;\n52                 &lt;NA&gt;\n53                 &lt;NA&gt;\n54                 &lt;NA&gt;\n55                 &lt;NA&gt;\n56                 &lt;NA&gt;\n57                 &lt;NA&gt;\n58                 &lt;NA&gt;\n59                 &lt;NA&gt;\n60                 &lt;NA&gt;\n61                 &lt;NA&gt;\n62                 &lt;NA&gt;\n63                 &lt;NA&gt;\n64                 &lt;NA&gt;\n65                 &lt;NA&gt;\n66                 &lt;NA&gt;\n67                 &lt;NA&gt;\n68                 &lt;NA&gt;\n69                 &lt;NA&gt;\n70 NAD83 / Conus Albers\n71 NAD83 / Conus Albers\n72 NAD83 / Conus Albers\n73 NAD83 / Conus Albers\n74 NAD83 / Conus Albers\n75 NAD83 / Conus Albers\n76                 &lt;NA&gt;\n77 NAD83 / Conus Albers\n78                 &lt;NA&gt;\n79                 &lt;NA&gt;\n80                 &lt;NA&gt;\n81                 &lt;NA&gt;\n82                 &lt;NA&gt;\n\n\n\n\n1.6.2 Process for .gdb &gt; .tif\nWhen you unzip gSSURGO, the data is in ESRI geodatabase (.gdb) format. I had thought this was a proprietary format that you had to open in an ESRI program like ArcMap, but this 2015 blog post from UCLA suggests otherwise (and helpfully explains some GDAL drivers for opening .gdbs). Also this blog post from 2021 walks through opening a .gdb in QGIS, but doesn’t mention a maximum file size (which I thought was an issue?). Anyway, I think QGIS froze when I tried to open the .gdb, so I used ArcMap instead to save it is a .tif, as describe below.\nFor future, gis.stackexchange post suggests that maybe using a database like spatialite or postgis would be helpful? Learn more about this, starting with the QGIS docs here (esp module 16 and 18). Also for future,some breadcrumbs that maybe .gdb IS in fact proprietary and must be opened using ESRI software is necessary. Distinction that there is no GDAL raster driver for ESRI GeoDatabase files (for vector I believe there is a driver, stumbled on some posts about this).\nOther posts on this topic, both of which support the majority opinion that for rasters you really do need to open in ArcMap and export to get free of the .gdb :\nhttps://gis.stackexchange.com/questions/385255/how-to-extract-raster-from-gdb-instead-of-empty-polygons\nhttps://stackoverflow.com/questions/27821571/working-with-rasters-in-file-geodatabase-gdb-with-gdal\n(Side note, it is possible to access the tabular data in R without changing the .gdb format as demonstrated in the code chunk above, but for the spatial data I still needed to convert from .gdb to .tif).\nArcMap documentation explains that there are two ways to export a dataset (which is when you would have the option to change the file type). These are:\n\nexport raster data dialog box. I get this by right clicking the data layer listing in the side panel of ArcMap and selecting “export…”\ncopy raster tool. this tool can be found in Data management toolbox &gt; raster toolset. The documentation is helpful. I used this to scale my pixels to a new bit depth (32bit -&gt; 16bit) after replacing the values with my shorter MUKEYs using a reclass (see below). This reduced the overall file size of the tif significantly.\n\nMore about the export raster data dialog box (in ESRI’s words, from documentation linked above:\n\nThe dialog box allows you to export a raster dataset or a portion of a raster dataset. Unlike other raster import or export tools, the Export Raster Data dialog box gives you additional capabilities such as clipping via the current data frame area, clipping via a selected graphic, choosing the spatial reference of the data frame, using the current renderer, choosing the output cell size, or specifying the NoData value. In addition, you will be able to choose the output format for the raster dataset: BMP, ENVI, Esri BIL, Esri BIP, Esri BSQ, GIF, GRID, IMG, JPEG, JPEG 2000, PNG, TIFF, or exporting to a geodatabase.\n\nI learned through trial and error about what happens when you lower the pixel depth even though you have values beyond the acceptable range: “If the pixel type is demoted (lowered), the raster values outside the valid range for that pixel depth will be truncated and lost”\n\n\n1.6.3 Process for raster reclass (new MUKEYs)\nOpen .tif version of gSSURGO in ArcMap version XXX.\nFirst need to create a table of new (shorter) MUKEY values. The table I provided to the raster reclass tool below was a text file of our cross-walk between the original map unit keys and our new, shorter map unit keys that allowed us to reduce the file size by going down to 16bit pixel depth. I created this original crosswalk table by:\n\nBuild pyramids\nBuild attribute table in Data Management Toolbox &gt; Raster &gt; Raster Properties &gt; Build Raster Attribute Table\nOpen attribute table, add field called “new_mu” with “short integer type” (b/c only 2 bits to store short integers and we want to reduce size). Field properties “precision” leaving at 0. Now new_mu is the same as OID, just integers starting at 0 and going all the way to 7861\nSaved attribute table as a .txt file for future reference.\n\nThen, I performed a raster reclass by table. Had to turn on the 3D analyst tools before I could access them (they were grayed out). Using Customize menu &gt; Extensions, turned on the 3D analyst tools and spatial analyst tools\nNow in the ArcToolbox, going to 3D analyst tools &gt; Raster Reclass &gt; Reclass by Table\nInput raster: MapunitRaster_10m_Clip1\nInput remap table: mukey_new_crosswalk.txt\nFrom value field: MUKEY\nTo value field: MUKEY\nOutput value fuield: MUKEY_new\nChange missing values to NoData: DATA?\nSUCCESS with raster reclass above. To save, I used “Export Raster Data”\nDialog box shows that the uncompressed size is 5.42 GB (!!), and the pixel depth is 16bit. I am assigning the NoData value as 7999 (our max MU value is 7861)\nSaving to C:\\Users\\blair304\\Desktop\\MapunitRaster_10m_Clip1, name Reclass_tif1.tif, compression type NONE, compression quality set at default 75 (can’t see a way to change it?)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "02-data_sources.html#ncss-kellogg-lab-data",
    "href": "02-data_sources.html#ncss-kellogg-lab-data",
    "title": "1  Data Sources",
    "section": "1.7 NCSS (Kellogg Lab) Data",
    "text": "1.7 NCSS (Kellogg Lab) Data\nNavigate to this link: https://ncsslabdatamart.sc.egov.usda.gov/\nClick on “Advanced Query”\nSpecify:\n\nCountry: United States\nState: Minnesota\nSubmission Date Jan 1, 2000 - Oct 17 2022\n\nThis returns 145 records.\nHad to do the downloads separated by “data tiers” because I got a network timeout / server error when I tried highlighting everything. Here are the batches:\n\nCarbon and extractions\nPSDA & Rock frags, WAter content\nBulk density & moisture, water content\nCEC and bases, salt, organic\nPhosphorus\nTaxonomy tier 1 & 2\npH & carbonates\n\n\n1.7.1 Accessing data\n\nFor each chunk of data I downloaded (see bullet points above), have a folder with CSV files. Each folder contains a data dictionary, a “site” csv, and a “pedon” csv\nlatitude & longitude data are in the “site” files. Not clear if there is usually 1 pedon per site, and so this lat/lon are probably right? Or would there be multiple pedons at a given site, so the site location won’t be quite right?\nwith code in R/unzip_merge_ncss_data.R, created “NCSS_validation_point_key_site_pedon.csv”, which is all pedons with location data (56 total)\n\nNeed to match up the right lab methods from the NCSS dataset so they correspond with the data I pulled from gSSURGO. Looking up more info in the gSSURGO metadata about these to see if methods are listed\n\n\n\n\n\n\n\n\nvariable\ngSSURGO info\nSpreadsheet\n\n\n\n\nx clay\n\n\n\n\nx SOC (for OM)\nOM (LOI), pull SOC data from NCSS and convert\n\n\n\nx CEC\npH 7\n\n\n\nx bulk density\n1/3 bar\nDb1/3_4A1d_Caj_g/cc_0_CMS_0_0 AND/OR Db13b_3B1b_Caj_0_SSL_0_0 AND/OR Db13b_DbWR1_Caj_g/cc_0_SSL_0_0\n\n\nx EC\nsaturated paste\n\n\n\nx pH\n1:1 H2O\n\n\n\nx carbonates\nweight percent CaCO3 equivalent\n\n\n\nx LEP\n\nCOLE?\n\n\nO KSAT\n\nSee notes below on ksat, don’t have this data for now, think it’s not the most important for validation\n\n\nx AWC\nvolume fraction, diff b/t water contents at 1/10 or 1/3 and 15 bar\n0 or 1/3 bar and 15 bars tension\n\n\nx fragvol_r\nvolume percentage of horizon occupied by 2mm or larger fraction (20mm or larger for wood fragments) on a whole soil basis\n? I found weight fractions but not volume fractions in the PSDA and rock fragments table.\n\n\n\nI couldn’t find ksat data in any of the NCSS tables. Suspect it might be because it is estimated using some kind of pedotransfer function. I found a Masters thesis from 2017 by Joshua Randall Weaver that seems to confirm this based on references in their literature review. The thesis is “Comparison of Saturated Hydraulic Conductivity using Geospatial Analysis of Field and SSURGO data for septic tank suitability assessment”, Clemson University. On pg 2 (introduction), Weaver states\n\n“Currently, SSURGO-reported saturated hydraulic conductivity (Ksat) data are often estimated from particle-size analysis (PSA) data from specific locations and then extrapolated across large areas based on soil map units (O’Neal, 1952; Rawls and Brakensiek, 1983; Williamson et al., 2014).” Rare comparisons of SSURGO recorded PSA-derived Ksat values are often different from site-specific field Ksat measurements (Hart et al. 2008). The freely available PSA-derived Ksat data from SSURGO is frequently used for regional and national modelling for the purposes of environmental management, but spatial variability associated with using SSURGO data instead of site-specific data is largely unknown (Hoos and McMahon, 2009). ”\n\nFor now I’m not going to worry too much about Ksat data, as it doesn’t seem like the most important thing to validate (very few SH studies collect it right now, it’s highly variable, it will be related to particle size). So I’m leaving this aside for now, but wanted to document in case I come back in the future.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "02-data_sources.html#noaa-ncei-u.s.-climate-normals---station-data",
    "href": "02-data_sources.html#noaa-ncei-u.s.-climate-normals---station-data",
    "title": "1  Data Sources",
    "section": "1.8 NOAA NCEI U.S. Climate Normals - Station Data",
    "text": "1.8 NOAA NCEI U.S. Climate Normals - Station Data\n\nOn January 9, 2023 Access the map interface for US Climate Normals Data at https://www.ncei.noaa.gov/maps/normals\nOriginally did to to access station data, but later also to download climate rasters for the climate clustering analysis (see 32-sample-climate-vars.qmd)\nSelect 1991-2020 Climate Normals in the side bar, Annual Normals (hourly, daily, and monthly are also available)\nSelect the wrench icon next to the selected dataset in the sidebar to open the “Tools” for that dataset. Use the “identify” (i) tool to pick a station and learn it’s name and ID number.\nI picked two stations representing the far NW and far SE corners of my area of interest:\n\nOne in the far northwest (Hallock, MN USC00213455) and one in the far southeast (Caledonia MN, USC00211198)\nNW: Hallock, MN MAT is 39.1 F and MAP is 22.31 inches\nSE: Caledonia, MN MAT is 45.5 F and MAP is 38.32 in\n\nRather than download the full dataset, I used the station names I had identified to see a quick summary with the “U.S. Climate Normals Quick Access” Tool.\n\nSelected the “Annual/Seasonal Tab” and 1991-2020 tab\n\n\nRecommended dataset citation, per metadata page, is Arguez, A., I. Durre, S. Applequist, R. Vose, M. Squires, X. Yin, R. Heim, and T. Owen, 2012: NOAA’s 1981-2010 climate normals: An overview. Bull. Amer. Meteor. Soc., 93, 1687-1697. (this is saved in my Zotero library already)\n\nQuick conversions for getting the MAT and MAP at the NW and SE corners of my area of interest:\n\n# MAT convert from F to C: subtract 32 and multiply by 5/9\n(39.1 - 32)*(5/9) # Hallock (NW)\n\n[1] 3.944444\n\n(45.5- 32)*(5/9) # Caledonia (SE)\n\n[1] 7.5\n\n# MAP convert inches to mm (1in=2.54cm, 10mm = 1cm)\n22.31*2.54*10 # Hallock (NW)\n\n[1] 566.674\n\n38.32*2.54*10 # Caledonia (SE)\n\n[1] 973.328",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Sources</span>"
    ]
  },
  {
    "objectID": "03-mus_comp_in_aoi.html",
    "href": "03-mus_comp_in_aoi.html",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "",
    "text": "2.1 Overview\nI have a list of map unit keys from my clipped area of interest (saved crosswalk text file after clipping my AOI in ArcMap, see Chapter 1). In order to reduce the file size of the TIF I was working with, I created new map unit key IDs. The cross-walk table is in data/gSSURGO_MN/mukey_new_crosswalk.txt. There are 7,862 unique map unit keys in my area of interest (AOI).\nThis is what the cross-walk table looks like:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explore Map Units & Components in AOI</span>"
    ]
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#map-units",
    "href": "03-mus_comp_in_aoi.html#map-units",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.2 Map Units",
    "text": "2.2 Map Units\nI can use the map unit keys (MUKEYs) from my AOI to get more information on my target map units by calling up different tables from gSSURGO and subsetting based on the MUKEYs, and identifying the components within each mapunit in my AOI.\nFor more info about SSURGO tables and columns, refer to descriptions in the SSURGO metadata PDFs in data/gSSURGO_MN/\n\n# which layers (tables) are available to me?\n# can inspect layers in .gdb\nsf::st_layers(\"./data/gSSURGO_MN/gSSURGO_MN.gdb\")\n\nDriver: OpenFileGDB \nAvailable layers:\n                   layer_name     geometry_type features fields\n1                    chaashto                NA   223330      4\n2               chconsistence                NA     2794     10\n3               chdesgnsuffix                NA    62846      3\n4                     chfrags                NA   321265     12\n5                    chorizon                NA   130493    171\n6                     chpores                NA       16      9\n7                    chstruct                NA   110021      7\n8                 chstructgrp                NA    95901      4\n9                      chtext                NA        0      7\n10                  chtexture                NA   318704      4\n11               chtexturegrp                NA   313739      6\n12               chtexturemod                NA    50000      3\n13                  chunified                NA   240027      4\n14              cocanopycover                NA       50      6\n15                  cocropyld                NA    34146     12\n16             codiagfeatures                NA    31760     12\n17                 coecoclass                NA    52483      6\n18                  coeplants                NA    25544      7\n19               coerosionacc                NA     5743      4\n20                  coforprod                NA     2726     12\n21                 coforprodo                NA        0     10\n22               cogeomordesc                NA    83667      8\n23           cohydriccriteria                NA    25091      3\n24                   cointerp                NA  5698886     13\n25                    comonth                NA   421728     17\n26                  component                NA    42529    109\n27                       copm                NA    49990      7\n28                    copmgrp                NA    34883      4\n29               copwindbreak                NA   161238      8\n30             corestrictions                NA     4369     13\n31                cosoilmoist                NA   666791      9\n32                 cosoiltemp                NA       96      9\n33                cosurffrags                NA    10955     15\n34              cosurfmorphgc                NA    17984      6\n35             cosurfmorphhpp                NA    28248      3\n36              cosurfmorphmr                NA      460      3\n37              cosurfmorphss                NA    42228      4\n38                 cotaxfmmin                NA    33654      3\n39               cotaxmoistcl                NA    20332      3\n40                     cotext                NA    42529      7\n41               cotreestomng                NA    32192      5\n42                cotxfmother                NA    31091      3\n43               distinterpmd                NA    11868      8\n44               distlegendmd                NA       92     11\n45                     distmd                NA       92      4\n46                   featdesc                NA      801      6\n47                  laoverlap                NA      512      6\n48                     legend                NA       92     14\n49                 legendtext                NA        0      7\n50                    mapunit                NA    10688     24\n51                      month                NA       12      2\n52                   muaggatt                NA    10688     40\n53                 muaoverlap                NA    35224      4\n54                  mucropyld                NA    23355     10\n55                     mutext                NA    12230      7\n56                  sacatalog                NA       92     11\n57                   sainterp                NA    11868      9\n58               sdvalgorithm                NA        8      4\n59               sdvattribute                NA      211     53\n60                  sdvfolder                NA       20      6\n61         sdvfolderattribute                NA      213      2\n62               mdstatdomdet                NA     6930      5\n63               mdstatdommas                NA      123      2\n64               mdstatidxdet                NA      172      4\n65               mdstatidxmas                NA      149      3\n66             mdstatrshipdet                NA       66      5\n67             mdstatrshipmas                NA       63      5\n68              mdstattabcols                NA      865     14\n69                 mdstattabs                NA       75      5\n70                   FEATLINE Multi Line String    60902      5\n71                  FEATPOINT             Point   254435      4\n72                     MULINE Multi Line String        0      5\n73                    MUPOINT             Point        0      4\n74                  SAPOLYGON     Multi Polygon       96      5\n75                  MUPOLYGON     Multi Polygon  2123552      6\n76                      Valu1                NA    10688     58\n77          MapunitRaster_10m     Multi Polygon        1      3\n78      VAT_MapunitRaster_10m                NA    10688      3\n79 fras_aux_MapunitRaster_10m                NA        1      3\n80 fras_blk_MapunitRaster_10m                NA   180371      6\n81 fras_bnd_MapunitRaster_10m                NA        1     18\n82 fras_ras_MapunitRaster_10m                NA        1      3\n               crs_name\n1                  &lt;NA&gt;\n2                  &lt;NA&gt;\n3                  &lt;NA&gt;\n4                  &lt;NA&gt;\n5                  &lt;NA&gt;\n6                  &lt;NA&gt;\n7                  &lt;NA&gt;\n8                  &lt;NA&gt;\n9                  &lt;NA&gt;\n10                 &lt;NA&gt;\n11                 &lt;NA&gt;\n12                 &lt;NA&gt;\n13                 &lt;NA&gt;\n14                 &lt;NA&gt;\n15                 &lt;NA&gt;\n16                 &lt;NA&gt;\n17                 &lt;NA&gt;\n18                 &lt;NA&gt;\n19                 &lt;NA&gt;\n20                 &lt;NA&gt;\n21                 &lt;NA&gt;\n22                 &lt;NA&gt;\n23                 &lt;NA&gt;\n24                 &lt;NA&gt;\n25                 &lt;NA&gt;\n26                 &lt;NA&gt;\n27                 &lt;NA&gt;\n28                 &lt;NA&gt;\n29                 &lt;NA&gt;\n30                 &lt;NA&gt;\n31                 &lt;NA&gt;\n32                 &lt;NA&gt;\n33                 &lt;NA&gt;\n34                 &lt;NA&gt;\n35                 &lt;NA&gt;\n36                 &lt;NA&gt;\n37                 &lt;NA&gt;\n38                 &lt;NA&gt;\n39                 &lt;NA&gt;\n40                 &lt;NA&gt;\n41                 &lt;NA&gt;\n42                 &lt;NA&gt;\n43                 &lt;NA&gt;\n44                 &lt;NA&gt;\n45                 &lt;NA&gt;\n46                 &lt;NA&gt;\n47                 &lt;NA&gt;\n48                 &lt;NA&gt;\n49                 &lt;NA&gt;\n50                 &lt;NA&gt;\n51                 &lt;NA&gt;\n52                 &lt;NA&gt;\n53                 &lt;NA&gt;\n54                 &lt;NA&gt;\n55                 &lt;NA&gt;\n56                 &lt;NA&gt;\n57                 &lt;NA&gt;\n58                 &lt;NA&gt;\n59                 &lt;NA&gt;\n60                 &lt;NA&gt;\n61                 &lt;NA&gt;\n62                 &lt;NA&gt;\n63                 &lt;NA&gt;\n64                 &lt;NA&gt;\n65                 &lt;NA&gt;\n66                 &lt;NA&gt;\n67                 &lt;NA&gt;\n68                 &lt;NA&gt;\n69                 &lt;NA&gt;\n70 NAD83 / Conus Albers\n71 NAD83 / Conus Albers\n72 NAD83 / Conus Albers\n73 NAD83 / Conus Albers\n74 NAD83 / Conus Albers\n75 NAD83 / Conus Albers\n76                 &lt;NA&gt;\n77 NAD83 / Conus Albers\n78                 &lt;NA&gt;\n79                 &lt;NA&gt;\n80                 &lt;NA&gt;\n81                 &lt;NA&gt;\n82                 &lt;NA&gt;\n\nmn_gdb &lt;- \"data/gSSURGO_MN/gSSURGO_MN.gdb\" \n\n# read only mapunit table, as dataframe\nmn_mapunits &lt;- sf::st_read(dsn = mn_gdb, layer = \"mapunit\")\n\nReading layer `mapunit' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n\nThese are the columns in the mapunit table:\n\ncolnames(mn_mapunits)\n\n [1] \"musym\"         \"muname\"        \"mukind\"        \"mustatus\"     \n [5] \"muacres\"       \"mapunitlfw_l\"  \"mapunitlfw_r\"  \"mapunitlfw_h\" \n [9] \"mapunitpfa_l\"  \"mapunitpfa_r\"  \"mapunitpfa_h\"  \"farmlndcl\"    \n[13] \"muhelcl\"       \"muwathelcl\"    \"muwndhelcl\"    \"interpfocus\"  \n[17] \"invesintens\"   \"iacornsr\"      \"nhiforsoigrp\"  \"nhspiagr\"     \n[21] \"vtsepticsyscl\" \"mucertstat\"    \"lkey\"          \"mukey\"        \n\n\nWe will subset to only include MUKEYs in our AOI.\n\n# keep only the map units in my AOI (n=7862)\ntarget_mapunits &lt;- mn_mapunits %&gt;% \n  filter(mukey %in% aoi_mu$MUKEY)\n\n# summary of map unit types\ntarget_mapunits %&gt;% \n  dplyr::group_by(mukind) %&gt;% \n  dplyr::summarise(n = n()) \n\n\n  \n\n\n# when is mukind undefined?\ntarget_mapunits %&gt;% \n  filter(is.na(mukind))\n\n\n  \n\n\n# what's going on with W = water?\nwater_mukeys &lt;- target_mapunits %&gt;% \n  filter(musym == \"W\") %&gt;% \n  pull(mukey)\n\ntarget_mapunits %&gt;% \n  filter(mukey %in% water_mukeys) %&gt;% \n  head()\n\n\n  \n\n\n# saving CSV of target map unit info\nwrite_csv(target_mapunits, \"data/target_mapunit_table.csv\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explore Map Units & Components in AOI</span>"
    ]
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#components",
    "href": "03-mus_comp_in_aoi.html#components",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.3 Components",
    "text": "2.3 Components\nFirst a reminder of the columns in the component table:\n\nmn_components &lt;- sf::st_read(dsn = mn_gdb, layer = \"component\")\n\nReading layer `component' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\ncolnames(mn_components)\n\n  [1] \"comppct_l\"        \"comppct_r\"        \"comppct_h\"       \n  [4] \"compname\"         \"compkind\"         \"majcompflag\"     \n  [7] \"otherph\"          \"localphase\"       \"slope_l\"         \n [10] \"slope_r\"          \"slope_h\"          \"slopelenusle_l\"  \n [13] \"slopelenusle_r\"   \"slopelenusle_h\"   \"runoff\"          \n [16] \"tfact\"            \"wei\"              \"weg\"             \n [19] \"erocl\"            \"earthcovkind1\"    \"earthcovkind2\"   \n [22] \"hydricon\"         \"hydricrating\"     \"drainagecl\"      \n [25] \"elev_l\"           \"elev_r\"           \"elev_h\"          \n [28] \"aspectccwise\"     \"aspectrep\"        \"aspectcwise\"     \n [31] \"geomdesc\"         \"albedodry_l\"      \"albedodry_r\"     \n [34] \"albedodry_h\"      \"airtempa_l\"       \"airtempa_r\"      \n [37] \"airtempa_h\"       \"map_l\"            \"map_r\"           \n [40] \"map_h\"            \"reannualprecip_l\" \"reannualprecip_r\"\n [43] \"reannualprecip_h\" \"ffd_l\"            \"ffd_r\"           \n [46] \"ffd_h\"            \"nirrcapcl\"        \"nirrcapscl\"      \n [49] \"nirrcapunit\"      \"irrcapcl\"         \"irrcapscl\"       \n [52] \"irrcapunit\"       \"cropprodindex\"    \"constreeshrubgrp\"\n [55] \"wndbrksuitgrp\"    \"rsprod_l\"         \"rsprod_r\"        \n [58] \"rsprod_h\"         \"foragesuitgrpid\"  \"wlgrain\"         \n [61] \"wlgrass\"          \"wlherbaceous\"     \"wlshrub\"         \n [64] \"wlconiferous\"     \"wlhardwood\"       \"wlwetplant\"      \n [67] \"wlshallowwat\"     \"wlrangeland\"      \"wlopenland\"      \n [70] \"wlwoodland\"       \"wlwetland\"        \"soilslippot\"     \n [73] \"frostact\"         \"initsub_l\"        \"initsub_r\"       \n [76] \"initsub_h\"        \"totalsub_l\"       \"totalsub_r\"      \n [79] \"totalsub_h\"       \"hydgrp\"           \"corcon\"          \n [82] \"corsteel\"         \"taxclname\"        \"taxorder\"        \n [85] \"taxsuborder\"      \"taxgrtgroup\"      \"taxsubgrp\"       \n [88] \"taxpartsize\"      \"taxpartsizemod\"   \"taxceactcl\"      \n [91] \"taxreaction\"      \"taxtempcl\"        \"taxmoistscl\"     \n [94] \"taxtempregime\"    \"soiltaxedition\"   \"castorieindex\"   \n [97] \"flecolcomnum\"     \"flhe\"             \"flphe\"           \n[100] \"flsoilleachpot\"   \"flsoirunoffpot\"   \"fltemik2use\"     \n[103] \"fltriumph2use\"    \"indraingrp\"       \"innitrateleachi\" \n[106] \"misoimgmtgrp\"     \"vasoimgtgrp\"      \"mukey\"           \n[109] \"cokey\"           \n\n\nSummarizing some info about number of components, major components\n\n# keep the components in my target map units\ntarget_comp &lt;- mn_components %&gt;% \n  filter(mukey %in% target_mapunits$mukey) \n\n# looking at this, it's probabaly safe to exclude \n# compname == \"Water\" when the comppct_r is also 100%\n# there's one here where water is only 5%, \"Riverwash\", rest are 100%\nwater_comp &lt;- target_comp %&gt;% filter(mukey %in% water_mukeys)\n\n# how many components?\nnrow(target_comp)\n\n[1] 31065\n\n# how many major components? (defined by \"majcompflag\")\ntarget_comp %&gt;% \n  filter(majcompflag == \"Yes\") %&gt;% \n  nrow() \n\n[1] 10588\n\n\nFor our unique MUKEYs, how many components in each?\n\n# how many components per mukey? \ncomp_nest &lt;- target_comp %&gt;%\n  dplyr::group_by(mukey) %&gt;% \n  nest()\n\ncomp_nest_n &lt;- comp_nest %&gt;% \n  dplyr::mutate(n_comp = map_dbl(data, nrow))\n\ncomp_nest_n %&gt;% \n  dplyr::group_by(n_comp) %&gt;% \n  count(name = \"n_mapunits\")\n\n\n  \n\n\n\n\n# just the major components from target MUs\nmaj_comp &lt;- target_comp %&gt;% \n  filter(majcompflag == \"Yes\")\n\nnrow(maj_comp)\n\n[1] 10588",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explore Map Units & Components in AOI</span>"
    ]
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#taxonomy-summaries",
    "href": "03-mus_comp_in_aoi.html#taxonomy-summaries",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.4 Taxonomy Summaries",
    "text": "2.4 Taxonomy Summaries\n\n2.4.1 Order\n\ntarget_comp %&gt;% \n  dplyr::group_by(taxorder) %&gt;% \n  dplyr::summarise(n = n()) %&gt;% \n  arrange(n)\n\n\n  \n\n\n# what's up with taxonomic order = NA?\n# not really farmable stuff.\nmaj_comp %&gt;% \n  filter(is.na(taxorder)) %&gt;% \n  pull(compname) %&gt;% \n  unique()\n\n [1] \"Water\"                  \"Pits\"                   \"Dumps\"                 \n [4] \"Urban land\"             \"Riverwash\"              \"Highway\"               \n [7] \"Pits, gravel\"           \"Rock outcrop\"           \"Beaches\"               \n[10] \"Pits, limestone quarry\" \"Dune land\"              \"Terrace escarpments\"   \n[13] \"Alluvial\"              \n\n\n\n\n2.4.2 Suborder\n\nsuborders &lt;- target_comp %&gt;% \n  dplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(n = n()) \n\n  \nsuborders %&gt;% \n  ggplot() +\n  geom_col(aes(y = reorder(taxsuborder, -n), x = n)) +\n  ylab(\"Suborder\") + \n  xlab(\"Count\") + \n  theme_bw() +\n  ggtitle(\"Number of components by suborder\")\n\n\n\n\n\n\n\n# what's up with taxonomic suborder = NA?\ncomps_no_taxsuborder &lt;- target_comp %&gt;% \n  filter(is.na(taxsuborder)) %&gt;% \n  pull(compname) %&gt;% \n  unique()\n\nlength(comps_no_taxsuborder)\n\n[1] 829\n\nhead(comps_no_taxsuborder, n = 20)\n\n [1] \"Water\"                                  \n [2] \"Pits\"                                   \n [3] \"Dumps\"                                  \n [4] \"Cormant\"                                \n [5] \"Urban land\"                             \n [6] \"Mavie\"                                  \n [7] \"Kratka\"                                 \n [8] \"Radium\"                                 \n [9] \"Sahkahtay\"                              \n[10] \"Deerwood\"                               \n[11] \"Flaming\"                                \n[12] \"Northwood\"                              \n[13] \"Rosewood\"                               \n[14] \"Roliss\"                                 \n[15] \"Hamre\"                                  \n[16] \"Percy\"                                  \n[17] \"Berner\"                                 \n[18] \"Soils that have a mineral surface layer\"\n[19] \"Strathcona\"                             \n[20] \"Huot\"                                   \n\ntail(comps_no_taxsuborder, n = 20)\n\n [1] \"Sawmill\"                             \n [2] \"Marshan\"                             \n [3] \"Tripoli\"                             \n [4] \"Poorly drained sandy alluvial soils\" \n [5] \"Poorly and very poorly drained soils\"\n [6] \"Newalbin\"                            \n [7] \"Clrippin\"                            \n [8] \"Collinwood\"                          \n [9] \"Lakefield\"                           \n[10] \"Ocheyedan\"                           \n[11] \"Fostoria\"                            \n[12] \"Crippen\"                             \n[13] \"Farrer\"                              \n[14] \"Ocheydan\"                            \n[15] \"Grogan\"                              \n[16] \"Farrar\"                              \n[17] \"PD sandy soils\"                      \n[18] \"Moundprairie\"                        \n[19] \"PD soils\"                            \n[20] \"Walford Variant\"                     \n\n\n\n\n2.4.3 Subgroup & Family\nOn 11 August, 2022 Nic and I talked about how this could be another way to pull out mineralogy information. Could grab “smectitic” or “mixed” from these names if we wanted to do yes/no variable.\n\ntarget_comp %&gt;% \n  pull(taxclname) %&gt;% \n  unique() %&gt;% \n  head()\n\n[1] \"Sandy over clayey, mixed over smectitic, frigid Typic Calciaquolls\"\n[2] \"Sandy over clayey, mixed over smectitic, frigid Aquic Calciudolls\" \n[3] \"Mixed, frigid Aquic Udipsamments\"                                  \n[4] \"Fine, smectitic, frigid Typic Epiaquerts\"                          \n[5] \"Sandy, mixed, frigid Typic Endoaquolls\"                            \n[6] \"Sandy, mixed, frigid Aeric Calciaquolls\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explore Map Units & Components in AOI</span>"
    ]
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#variables-of-interest",
    "href": "03-mus_comp_in_aoi.html#variables-of-interest",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.5 Variables of Interest",
    "text": "2.5 Variables of Interest\nI think my next step is to use the chorizon table and pull the variables I’m interested in for each of my major components. Devine et al. 2021 did 0-30cm depth-weighted averages. I am going to do 0-20cm because I believe this is more likely to be the available depth in our validation datasets (esp. CIG).\nThen I can turn my horizon table into an SPC object, with cokey as “site”?\n\nchorizon &lt;- mn_components &lt;- sf::st_read(dsn = mn_gdb, layer = \"chorizon\")\n\nReading layer `chorizon' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\ntarget_horiz &lt;- chorizon %&gt;% \n  filter(cokey %in% maj_comp$cokey)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explore Map Units & Components in AOI</span>"
    ]
  },
  {
    "objectID": "03-mus_comp_in_aoi.html#sec-diag-feat",
    "href": "03-mus_comp_in_aoi.html#sec-diag-feat",
    "title": "2  Explore Map Units & Components in AOI",
    "section": "2.6 Diagnostic Features",
    "text": "2.6 Diagnostic Features\nHere, I’m pulling out the depth to restrictive horizon (if any) in the profile. Note that this may be &gt;20cm, which is the depth we are summarizing to for the soil properties.\n\n# read only diag features table, as dataframe\ndiag &lt;- sf::st_read(dsn = mn_gdb, layer = \"codiagfeatures\")\n\nReading layer `codiagfeatures' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n# check out what kind of diagnostic features we have\nunique(diag$featkind) %&gt;% head(20)\n\n [1] \"Histic epipedon\"                         \n [2] \"Ochric epipedon\"                         \n [3] \"Albic materials\"                         \n [4] \"Albic horizon\"                           \n [5] \"Argillic horizon\"                        \n [6] \"Aquic conditions\"                        \n [7] \"Lamellae\"                                \n [8] \"Sapric soil materials\"                   \n [9] \"Hemic soil materials\"                    \n[10] \"Fibric soil materials\"                   \n[11] \"Cambic horizon\"                          \n[12] \"Mollic epipedon\"                         \n[13] \"Calcic horizon\"                          \n[14] \"Abrupt textural change\"                  \n[15] \"Slickensides\"                            \n[16] \"Strongly contrasting particle size class\"\n[17] \"Lithologic discontinuity\"                \n[18] NA                                        \n[19] \"Lithic contact\"                          \n[20] \"Spodic horizon\"                          \n\ntarget_diag &lt;- diag %&gt;% \n  filter(cokey %in% maj_comp$cokey)\n\nOk, so from above we can see that 10458 cokeys out of a total of 10588 have something entered in the featkind field as a diagnostic feature. Now let’s investigate how many of those might be considered restrictive to roots (relevant in agricultural context):\n\ntarget_restr &lt;- target_diag %&gt;% \n  filter(featkind %in% c(\"Lithic contact\", \n                         \"Densic contact\",\n                         \"Paralithic contact\"))\n\n# how many cokeys have a restrictive horizon? \nlength(unique(target_restr$cokey))\n\n[1] 69\n\ntarget_restr\n\n\n  \n\n\n# save this in case we want to investigate further\n#it's such a small number, seems unlikely \nwrite_csv(target_restr, \"./data/restr_horiz_data_cokey.csv\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Explore Map Units & Components in AOI</span>"
    ]
  },
  {
    "objectID": "04-subset-component-data.html",
    "href": "04-subset-component-data.html",
    "title": "3  Identify Target Components",
    "section": "",
    "text": "3.1 Setup\nFirst, load the list of MUKEYs generated in previous chapter 03-mus_comp_in_aoi.qmd\nWe will use our MUKEYs to get the relevant components. I’ve created a list column with all the components for each MUKEY in it. There are 7862 unique map units in my area of interest (AOI).\nmn_gdb &lt;- \"data/gSSURGO_MN/gSSURGO_MN.gdb\" \n\n# read only component table, as dataframe\nmn_comp &lt;- sf::st_read(dsn = mn_gdb, layer = \"component\")\n\nReading layer `component' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\ntarget_comp &lt;- mn_comp %&gt;% \n  dplyr::filter(mukey %in% mus$mukey)\n\ncomp_nest &lt;- target_comp %&gt;% \n  dplyr::group_by(mukey) %&gt;% \n  nest() %&gt;% \n  dplyr::mutate(n_comp = map_dbl(data, nrow), \n                max_comp_pct = map_dbl(data,\n                                       ~max(.x[\"comppct_r\"])),\n                min_comp_pct = map_dbl(data, ~min(.x[\"comppct_r\"])))\n\nhead(comp_nest)\nAfter some troubleshooting in later steps, I came back and decided it makes the most sense to save a key of the dominant component percent in each map unit here, before anything gets dropped in the conversion to an aqp object, see Section 4.4.4 for more on this.\nThis is relevant because we will use the dominant component percentage (which component has highest comppct_r as one of our “data sufficiency” checks to determine if a given MUKEY is included in the clustering analysis. In doing some exploratory work in Section 6.9, I noticed that we had some\ndom_cmp_key &lt;- target_comp %&gt;% \n  dplyr::select(cokey, mukey, comppct_r) %&gt;% \n  dplyr::group_by(mukey) %&gt;% \n  dplyr::summarise(dom_comppct = max(comppct_r))\n\nwrite_csv(dom_cmp_key, \"./data/key_dominant_component_percent.csv\")  \n  \n# remove b/c this is big, we are done with it\nrm(mn_comp)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identify Target Components</span>"
    ]
  },
  {
    "objectID": "04-subset-component-data.html#components-per-mapunit",
    "href": "04-subset-component-data.html#components-per-mapunit",
    "title": "3  Identify Target Components",
    "section": "3.2 Components per mapunit",
    "text": "3.2 Components per mapunit\nOut of curiosity, what does the distribution look like for number of components in a mapunit?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identify Target Components</span>"
    ]
  },
  {
    "objectID": "04-subset-component-data.html#subset-to-components-15",
    "href": "04-subset-component-data.html#subset-to-components-15",
    "title": "3  Identify Target Components",
    "section": "3.3 Subset to components >15%",
    "text": "3.3 Subset to components &gt;15%\n\ncomp_sub &lt;- comp_nest %&gt;%\n  dplyr::mutate(data_maj15 = map(data, ~ filter(.x, comppct_r &gt;= 15))) %&gt;%\n  dplyr::select(mukey, data_maj15) %&gt;%\n  dplyr::mutate(\n    n_comp_maj = map_dbl(data_maj15, nrow),\n    max_pct = map_dbl(data_maj15,\n                           ~ max(.x[\"comppct_r\"])),\n    min_pct = map_dbl(data_maj15, ~ min(.x[\"comppct_r\"]))\n  )\n  \n\nhead(comp_sub)\n\n\n  \n\n\n\nNow how many major components are we working with per map unit?\n\n\n\n\n\n\n\n\n\nI’m saving this simple list of map units and number of components, max component percent for a later step where we determine if there is enough data to include that map unit in our final analysis.\n\nmajor_mu_summary &lt;- comp_sub %&gt;% \n  dplyr::select(mukey, n_comp_maj, max_pct, min_pct)\n\nwrite_csv(major_mu_summary, \"./data/mu_summary_maj_only.csv\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identify Target Components</span>"
    ]
  },
  {
    "objectID": "04-subset-component-data.html#identify-unique-components",
    "href": "04-subset-component-data.html#identify-unique-components",
    "title": "3  Identify Target Components",
    "section": "3.4 Identify unique components",
    "text": "3.4 Identify unique components\n\ncomp_unnest &lt;- comp_sub %&gt;% \n  dplyr::select(data_maj15, mukey) %&gt;% \n  tidyr::unnest(cols = c(data_maj15))\n\nwrite_csv(comp_unnest, \"./data/component_list.csv\")\n\nI have 7862 unique MUKEYs and 10785 and unique COKEYs. It appears that there are no repeated COKEYs shared between mapunits.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identify Target Components</span>"
    ]
  },
  {
    "objectID": "04-subset-component-data.html#pull-horizon-data",
    "href": "04-subset-component-data.html#pull-horizon-data",
    "title": "3  Identify Target Components",
    "section": "3.5 Pull Horizon Data",
    "text": "3.5 Pull Horizon Data\n\n# most of the soil property data we want is in the\n# chorizon table, om, pH, clay, etc.\nchoriz &lt;- sf::st_read(dsn = mn_gdb, layer = \"chorizon\")\n\nReading layer `chorizon' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\ntarget_choriz &lt;- choriz %&gt;% \n  dplyr::filter(cokey %in% comp_unnest$cokey)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identify Target Components</span>"
    ]
  },
  {
    "objectID": "04-subset-component-data.html#coarse-fragments",
    "href": "04-subset-component-data.html#coarse-fragments",
    "title": "3  Identify Target Components",
    "section": "3.6 Coarse Fragments",
    "text": "3.6 Coarse Fragments\n\nNeed to pull fragvol_r from the chfrags table, this is the volume percentage of horizon occupied by 2mm or larger fraction (20mm or larger for wood fragments) on a whole soil basis\nTurns out I need to aggregate this by chkey, there can be multiple fragvol_r entries for a given component-horizon.\nDidn’t end up using this data, but keeping the code here in case we need to refer to it\n\n\n# some coarse frag data in the chfrag table\nchfrag &lt;- sf::st_read(dsn = mn_gdb, layer = \"chfrags\")\n\nReading layer `chfrags' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\gSSURGO_MN\\gSSURGO_MN.gdb' \n  using driver `OpenFileGDB'\n\n\nWarning: no simple feature geometries present: returning a data.frame or tbl_df\n\n# keep only the component horizons I'm interested in\ntarget_chfrag &lt;-  chfrag %&gt;% \n  dplyr::filter(chkey %in% target_choriz$chkey)\n\n# sum volume of coarse frags in a given horizon\nfrag_hz_summary &lt;- target_chfrag %&gt;% \n  dplyr::group_by(chkey) %&gt;% \n  dplyr::summarise(fragvol_r_sum = sum(fragvol_r, na.rm = TRUE),\n            .groups = \"drop\") %&gt;% \n  dplyr::select(chkey, fragvol_r_sum) \n\n# add coarse frag col to my df\ntarget_choriz_frag &lt;- left_join(target_choriz, frag_hz_summary, by = \"chkey\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identify Target Components</span>"
    ]
  },
  {
    "objectID": "04-subset-component-data.html#save-data",
    "href": "04-subset-component-data.html#save-data",
    "title": "3  Identify Target Components",
    "section": "3.7 Save Data",
    "text": "3.7 Save Data\n\nwrite_csv(target_choriz_frag, \"data/target_choriz_all.csv\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identify Target Components</span>"
    ]
  },
  {
    "objectID": "05-horiz-data.html",
    "href": "05-horiz-data.html",
    "title": "4  Horizon Data Averages",
    "section": "",
    "text": "4.1 Overview\nGoal is to calculate 0-20cm depth weighted averages for all the soil properties of interest, for each component.\nI think I want to turn my horizon dataframe into an SPC object (aqp package), see this demo. I think I would use cokey as “site”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Horizon Data Averages</span>"
    ]
  },
  {
    "objectID": "05-horiz-data.html#toy-example",
    "href": "05-horiz-data.html#toy-example",
    "title": "4  Horizon Data Averages",
    "section": "4.2 Toy Example",
    "text": "4.2 Toy Example\nTrying this process with just 4 components I selected at random, to make sure I understand what’s happening with a smaller dataset.\n\n4.2.1 Select some components\n\n# randomly selected cokeys\nex_cokeys &lt;- c(21760486, 21791957, 21760347, 21782338)\n\n# filter to example horizons\n# add some info from comps table for context\ntoy_hz &lt;- horiz %&gt;% \n  dplyr::filter(cokey %in% ex_cokeys) %&gt;% \n  left_join(x = ., y = comp_info, by = \"cokey\")\n\n# what kinds of soils are we working with?\ntoy_hz %&gt;% \n  dplyr::select(cokey, compname, taxsuborder, geomdesc, drainagecl) %&gt;% \n  unique()\n\n\n  \n\n\n\n\n\n4.2.2 Promote to SPC Object\nBefore we promote to SPC object, need to make sure all my horizons are pre-sorted by profile id cokey, and then by horizon top boundary hzdept_r. Following along with “Object Creation” in the Introduction to SPC Objects docs.\n\ntoy_sort &lt;- toy_hz %&gt;% \n  arrange(cokey, hzdept_r) \n\n# take a look\ntoy_sort %&gt;% \n  select(compname, hzname, hzdept_r)\n\n\n  \n\n\n\n\n# upgrade to SPC\ndepths(toy_sort) &lt;-  cokey ~ hzdept_r + hzdepb_r\n\n# specify horizon name col\nhzdesgnname(toy_sort) &lt;- 'hzname'\n\n# confirm it worked\nclass(toy_sort)\n\n[1] \"SoilProfileCollection\"\nattr(,\"package\")\n[1] \"aqp\"\n\n# check out the object\nprint(toy_sort)\n\nSoilProfileCollection with 4 profiles and 14 horizons\nprofile ID: cokey  |  horizon ID: hzID \nDepth range: 152 - 200 cm\n\n----- Horizons (6 / 14 rows  |  10 / 177 columns) -----\n    cokey hzID hzdept_r hzdepb_r hzname desgndisc desgnmaster desgnmasterprime\n 21760347    1        0       23     Ap        NA           A             &lt;NA&gt;\n 21760347    2       23       41      A        NA           A             &lt;NA&gt;\n 21760347    3       41       51     AB        NA          AB             &lt;NA&gt;\n 21760347    4       51       91    Bkg        NA           B             &lt;NA&gt;\n 21760347    5       91      200     Cg        NA           C             &lt;NA&gt;\n 21760486    6        0       18     Ap        NA           A             &lt;NA&gt;\n desgnvert hzdept_l\n        NA        0\n        NA       18\n        NA       36\n        NA       46\n        NA       74\n        NA        0\n[... more horizons ...]\n\n----- Sites (4 / 4 rows  |  1 / 1 columns) -----\n    cokey\n 21760347\n 21760486\n 21782338\n 21791957\n\nSpatial Data:\n[EMPTY]\n\n\n\n\n4.2.3 Aggregate Along Slabs\nWe want to work with just the 0-20cm depth, can use slab() to do this. Example in “Aggregating Soil Profile Collections Along Regular Slabs”\nHere I’m grouping by cokey (individual profiles) so that we get a depth-wise summary for each cokey, weighted by horizon.\nUsing slab.fun = mean, I double checked and this computes a depth-weighted mean. This is clear if you examine both the horizon data and the “slab” data below for the component 21760486. The top horizon (0-18cm) has OM 2.5%, and the second horizon (18-28cm) has OM 0.25. The depth weighted average (to 20cm depth) is 2.275\nI’m pretty sure I want to include na.rm = TRUE here for slab? Since I’m doing this for each profile, it seems unlikely that I’d have data for just one horizon, but not the next horizon. I suppose I could check this for our components\n\nslab_ex &lt;- slab(object = toy_sort, \n                fm = cokey ~ om_r + claytotal_r + cec7_r,\n                slab.structure = c(0,20),\n                slab.fun = mean,\n                na.rm = TRUE) \n\n# our example component, 0-20cm depth spans 2 horizons here\ntoy_hz %&gt;% \n  select(cokey, hzdept_r, hzdepb_r, om_r, claytotal_r) %&gt;% \n  arrange(cokey, hzdept_r) %&gt;% \n  filter(cokey == 21760486)\n\n\n  \n\n\n# slab results, note they are \"long\" format\n# om_r = 2.275\nslab_ex %&gt;% \n  filter(cokey == 21760486)\n\n\n  \n\n\n# do the weighted mean for om_r by hand \n# should match slab results above for cokey 21760486 \nweighted.mean(x = c(2.5, 0.25), w = c(18,2)) \n\n[1] 2.275",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Horizon Data Averages</span>"
    ]
  },
  {
    "objectID": "05-horiz-data.html#variables",
    "href": "05-horiz-data.html#variables",
    "title": "4  Horizon Data Averages",
    "section": "4.3 Variables",
    "text": "4.3 Variables\n\n4.3.1 List to include\n\nvars_of_interest &lt;-\n  c(\n    'claytotal_r',\n    'silttotal_r',\n    'sandtotal_r',\n    'claysizedcarb_r', \n    'om_r',\n    'cec7_r',\n    'dbthirdbar_r',\n    'fragvol_r_sum',\n    'kwfact', # erodibility factor, learn more?\n    'ec_r',\n    'ph1to1h2o_r',\n    'sar_r',\n    'caco3_r',\n    'lep_r',\n    'ksat_r',\n    'awc_r'\n    # 'freeiron_r', # no data\n    # 'feoxalate_r' # no data\n  )\n\n\n\n4.3.2 Number Missing\nHow many horizons are missing data for our variables of interest?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Horizon Data Averages</span>"
    ]
  },
  {
    "objectID": "05-horiz-data.html#full-dataset-spc-workflow",
    "href": "05-horiz-data.html#full-dataset-spc-workflow",
    "title": "4  Horizon Data Averages",
    "section": "4.4 Full dataset SPC workflow",
    "text": "4.4 Full dataset SPC workflow\n\n4.4.1 Create SPC object\nHere I turn the entire horiz dataframe into an SPC object.\n\n# sort so all horizons are in order for each cokey\nhz_sort &lt;- horiz %&gt;% \n  arrange(cokey, hzdept_r) \n\n# upgrade to SPC\ndepths(hz_sort) &lt;-  cokey ~ hzdept_r + hzdepb_r\n\n# check it out \n# note only 10237 profiles, started with 10785 cokeys\n# figure out missing ones below\nhz_sort\n\nSoilProfileCollection with 10237 profiles and 37552 horizons\nprofile ID: cokey  |  horizon ID: hzID \nDepth range: 28 - 208 cm\n\n----- Horizons (6 / 37552 rows  |  10 / 173 columns) -----\n    cokey hzID hzdept_r hzdepb_r   hzname desgndisc desgnmaster\n 21694774    1        0       20       Ap        NA           A\n 21694774    2       20       56       Bw        NA           B\n 21694774    3       56      114        E        NA           E\n 21694774    4      114      116       Bt        NA           B\n 21694774    5      116      203 E and Bt        NA     E and B\n 21694776    6        0       20       Ap        NA           A\n desgnmasterprime desgnvert hzdept_l\n             &lt;NA&gt;        NA        0\n             &lt;NA&gt;        NA       13\n             &lt;NA&gt;        NA       41\n             &lt;NA&gt;        NA       61\n             &lt;NA&gt;        NA       63\n             &lt;NA&gt;        NA        0\n[... more horizons ...]\n\n----- Sites (6 / 10237 rows  |  1 / 1 columns) -----\n    cokey\n 21694774\n 21694776\n 21694781\n 21694789\n 21694790\n 21694791\n[... more sites ...]\n\nSpatial Data:\n[EMPTY]\n\n# specify horizon name col\nhzdesgnname(hz_sort) &lt;- 'hzname'\n\n\n\n4.4.2 Custom slab function\nI will use this function along with the map function from {purrr} to calculate weighted means of the soil properties on our list.\n\n# depending on your version of aqp, may see this warning,\n# could update the function below to use \"dice\" as recommended\n#\n# Note: aqp::slice() will be deprecated in aqp version 2.0\n#--&gt; Please consider using the more efficient aqp::dice()\n\nslab_fun &lt;- function(var, spc_obj){\n  \n  slab_formula &lt;- as.formula(paste(\"cokey ~ \", var))\n  \n  slab_df &lt;- slab(object = spc_obj, \n                fm = slab_formula,\n                slab.structure = c(0,20),\n                slab.fun = mean,\n                na.rm = TRUE) \n  \n  return(slab_df)\n  \n}\n\n\n\n4.4.3 Apply (map) slab function\nDebugging victory here! I was having trouble with my custom function (defined above) working with any of the variants of purrr::map() . Found the answer in the map2() documentation (but it seems to work here, even though I’m using just map() ). Under the Details section: “Note that arguments to be vectorised over come before .f , and arguments that are supplied to every call come after .f”\nSo in my case, the same spc object is used for all of these function calls; I needed to put that argument after .f .\n\n# # dataframe for results to land in\nr &lt;- data.frame(var_name = vars_of_interest)\n\n# map over my list of vars, using custom slab function\n# takes a minute or so\n# depending on version of aqp may get warning about slice (used by slab)\n# being deprecated in v2.0\nrnest &lt;- r %&gt;% \n  dplyr::mutate(aggr_data = map(.x = var_name,\n                          .f = slab_fun, \n                          spc_obj = hz_sort))\n\n# result is a list col with soil prop data nested \nhead(rnest)\n\n\n  \n\n\n\n\n# unnest, save long version b/c nice for \n# facetted plots\nrlong &lt;- rnest %&gt;% \n  unnest(cols = c(aggr_data)) %&gt;% \n  select(-var_name)   # don't need, col also returned by slab\n\n  \n# save wider version b/c better for modeling\n# this matches the number of elements in our SPC (10237)\n# not sure why the difference between nrow of comps and n\n# elements in our SPC... look into this. \nrwide &lt;- rlong %&gt;% \n  pivot_wider(names_from = \"variable\",\n              values_from = c(\"value\", \"contributing_fraction\"),\n              names_glue = \"{variable}_{.value}\")\n\n\n\n4.4.4 Missing/Dropped Components?\nI started with 10785 that were identified in 04-subset-component-data.qmd. However, when I promoted this list to an SPC object in aqp we got only 10237 . Check out what’s missing.\n\ncomps_start &lt;- comps$cokey\n\ncomps_end &lt;- rwide$cokey\n\nmissing_cokeys &lt;- setdiff(comps_start, comps_end)\n\nmissing_comps &lt;- comps %&gt;% \n  filter(cokey %in% missing_cokeys) %&gt;% \n  select(compname,\n         compkind,\n         majcompflag,\n         taxclname,\n         taxorder,\n         taxsuborder,\n         taxgrtgroup,\n         taxsubgrp,\n         mukey)\n\nHere’s more info about the missing components, by component kind:\n\n\n\n\n\n\n\n\n\nCheck out component names, grouped by component kind.\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Horizon Data Averages</span>"
    ]
  },
  {
    "objectID": "05-horiz-data.html#save-data",
    "href": "05-horiz-data.html#save-data",
    "title": "4  Horizon Data Averages",
    "section": "4.5 Save Data",
    "text": "4.5 Save Data\nBased on the summaries of missing/dropped components above, I think this is all stuff we would have wanted to exclude anyway (and data availability is likely low…). Saving long and wide versions of the 20cm slab-aggregated dataset for further analysis.\n\ndatestamp &lt;- lubridate::today() %&gt;% str_replace_all(\"-\", \"\")\n\nwrite_csv(rlong, glue(\"./data/long_slab_aggregated_soil_props_{datestamp}.csv\"))\n\nwrite_csv(rwide, glue(\"./data/wide_slab_aggregated_soil_props_{datestamp}.csv\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Horizon Data Averages</span>"
    ]
  },
  {
    "objectID": "05-horiz-data.html#implicit-zeroes-not-used",
    "href": "05-horiz-data.html#implicit-zeroes-not-used",
    "title": "4  Horizon Data Averages",
    "section": "4.6 Implicit Zeroes (not used)",
    "text": "4.6 Implicit Zeroes (not used)\nNic suggested that I check on the cokeys where fragvol_r_sum is NA, it’s possible that these are actually implied zeroes. The main place we’ll use this info is for calculating SOC stocks (didn’t end up doing this for the manuscript).\n\nfrag_check &lt;- horiz %&gt;% \n  select(chkey,\n         cokey,\n         fragvol_r_sum) %&gt;% \n  left_join(comp_info, by = c('cokey')) %&gt;% \n  filter(is.na(fragvol_r_sum))\n\nSo out of 37552 chkeys, the sum of coarse fragment volume (fragvol_r_sum) is NA for 7414 of them.\n\n4.6.1 Geomorphic Description\nHere I’m trying to pull out big groups of similar soils based on geomorphic description, which is helpful as a starting point for whether we would expect there to be coarse fragments.\nLook into using the fetchOSD function http://ncss-tech.github.io/AQP/soilDB/soil-series-query-functions.html\nSoil Survey Manual online, helpful info about how this info is recorded: https://www.nrcs.usda.gov/wps/portal/nrcs/detail/soils/ref/?c id=nrcs142p2_054252\n\n# how many are on lake plains? unlikely to have \n# many coarse frags here \nlake &lt;- frag_check %&gt;% \n  filter(str_detect(geomdesc, \"lake plain\"))\n\n# how many cokeys?\nnrow(lake)\n\n[1] 2818\n\n# a few examples\nhead(unique(lake$geomdesc))\n\n[1] \"depressions on lake plains\"                            \n[2] \"flats on lake plains\"                                  \n[3] \"till-floored lake plains on lake plains\"               \n[4] \"rises on lake plains\"                                  \n[5] \"depressions on till-floored lake plains on lake plains\"\n[6] \"lake plains\"                                           \n\nlake %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAlbolls\nGalchutt, Barbert\n\n\nAqualfs\nSpooner, Brickton\n\n\nAquents\nCormant, Grygla, Lallie, Aquents, Urness\n\n\nAquepts\nHaug, Deerwood, Wildwood, Northwood, Leafriver, Sago, Sax, Hamre\n\n\nAquerts\nFargo, Viking, Grano, Northcote, Hegne, Eaglepoint, Reis, Clearwater, Ludden, Beauford\n\n\nAquolls\nThiefriver, Garborg, Colvin, Rosewood, Ulen, Augsburg, Borup, Wheatville, Glyndon, Bearden, Noyes, Lamoure, Espelie, Wabanica, Woodslake, Boash, Mustinka, Hamar, Grimstad, Nielsville, Venlo, Arveson, Lindaas, Perella, Aquolls, Gunclub, Vallers, Antler, Flom, Kratka, Syrene, Wyndmere, Elmville, Winger, Parnell, Kittson, Bigstone, Quam, Spicer, Marna, Brownton, Waldorf, Madelia, Lura, Leen, Chetomba, Prinsburg, Nishna, Baroda\n\n\nHemists\nTacoosh, Mooselake, Uskabwanka, Rifle\n\n\nOrthents\nUdorthents, Orthents, Bold\n\n\nPsamments\nCorliss, Poppleton, Zimmerman, Barber, Guida\n\n\nSaprists\nMarkey, Cathro, Berner, Dora, Bullwinkle, Tawas, Seelyeville, Rondeau, Histosols, Haslie, Klossner\n\n\nUdalfs\nSkime, Moranville, Dalbo, Baudette, Debs, Rosy, Kilkenny, Shorewood\n\n\nUderts\nSinai, Hattie\n\n\nUdolls\nHecla, Huot, Foldahl, Croke, Hilaire, Glyndon, Zell, LaDelle, Flaming, Eckman, Overly, Bygland, Lizzie, Swenoda, Kittson, Wheatville, McIntosh, Wolverton, Doran, Towner, Lohnes, Gardena, Egeland, Byrne, Tara, Rondell, Shorewood, Truman, Gardencity, Collinwood, Kingston, Seaforth, Waubay, Rusklyn, Poinsett, Barrington, Ocheyedan, Lakefield, Good Thunder, Corwith, Grogan\n\n\n\n\n\n\n\n\n\n# how many are on flood plains? \nflood &lt;- frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(str_detect(geomdesc, \"flood plain\"))\n\n# how many cokeys?\nnrow(flood)\n\n[1] 684\n\n# a few examples\nhead(unique(flood$geomdesc))\n\n[1] \"natural levees on flood plains\"  \"flood plains\"                   \n[3] \"terraces on flood plains\"        \"flood plains on river valleys\"  \n[5] \"flood plains on alluvial plains\" \"flats on flood plains\"          \n\nflood  %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAquents\nLallie, Totagatic, Blue Earth, Chaska, Kalmarville\n\n\nAquerts\nLudden\n\n\nAquolls\nLamoure, Rauville, Cohoctah, Sedgeville, Suckercreek, Calco, Havelock, Nishna, Cedarrock, Faxon, Colo, Comfrey, Otter, Millington, Alluvial land, Southbrook, Riverston, Coland, Sawmill\n\n\nFluvents\nCashel, Fairdale, Alluvial land, Dorchester, McPaul, Dunnbot, Rawles, Dockery\n\n\nHemists\nLougee, Boots\n\n\nPsamments\nAlgansee, Scotah\n\n\nSaprists\nBowstring, Haslie, Seelyeville, Nidaros, Cathro, Muskego, Klossner, Houghton, Rondeau\n\n\nUderts\nWahpeton, Sinai\n\n\nUdolls\nLa Prairie, Hanlon, Lawson, Littleton, Kennebec, Ceresco, Alluvial land\n\n\nNA\nMuck and peat\n\n\n\n\n\n\n\n\n\n# how many are on sandhills? \nsandhill &lt;- frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(str_detect(geomdesc, \"sandhill\"))\n\n# how many cokeys?\nnrow(sandhill)\n\n[1] 20\n\n# a few examples\nhead(unique(sandhill$geomdesc))\n\n[1] \"sand sheets on sandhills\" \"dunes on sandhills\"      \n[3] \"depressions on sandhills\"\n\nsandhill %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAquents\nBantry\n\n\nHemists\nRifle\n\n\nPsamments\nSerden, Aylmer\n\n\n\n\n\n\n\n\n\n# how many are organic soils? \nhist &lt;- frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) \n\n# how many cokeys?\nnrow(hist)\n\n[1] 803\n\n# some examples\nhead(unique(hist$geomdesc))\n\n[1] \"-- Error in Exists On --\"                              \n[2] \"depressions on outwash plains\"                         \n[3] \"fens on beach ridges\"                                  \n[4] \"depressions on till plains\"                            \n[5] \"seeps on till plains\"                                  \n[6] \"depressions on moraines, depressions on outwash plains\"\n\nhist %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nFibrists\nBrophy\n\n\nHemists\nRifle, Mooselake, Greenwood, Tacoosh, Uskabwanka, Lougee, Carlos, Millerville, Caron\n\n\nSaprists\nSeelyeville, Markey, Lupton, Berner, Haslie, Cathro, Bullwinkle, Rondeau, Nidaros, Loxley, Histosols, Bowstring, Muck and peat, Muck, Muskego, Houghton, Klossner, Haplosaprists, Palms, Lena, Medo, Marsh, Muck soil\n\n\n\n\n\n\n\n\n\n# outwash plains?\noutwash &lt;- frag_check %&gt;% \n   filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(str_detect(geomdesc, \"outwash plain\"))\n\n# how many cokeys?\nnrow(outwash)\n\n[1] 470\n\n# some examples \nhead(unique(outwash$geomdesc))\n\n[1] \"depressions on outwash plains\"                       \n[2] \"lakeshores on outwash plains\"                        \n[3] \"flats on outwash plains\"                             \n[4] \"hillslopes on moraines, hillslopes on outwash plains\"\n[5] \"outwash plains\"                                      \n[6] \"hillslopes on outwash plains\"                        \n\noutwash %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAqualfs\nEpoufette\n\n\nAquents\nUrness\n\n\nAquepts\nLeafriver, Madaus, Minocqua, Wabuse\n\n\nAquolls\nIsanti, Warman, Colvin, Hamar, Rockwell, Fossum, Shakopee, Fieldon, Granby, Darfur, Trosky, Talcot\n\n\nBoralfs\nSoderville\n\n\nPsamments\nPsamments, Sartell, Zimmerman, Cantlin, Chelsea\n\n\nUdalfs\nAnoka, Dalbo, Soderville, Menomin\n\n\nUdolls\nEstelline, Maddock, Malachy, Flandreau, Litchfield, Langola, Kost, Glendorado, Athelwold, Embden, Torning, Dickinson, Gardencity, Tomall, Grogan, Kennebec, Waukegan, Strayhoss, Everts, Brandt, Sandberg, Allendorf, Lasa\n\n\nUstolls\nDempster, Flandreau, Graceville\n\n\n\n\n\n\n\n\n\n# stream terraces? \nterrace &lt;- frag_check %&gt;% \n   filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(!str_detect(geomdesc, \"outwash plain\")) %&gt;% \n  filter(str_detect(geomdesc, \"stream terrace\")) \n\n# how many cokeys?\nnrow(terrace)\n\n[1] 135\n\n# some examples\nhead(unique(terrace$geomdesc))\n\n[1] \"natural levees on alluvial plains, stream terraces on alluvial plains\"\n[2] \"stream terraces on river valleys\"                                     \n[3] \"stream terraces\"                                                      \n[4] \"hills on stream terraces\"                                             \n[5] \"escarpments on stream terraces\"                                       \n[6] \"swales, depressions on stream terraces\"                               \n\nterrace %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAquolls\nTalcot, Duelm, Faxon, Isanti, Tilfer, Joliet\n\n\nOrthents\nUdorthents\n\n\nPsamments\nSartell\n\n\nUdalfs\nSoderville, Anoka, Meridian, Eleva\n\n\nUderts\nWahpeton\n\n\nUdolls\nLaDelle, Hubbard, Langola, Dorset, Richwood, Copaston, Lasa, Dickinson, Ridgeport, Grogan, Allendorf\n\n\n\n\n\n\n\n\n\n# loess hills? \nloess &lt;- frag_check %&gt;%\n    filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(!str_detect(geomdesc, \"outwash plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"stream terrace\")) %&gt;% \n  filter(str_detect(geomdesc, \"loess\"))\n\nnrow(loess)  \n\n[1] 260\n\n# some examples\nhead(unique(loess$geomdesc))\n\n[1] \"loess hills\"                  \"knolls on loess hills\"       \n[3] \"valley sides on loess bluffs\" \"valley sides on loess hills\" \n[5] \"loess bluffs\"                \n\nloess %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nPsamments\nBoone\n\n\nUdalfs\nWhalan, Oak Center, Gale, Eleva, Hersey, Fayette, Downs, Massbach, Brinkman, Frankville, Nasset, Mt. Carroll\n\n\nUdolls\nJoy, Dinsmore, Tama, Shullsburg, Schapville, Port Byron, Elizabeth\n\n\n\n\n\n\n\n\n\n# drainageways? \ndrain &lt;- frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(!str_detect(geomdesc, \"outwash plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"stream terrace\")) %&gt;% \n  filter(!str_detect(geomdesc, \"loess\")) %&gt;% \n  filter(str_detect(geomdesc, \"drain\"))\n\n# how many cokeys?\nnrow(drain)\n\n[1] 131\n\n# some examples \nhead(unique(drain$geomdesc)) \n\n[1] \"drainageways on river valleys\"              \n[2] \"drainageways on moraines, flats on moraines\"\n[3] \"drainageways on till plains\"                \n[4] \"drainageways on terraces\"                   \n[5] \"flats on moraines, drainageways on moraines\"\n[6] \"drainageways\"                               \n\ndrain %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAqualfs\nBlomford\n\n\nAquents\nFluvaquents\n\n\nAquolls\nFaxon, Fulda, Parnell, Maxfield, Colo, Ossian, Otter, Badger, Hidewood, Little Cottonwood\n\n\nUdolls\nLaDelle, Zell, Joy, Barremills\n\n\n\n\n\n\n\n\n\nother &lt;-  frag_check %&gt;% \n  filter(!str_detect(geomdesc, \"lake plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"flood plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"sandhill\")) %&gt;% \n  filter(!taxsuborder %in% c(\"Saprists\", \"Hemists\", \"Fibrists\")) %&gt;% \n  filter(!str_detect(geomdesc, \"outwash plain\")) %&gt;% \n  filter(!str_detect(geomdesc, \"stream terrace\")) %&gt;% \n  filter(!str_detect(geomdesc, \"loess\")) %&gt;% \n  filter(!str_detect(geomdesc, \"drain\"))\n\n# other terrace related \nother %&gt;% filter(str_detect(geomdesc, \"terrace\")) %&gt;% pull(geomdesc) %&gt;% unique()\n\n [1] \"terraces\"                               \n [2] \"terraces, hills\"                        \n [3] \"hills, terraces\"                        \n [4] \"escarpments on terraces, hills\"         \n [5] \"escarpments on terraces\"                \n [6] \"outwash terraces\"                       \n [7] \"strath terraces\"                        \n [8] \"terraces on river valleys\"              \n [9] \"outwash terraces on till plains\"        \n[10] \"hills on terraces\"                      \n[11] \"escarpments, terraces\"                  \n[12] \"swales on outwash terraces, till plains\"\n[13] \"terraces on uplands\"                    \n\n# drumlin or moraine related\nother %&gt;% filter(str_detect(geomdesc, \"drumlin\")|\n                   str_detect(geomdesc, \"moraine\")) %&gt;% pull(geomdesc) %&gt;% unique()\n\n [1] \"flats on moraines, rises on moraines\"               \n [2] \"rises on moraines\"                                  \n [3] \"depressions on moraines\"                            \n [4] \"hillslopes on moraines, rises on moraines\"          \n [5] \"moraines on till plains\"                            \n [6] \"moraines\"                                           \n [7] \"flats on moraines\"                                  \n [8] \"swales on moraines\"                                 \n [9] \"hillslopes on moraines\"                             \n[10] \"flats on moraines, swales on moraines\"              \n[11] \"hillslopes on glacial lakes on moraines\"            \n[12] \"depressions on interdrumlins\"                       \n[13] \"hillslopes on drumlins\"                             \n[14] \"glacial lakes on hillslopes on moraines\"            \n[15] \"rims on depressions on moraines, flats on moraines\" \n[16] \"rises on moraines, flats on moraines\"               \n[17] \"swales on moraines, flats on moraines\"              \n[18] \"drumlins\"                                           \n[19] \"flats, moraines\"                                    \n[20] \"hills on moraines\"                                  \n[21] \"beaches on moraines\"                                \n[22] \"shores on beaches on moraines\"                      \n[23] \"marshes on moraines\"                                \n[24] \"rises on ground moraines\"                           \n[25] \"ground moraines\"                                    \n[26] \"flats on ground moraines, swales on ground moraines\"\n[27] \"ground moraines on till plains\"                     \n\n# hill related (some of these don't really make sense to me)\nother %&gt;% filter(str_detect(geomdesc, \"hill\")) %&gt;% pull(geomdesc) %&gt;% unique()\n\n [1] \"hillslopes on till plains, ridges on till plains\"\n [2] \"hillslopes on moraines, rises on moraines\"       \n [3] \"hillslopes on moraines\"                          \n [4] \"hillslopes on glacial lakes on moraines\"         \n [5] \"hillslopes on till plains\"                       \n [6] \"hillslopes on drumlins\"                          \n [7] \"glacial lakes on hillslopes on moraines\"         \n [8] \"hills on moraines\"                               \n [9] \"hills on till plains\"                            \n[10] \"terraces, hills\"                                 \n[11] \"hills, terraces\"                                 \n[12] \"escarpments on terraces, hills\"                  \n[13] \"hills\"                                           \n[14] \"hillslopes on uplands\"                           \n[15] \"hills on terraces\"                               \n[16] \"hills, valley sides\"                             \n[17] \"hills on outwash deltas\"                         \n[18] \"hillslopes on uplands, hillslopes on uplands\"    \n[19] \"hillslopes on interfluves on uplands\"            \n[20] \"swales on hillslopes on uplands\"                 \n\n# note that 'other' now contains only 208 unique compnames\nlength(unique(other$compname))\n\n[1] 208\n\nother %&gt;% select(compname, taxsuborder) %&gt;% \n  distinct() %&gt;% \ndplyr::group_by(taxsuborder) %&gt;% \n  dplyr::summarise(comps = str_c(compname, collapse = \", \")) %&gt;% \n  gt()\n\n\n\n\n\n\n\n\ntaxsuborder\ncomps\n\n\n\n\nAlbolls\nBarbert\n\n\nAqualfs\nWillosippi, Zwingle\n\n\nAquents\nUrness, Blue Earth, Tadkee, Aquents, Beaches, Beach\n\n\nAquepts\nHamre, Haug, Twig, Giese\n\n\nAquerts\nHegne\n\n\nAquolls\nFram, Roliss, Borup, Grimstad, Lindaas, Dovray, Colvin, Quam, Winger, Bearden, Glencoe, Darfur, Parnell, Bigstone, Lura, Okoboji, Granby, Glyndon, Perella, Spicer, Southam, Fulda, Calcousta, Maxcreek, Garwin, Maxfield, Tilfer, Soils that have more clay, Romnell, Trosky, Knoke, Faxon, Joliet, Fieldon, McIntosh, Rushmore, Whitewood, Mound Creek, Mayer, Prinsburg, Chetomba, Biscay, Haverhill, Marcus\n\n\nFluvents\nCashel, Arenzville, Dorchester\n\n\nOrthents\nUdorthents, Bold\n\n\nPsamments\nMahtomedi, Boone, Chelsea\n\n\nUdalfs\nWarba, Dalbo, Brainerd, Jewett, Dorerton, Blooming, Mt. Carroll, Winneshiek, Seaton, Hersey, Downs, Eleva, Oak Center, Meridian, Waucoma, Nasset, Waubeek, Massbach, Frankville, Gale, Dubuque, Fayette, Hixton, Medary, Festina, Plumcreek, Whalan, Renova, Newry, Backbone, Spinks, Churchtown, NewGlarus, Southridge, Palsgrove, Norden, Dunbarton, Donnan, Pepin, Nordness, Lamont, La Farge\n\n\nUdepts\nTimula\n\n\nUderts\nSinai, Nutley\n\n\nUdolls\nTowner, Maddock, Lohnes, Bygland, Dickey, Tara, Langola, Kost, Copaston, Doran, Byrne, Yellowbank, Doland, Embden, Waubay, Poinsett, Swenoda, Brodale, Ripon, Sparta, Torning, Fordville, Fedji, Tomall, Highpoint Lake, Darnen, Bechyn, Crooksford, Louris, Port Byron, Rockton, Lindstrom, Tallula, Marlean, Klinger, Joy, Kennebec, Merton, Bellechester, Grogan, Wadena, Dickinson, Channahon, Tama, Atkinson, Wagen Prairie, Wangs, Hesch, Richwood, Moland, Schapville, Rasset, Brookings, Vienna, Lismore, Kranzburg, Singsaas, Oak Lake, Lake Benton, Round Lake, Ridgeport, Dodgeville, Etter, Lasa, Litchfield, Sac, Everly, Ransom, Athelwold, Judson, Overly, Germantown, Augusta Lake, Okabena, Ocheyedan, Pilot Grove, Sylvester, Emeline, Hoopeston, Lawler, Flagler, Keltner, Waukee, Wilmonton, Ocheda, Lakefield, Fostoria, Farrar, Edmund\n\n\nUstolls\nIhlen, Alcester, Moody, Splitrock, Trent, Sogn, Bluemound\n\n\nNA\nRock outcrop",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Horizon Data Averages</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html",
    "href": "06-soil-prop-eda.html",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "",
    "text": "5.1 Soil Texture Classes",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#clay-sand-and-silt",
    "href": "06-soil-prop-eda.html#clay-sand-and-silt",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.2 Clay, Sand, and Silt",
    "text": "5.2 Clay, Sand, and Silt\n\nplot_facet_hist(c(\"claytotal_r\", \"sandtotal_r\", \"silttotal_r\")) +\n  xlab(\"Weight %\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#carbonates",
    "href": "06-soil-prop-eda.html#carbonates",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.3 Carbonates",
    "text": "5.3 Carbonates\n\nplot_facet_hist(c(\"claysizedcarb_r\", \"caco3_r\")) +\n  xlab(\"Weight Percent\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#sec-ph-ec",
    "href": "06-soil-prop-eda.html#sec-ph-ec",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.4 pH and EC",
    "text": "5.4 pH and EC\n\nplot_facet_hist(c(\"ph1to1h2o_r\", \n                  \"ec_r\"), \n                nbins = 20) + \n  xlab(\"dS/m  ~~~~  unitless\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#organic-matter",
    "href": "06-soil-prop-eda.html#organic-matter",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.5 Organic Matter",
    "text": "5.5 Organic Matter\n\nplot_facet_hist(c(\"om_r\")) +\n  xlab(\"% LOI\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#bulk-density",
    "href": "06-soil-prop-eda.html#bulk-density",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.6 Bulk Density",
    "text": "5.6 Bulk Density\n\nplot_facet_hist(c(\"dbthirdbar_r\")) +\n  xlab(\"g/cm3\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#cec-ph-7",
    "href": "06-soil-prop-eda.html#cec-ph-7",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.7 CEC pH 7",
    "text": "5.7 CEC pH 7\n\nplot_facet_hist(c(\"cec7_r\")) +\n  xlab(\"meq/100g\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#ksat-and-awc",
    "href": "06-soil-prop-eda.html#ksat-and-awc",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.8 Ksat and AWC",
    "text": "5.8 Ksat and AWC\nIn Devine et al., they calculated AWC as a sum by soil component (so they multiplied the weighted average AWC by the depth to get a volume of water). Nic and I talked about this, and decided it’s not necessary. We can just use the AWC value from the database (the volume fraction), because we are always using the same depth (20cm). So multiplying by a constant wouldn’t change the distribution of values at all.\nAWC column description: The amount of water that an increment of soil depth, inclusive of fragments, can store that is available to plants. AWC is expressed as a volume fraction, and is commonly estimated as the difference between the water contents at 1/10 or 1/3 bar (field capacity) and 15 bars (permanent wilting point) tension and adjusted for salinity, and fragments.\n\nplot_facet_hist(c(\"ksat_r\", \n                  \"awc_r\")) + \n  xlab(\"um/s  ~~~~  cm/cm\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#linear-extensibility-and-sar",
    "href": "06-soil-prop-eda.html#linear-extensibility-and-sar",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.9 Linear Extensibility and SAR",
    "text": "5.9 Linear Extensibility and SAR\n\nplot_facet_hist(c(\"lep_r\", \n                  \"sar_r\")) + \n  xlab(\"%  ~~~~  unitless\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#volume-coarse-fragments",
    "href": "06-soil-prop-eda.html#volume-coarse-fragments",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.10 Volume Coarse Fragments",
    "text": "5.10 Volume Coarse Fragments\n\nplot_facet_hist(c(\"fragvol_r_sum\")) +\n  xlab(\"Volume %\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "06-soil-prop-eda.html#erodibility-factor",
    "href": "06-soil-prop-eda.html#erodibility-factor",
    "title": "5  Soil Property Exploratory Analysis",
    "section": "5.11 Erodibility Factor",
    "text": "5.11 Erodibility Factor\n\nplot_facet_hist(c(\"kwfact\"),\n                nbins = 20) +\n  xlab(\"Unitless\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soil Property Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html",
    "href": "07-map-unit-agg.html",
    "title": "6  Map unit aggregation",
    "section": "",
    "text": "6.1 Clustering Variables\nUsing the same variables they did in Devine et al., but added calcium carbonate (caco3_r_value)\nclust_vars &lt;- c(\n  \"claytotal_r_value\",\n  \"om_r_value\",\n  \"cec7_r_value\",\n  \"dbthirdbar_r_value\",\n  \"ec_r_value\",\n  \"ph1to1h2o_r_value\",\n  \"caco3_r_value\",\n  \"lep_r_value\",\n  \"ksat_r_value\",\n  \"awc_r_value\"\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html#drop-incomplete-cases",
    "href": "07-map-unit-agg.html#drop-incomplete-cases",
    "title": "6  Map unit aggregation",
    "section": "6.2 Drop incomplete cases",
    "text": "6.2 Drop incomplete cases\nI think we want to keep only complete cases for our COKEYs (all of the variables we want to cluster on should have a value). Otherwise that would mess with our clustering. Here I’m checking on how many NAs there are for the different variables, then I drop the incomplete cases.\n\ncmp %&gt;% \n  select(cokey, all_of(clust_vars)) %&gt;% \n  summarise(across(where(is.numeric), ~sum(is.na(.x)))) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"variable\",\n               values_to = \"n_missing\") %&gt;% \n  arrange(desc(n_missing))\n\n\n  \n\n\n\nAnd keep only the complete cases:\n\n# keep only cokey and clustering vars\ncmp_clust &lt;- cmp %&gt;% \n  select(cokey, all_of(clust_vars))\n\n# filter to include only complete cases \n# there are many ways to do this\n# another way would be filter(complete.cases(.))\ncmp_complete &lt;- cmp_clust %&gt;% drop_na()\n\nSo we went from 10237 to 9387 COKEYs. (We dropped 850).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html#create-table-for-map-unit-data",
    "href": "07-map-unit-agg.html#create-table-for-map-unit-data",
    "title": "6  Map unit aggregation",
    "section": "6.3 Create table for map unit data",
    "text": "6.3 Create table for map unit data\nHere, I take the (complete only) component data (0-20cm weighted averages for all our soil properties) and join it to my lookup table, which includes MUKEY. Then I nest COKEY data (multiple COKEYs can belong to one MUKEY). The result is a table with one row for each MUKEY.\nI also pull out some info about the number, min %, and max % of components in a given MUKEY for context.\nThe first time I did this, I forgot to deal with NAs in the cmp dataframe, and we had 7525 MUKEYs to work with. After filtering to include only complete cases based on the cluster variables defined above, we now have 7062 MUKEYs to work with. For reference, Devine et al. had 4,595 for inclusion in their model (after an additional drop step based on sufficient data representation, see below Section 6.5) .\n\n# nest by mukey\ncmp_nest &lt;- left_join(cmp_complete, cmp_lookup, by = c(\"cokey\")) %&gt;%\n  dplyr::select(cokey, mukey, everything()) %&gt;%\n  dplyr::group_by(mukey) %&gt;%\n  nest() %&gt;%\n  dplyr::mutate(\n    n_comp = map_dbl(data, nrow),\n    # max_comp_pct = map_dbl(data,\n    #                        ~ max(.[\"comppct_r\"])),\n    # min_comp_pct = map_dbl(data, ~ min(.[\"comppct_r\"])),\n    cokeys = map(data, ~pull(.data = ., var = cokey))\n  )",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html#calculate-map-unit-weighted-means",
    "href": "07-map-unit-agg.html#calculate-map-unit-weighted-means",
    "title": "6  Map unit aggregation",
    "section": "6.4 Calculate map unit weighted means",
    "text": "6.4 Calculate map unit weighted means\nI use the custom function below to calculate the component percent-weighted means for all soil properties and MUKEYs. Recall that I have the representative component percent, comppct_r in the list-column “data” within the cmp_nest dataframe. This came from the cmp_lookup table I loaded at the beginning.\nConveniently, we don’t need to do any extra work to normalize the COKEY weights. The default behavior of the weighted.mean function is to take the numerical vector of weights, w, and normalize it to sum to one. So we can simply supply comppct_r for the weights.\n\n# decided that I don't need to add \"na.rm\" argument \n# to the weighted.mean function here, because I've already removed all the NAs up above\ncalc_mu_wtd &lt;- function(df, myvars){\n\n  wts &lt;- df %&gt;% pull(comppct_r)\n  \n  df_mu_wtd &lt;- df %&gt;%\n    summarise(across(.cols = all_of(myvars), \n                   .fns = ~weighted.mean(.x, w = wts)\n                   ))\n  \n  return(df_mu_wtd)\n\n}\n\n\ndf_mu_sum &lt;- cmp_nest %&gt;% \n  mutate(mu_sum_data = map(.x = data, \n                           .f = calc_mu_wtd,\n                           myvars = clust_vars))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html#sec-dat-avail",
    "href": "07-map-unit-agg.html#sec-dat-avail",
    "title": "6  Map unit aggregation",
    "section": "6.5 Determine data availability",
    "text": "6.5 Determine data availability\nAnother thing to think about: what is our threshold for having “enough” data to appropriately represent a given MUKEY? For example, if we have a MUKEY represented by 1 component, but that component has a relatively small representative %? Like a comppct_r of &lt;50%? &lt;30%? Would we still consider that “representative” of that particular MUKEY?\nWhat Devine et al. did was use the following logic:\n\nData is available for at least 80% of the mapunit components OR\nData availability at least equal to the dominant component percentage (recall that this info is stored in the dom_cmp_key object for our analysis).\n\nFirst, a little context about the number of components in our remaining MUKEYs:\n\n# for context, tabulate mukeys by num comps\nncomp_counts &lt;- cmp_nest %&gt;% \n  group_by(n_comp) %&gt;% \n  count(name = \"n_mukeys\")\n\nhead(ncomp_counts)\n\n\n  \n\n\n\nNow, calculate the data availability for each MUKEY. Recall that since we dropped incomplete cases at the COKEY level above, we can get at data availability by summing the remaining comppct_r for each MUKEY.\nThe object dom_cmp_key I join in here was created in back in Chapter 3 . It specifies the comppct_r for the dominant component in a given map unit. It was important to pull this number early in our data aggregation process, before dropping any components due to missing data.\n\ndata_avail &lt;- df_mu_sum %&gt;%\n  mutate(avail_data_perc = map_dbl(data,  ~ sum(.[\"comppct_r\"]))) %&gt;%\n  select(mukey, cokeys, avail_data_perc) %&gt;% \n  left_join(., dom_cmp_key, by = \"mukey\")\n\n\nhead(data_avail)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html#exclude-based-on-data-availability",
    "href": "07-map-unit-agg.html#exclude-based-on-data-availability",
    "title": "6  Map unit aggregation",
    "section": "6.6 Exclude based on data availability",
    "text": "6.6 Exclude based on data availability\n\n# populate a column specifying whether \n# to include a given mukey.\n# the order of the conditions is important\n# must go from most specific to most general\n# see https://dplyr.tidyverse.org/reference/case_when.html\nexclude_key &lt;- data_avail %&gt;% \n  mutate(include = case_when(\n    avail_data_perc &gt;= 80 ~ \"yes\", # cond. 2 (n=5874)\n    avail_data_perc &gt;= dom_comppct ~ \"yes\", # cond. 1 (n=6912)\n    TRUE ~ \"no\" # doesn't meet either condition\n  )) %&gt;% \n  select(-cokeys)\n\n# add the info about include/excl to main dataset\ndf_mu_avail &lt;- full_join(exclude_key, df_mu_sum, by = \"mukey\")\n\n# keep only \"include\" mukeys\ndf_mu_incl &lt;- df_mu_avail %&gt;% \n  filter(include == \"yes\")\n\nAfter applying the criteria above for data availability, we went from 7062 to 6912 MUKEYs, a difference of 150.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html#save-results",
    "href": "07-map-unit-agg.html#save-results",
    "title": "6  Map unit aggregation",
    "section": "6.7 Save Results",
    "text": "6.7 Save Results\n\n# the data we want, summarised at the MUKEY level, \n# is in a list-column. That's why I use unnest here \n# before saving the results. \nmu_unnest &lt;- df_mu_incl %&gt;%\n  select(mukey,\n         mu_sum_data) %&gt;%\n  unnest(mu_sum_data)\n\nwrite_csv(mu_unnest, \"./data/mu_weighted_soil_props.csv\")\n\nAlso want to save a key that relates my included cokeys with their mukey. This will be a slightly different list than the component_list.csv because we dropped COKEYs that had missing data in any of the clustering variables, and we dropped MUKEYs that didn’t have sufficient data.\n\ncmp_list_incl &lt;- df_mu_incl %&gt;% \n  select(mukey, cokeys) %&gt;% \n  unnest(cokeys) %&gt;% \n  rename(cokey = cokeys)\n\nwrite_csv(cmp_list_incl, \"data/key_cokey_mukey_complete_cases_include.csv\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html#more-about-mukeys-we-are-excluding",
    "href": "07-map-unit-agg.html#more-about-mukeys-we-are-excluding",
    "title": "6  Map unit aggregation",
    "section": "6.8 More about MUKEYs we are excluding",
    "text": "6.8 More about MUKEYs we are excluding\nLearn a little more about what we are excluding:\n\n# how many MUKEYs are we excluding?\nexclude_key %&gt;% \n  group_by(include) %&gt;% \n  count(name = \"num_mukeys\")\n\n\n  \n\n\n# grab only mukeys we are excluding, join in more info\nmukeys_drop &lt;- exclude_key %&gt;% \n  filter(include == \"no\") \n\nmukeys_drop_details &lt;- mukeys_drop %&gt;% \n  left_join(., mukey_info, by = \"mukey\")\n\n# most of these are not prime farmland\nmukeys_drop_details %&gt;% \n  group_by(farmlndcl) %&gt;% \n  count()\n\n\n  \n\n\n# types of mus \nmukeys_drop_details %&gt;% \n  group_by(mukind) %&gt;% \n  count()\n\n\n  \n\n\n# how many acres are represented? \ndrop_acres &lt;- sum(mukeys_drop_details$muacres)\ndrop_acres\n\n[1] 512769\n\n# of the 150 MUKEYs, how many are urban?\nurban &lt;- mukeys_drop_details %&gt;% \n  filter(str_detect(muname, \"Urban\"))\n\nnrow(urban)\n\n[1] 54\n\n# and what percentage of the dropped acres are urban? \nround((sum(urban$muacres) / drop_acres) * 100, digits = 1)\n\n[1] 29.1\n\n# dropping urban stuff (don't care about it)\n# want to look more at the non-urban MUs\n\nmunames &lt;- mukeys_drop_details %&gt;% \n  filter(!str_detect(muname, \"Urban\")) %&gt;% \n  pull(muname)\n\n# splitting at the comma so I can drop slope info\nmuname_stripped &lt;- as.data.frame(str_split_fixed(munames, \",\", 2)) %&gt;% select(V1)\n\n# once we strip out the slope info\n# only 35 unique, non-urban munames\ndistinct(muname_stripped) %&gt;% \n  rename(abbrev_muname = V1) %&gt;% \n  arrange(abbrev_muname)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "07-map-unit-agg.html#sec-dom-eda",
    "href": "07-map-unit-agg.html#sec-dom-eda",
    "title": "6  Map unit aggregation",
    "section": "6.9 (old) Explore dominant component percentages",
    "text": "6.9 (old) Explore dominant component percentages\nKeeping this here as an illustration/explanation, but the dominant component percentages we actually want to use come from Chapter 3 . This is some troubleshooting and exploration I did on the road to figuring that out.\n\n# this adds MUKEY to our df \ncmp_mukey_detail &lt;- left_join(cmp, cmp_lookup, by = c(\"cokey\"))\n\nold_dom_cmp_key &lt;- cmp_mukey_detail %&gt;% \n  select(cokey, mukey, comppct_r) %&gt;% \n  group_by(mukey) %&gt;% \n  summarise(dom_comppct = max(comppct_r))\n\nhead(old_dom_cmp_key)\n\n\n  \n\n\n# again, these % look weird, and have some \n# very low %, because of the issues described below\nold_dom_cmp_key %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = dom_comppct)) +\n  ggtitle(\"Distribution of dominant component %\") + \n  xlab(\"Dominant component %\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThoughts after troubleshooting this. The dominant component percentages shouldn’t be calculated here, they should be done earlier, in Chapter 3, before we promote to SPC object. Below is an example with two MUKEYs to illustrate.\nBoth of these MUKEYs have large areas in components that don’t have data in the database (“Pits” and “Urban Land”). This makes sense. These are the types of components that are dropped when we promote our chorizon data to an aqp object, see Section 4.4.4 . There is data available for lower % components “Dassel” and “Udipsamment” here, but those aren’t the true dominant components, they are simply the only components that made it through the filtering that happened when I created my aqp object.\nSo if I want to use the dominant component percentage as a criterion in including/excluding data, I need to grab it earlier.\nMore broadly, if these areas are primarily urban land or pits, they aren’t really the type of thing we’d want to include in this primarily agricultural analysis anyway. So it’s good that we have a way to filter them out.\n\n# as an example\nold_dom_cmp_key %&gt;% \n  filter(dom_comppct &lt; 25) %&gt;% \n  left_join(., cmp_lookup, by = \"mukey\") %&gt;% \n  filter(mukey %in% c(394766, 428184))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Map unit aggregation</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html",
    "href": "08-var-transformations.html",
    "title": "7  Variable Transformations",
    "section": "",
    "text": "7.1 Overview\nSo it makes sense to me that we would want to apply transformations on our variables with skewed distributions, but it has been surprisingly hard to find more info about this in the textbooks and tutorials I’ve found on k-means. Because we are going to standardize / scale the data by subtracting the mean and dividing by 1 standard deviation, I think it makes sense that we’d want something close to normal for the starting distribution (since the mean is not really a helpful summary statistic for data that is very skewed…).\nI made plots to review the best transformations for normalizing the soil property data I will be including in my k-means clustering. Here, I summarize my decisions based on the plots below:\nlibrary(tidyverse)\nlibrary(glue)\n\nm &lt;- read_csv(\"data/mu_weighted_soil_props.csv\")\n\nhead(m)\n\n\n  \n\n\n# relates mukeys to cokey(s), only complete cases\n# that have sufficient data availability (\"include\")\n# from 07-map-unit-agg.qmd\n\ncmp_lookup &lt;- read_csv(\"data/key_cokey_mukey_complete_cases_include.csv\")\n\ncmp_details &lt;- read_csv(\"data/component_list.csv\")\n\ncmp_slab &lt;- read_csv(\"data/wide_slab_aggregated_soil_props_20220912.csv\")\n\nmukey_details &lt;- read_csv(\"./data/target_mapunit_table.csv\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#overview",
    "href": "08-var-transformations.html#overview",
    "title": "7  Variable Transformations",
    "section": "",
    "text": "Clay: square root\nOrganic matter: log10 or ln\nCEC: log10 or ln\nBulk density: cubed (dB ^ 3)\nEC: None\npH: None\nCarbonates: square root\nLEP: ln OR log10 (note that since we are setting LEP=0 values to 0.5, we don’t have to worry about adding an offset here before doing the log transformation).\nKsat: log10 or ln\nAWC: log10 or ln",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#zero-values",
    "href": "08-var-transformations.html#zero-values",
    "title": "7  Variable Transformations",
    "section": "7.2 Zero values",
    "text": "7.2 Zero values\nA key consideration for these transformations is whether we have zero values, and whether to set very low values to zero.\nIn cases where we have lots of zero values (carbonates would be a good example), think about whether we need to add a constant before applying a transformation in order to make the distribution more normal.\nLet’s count the zeroes, keeping only variables that have zero values:\nFor LEP, check textures of the samples with LEP = 0. If loamy sands or sands, this might be OK?\n\nm %&gt;% \n  summarise(across(where(is.numeric), ~sum(.x == 0))) %&gt;% \n  pivot_longer(cols = everything()) %&gt;% \n  filter(value &gt; 0)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#function-for-transformation-plots",
    "href": "08-var-transformations.html#function-for-transformation-plots",
    "title": "7  Variable Transformations",
    "section": "7.3 Function for transformation plots",
    "text": "7.3 Function for transformation plots\nThis function allows us to compare multiple transformations on the same variable in one faceted plot. The arguments that end in _adjust are options to add a constant to deal with 0 values that would otherwise give us errors / infinite values.\n\nplot_transformations &lt;- function(var,\n                                 df,\n                                 log10_adjust = 0,\n                                 ln_adjust = 0,\n                                 sqrt_adjust = 0,\n                                 nbins = 30) {\n  trans_df &lt;- df %&gt;%\n    select(mukey, {{var}}) %&gt;%\n    mutate(\n      log10_trans = log10({{var}} + log10_adjust),\n      ln_trans = log({{var}} + ln_adjust),\n      sqrt_trans = sqrt({{var}} + sqrt_adjust)) %&gt;%\n  pivot_longer(cols = -c(mukey))\n\n\n  trans_df %&gt;%\n    ggplot(aes(x = value)) +\n    geom_histogram(bins = nbins) +\n    facet_wrap(vars(name), scales = \"free\") +\n    theme_bw()\n\n}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#clay",
    "href": "08-var-transformations.html#clay",
    "title": "7  Variable Transformations",
    "section": "7.4 Clay",
    "text": "7.4 Clay\nHere, I think the square root transformation looks the best.\n\nplot_transformations(var = claytotal_r_value, df = m)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#organic-matter",
    "href": "08-var-transformations.html#organic-matter",
    "title": "7  Variable Transformations",
    "section": "7.5 Organic Matter",
    "text": "7.5 Organic Matter\nAt first I thought the square root looked good here, but if you zoom in (second plot), it has a long tail (probably histosols). I would pick either the log10 or the ln.\n\nplot_transformations(var = om_r_value, df = m)\n\n\n\n\n\n\n\nm %&gt;% \n  mutate(sqrt_om = sqrt(om_r_value)) %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = sqrt_om)) + \n  theme_bw() +\n  ggtitle(\"Note long tail when zoomed in\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#cec",
    "href": "08-var-transformations.html#cec",
    "title": "7  Variable Transformations",
    "section": "7.6 CEC",
    "text": "7.6 CEC\nWould pick log10 or ln here.\n\nplot_transformations(var = cec7_r_value, df = m)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#bulk-density",
    "href": "08-var-transformations.html#bulk-density",
    "title": "7  Variable Transformations",
    "section": "7.7 Bulk Density",
    "text": "7.7 Bulk Density\nLooking at the untransformed values, this is skewed left (others have been skewed right)\nNone of these transformations in the facet plot look great to me, I also tried squaring and cubing the values, I think cubing looks the best. However, this brings up the idea of interpretability, and what our transformations mean for interpreting the clusters. By squaring the bulk density values, I’m bringing low values closer together while spreading the higher values further apart. This deals with the skewness, but is this a reasonable thing to do from a interpretation perspective? It means that at the higher ranges of bulk density, values that are the same distance apart on the regular scale (say a 0.1 g/cm3 difference) are “more different” compared to values that are 0.1 g/cm3 different at the low end of our spectrum.\n\n# adding one here so we don't get any negative values \n# I'm not sure this is totally necessary? Think about\n# what happens when we rescale, \nplot_transformations(var = dbthirdbar_r_value,\n                     df = m,\n                     log10_adjust = 1,\n                     ln_adjust = 1,\n                     sqrt_adjust = 1)\n\n\n\n\n\n\n\n# I think this transformation looks the best\nm %&gt;% \n  mutate(cubed_db = dbthirdbar_r_value^3) %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = cubed_db)) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#sec-var-ec",
    "href": "08-var-transformations.html#sec-var-ec",
    "title": "7  Variable Transformations",
    "section": "7.8 EC",
    "text": "7.8 EC\nNo transformation for now, but need to think about if we have a threshold where we set everything below to zero. Consider setting our threshold at 1, but first look more closely at the MUKEYs with values =1, decide whether or not to include these. See Section 8.6.3 and the QGIS project in _qgis_files\nThe vast majority of our EC values are 0, approximately 89.6 . Of the values that are &gt;0, most are 1.5 dS/m or less, which Devine et al. set to zero. So it might be worth considering whether we even include EC? I looked up more info about the range of ECs that are a concern for crop production, perhaps that could inform our decision. See Section 7.8.2 .\n\nplot_transformations(var = ec_r_value,\n                     df = m,\n                     log10_adjust = 1,\n                     ln_adjust = 1,\n                     sqrt_adjust = 1)\n\n\n\n\n\n\n\n\n\n7.8.1 Explore EC values &gt;0\nLet’s take a look at the EC values that are greater than zero:\n\nm %&gt;% \n  filter(ec_r_value &gt; 0) %&gt;% \n  ggplot() + \n  geom_histogram(aes(x = ec_r_value), bins = 20) +\n  theme_bw() +\n  ggtitle(\"Distribution of EC values greater than 0\")\n\n\n\n\n\n\n\n\nWant to look more closely at the spatial distribution of these non-zero EC values, see Section 8.6.3 . To facilitate that, here I am making a simple lookup table of the MUKEYs and an “EC category” so I can make a simple map of where these non-zero values are.\n\nec_cat &lt;- m %&gt;% \n  select(mukey, ec_r_value) %&gt;% \n  mutate(ec_cat = case_when(\n    ec_r_value == 0 ~ 1,\n   (ec_r_value &gt; 0 & ec_r_value &lt; 1) ~ 2,\n   ec_r_value == 1 ~ 3,\n   ec_r_value &gt; 1 ~ 4\n  ))\n\nwrite_csv(ec_cat, \"data/ec_category_mukey.csv\")\n\n\nmukey_ec1 &lt;- m %&gt;% \n  filter(ec_r_value == 1) %&gt;% \n  select(mukey)\n\ndf_mukey_ec1 &lt;- left_join(mukey_ec1, mukey_details, by = \"mukey\")\n\nmukey_ecless1 &lt;- m %&gt;% \n  filter(ec_r_value &gt; 0 & ec_r_value &lt; 1) %&gt;% \n  select(mukey)\n\ndf_mukey_ecless1 &lt;- left_join(mukey_ecless1, mukey_details, by = \"mukey\")\n\n\n\n7.8.2 Crop tolerance ratings for EC\nThe line for saline or saline-sodic soils is 4 dS / m. A factsheet I found from NDSU online called “Corn Response to Soil Salinity” (saved in _refs) reports that corn yields started to decline above 1.96 dS/m (on sandy loam soils) and above 2.95 dS/M for silty clay loams. Crop response to salinity is related to soil texture; “coarser textured soils may not contain as much water as finer textured soils, making the salts more potent. So, if you have a sandy loam, your crop salt tolerances will be lower than if you have a silty clay loam”.\nThese tables are all taken from the excellent “Managing Saline Soils in North Dakota” fact sheet by David Franzen, published in 2003. Helpful context for interpreting these EC values. In general, it looks like vegetable crops are more sensitive at lower levels of EC compared to some of the major grain cash crops like corn, soybeans.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#ph",
    "href": "08-var-transformations.html#ph",
    "title": "7  Variable Transformations",
    "section": "7.9 pH",
    "text": "7.9 pH\nI think this one is fine without a transformation.\n\nplot_transformations(var = ph1to1h2o_r_value,\n                     df = m,\n                     nbins = 20)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#carbonates",
    "href": "08-var-transformations.html#carbonates",
    "title": "7  Variable Transformations",
    "section": "7.10 Carbonates",
    "text": "7.10 Carbonates\nSquare root\n\nplot_transformations(var = caco3_r_value,\n                     df = m,\n                     nbins = 20, \n                     log10_adjust = 1, \n                     ln_adjust = 1,\n                     sqrt_adjust = 1)\n\n\n\n\n\n\n\nm %&gt;% \n  filter(caco3_r_value &gt; 0) %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = sqrt(caco3_r_value)))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#sec-lep-var",
    "href": "08-var-transformations.html#sec-lep-var",
    "title": "7  Variable Transformations",
    "section": "7.11 LEP",
    "text": "7.11 LEP\nWould pick log10 or ln here, recall that since we are setting LEP=0 to 0.5 (see Section 8.6.4 ), don’t need to worry about adding an offset before the log.\n\nplot_transformations(var = lep_r_value,\n                     df = m,\n                     nbins = 15, \n                     log10_adjust = 1, \n                     ln_adjust = 1,\n                     sqrt_adjust = 1)\n\n\n\n\n\n\n\n\nLEP values and corresponding shrink-swell classes:\n\nLow: &lt;3\nModerate: 3-6\nHigh: 6-9\nVery high: &gt;= 9\n\n\n# setting LEP = 0 as it's own class here\n# because I want to investigate where these are on a map\nlep_cat &lt;- m %&gt;% \n  select(mukey, lep_r_value) %&gt;% \n  mutate(lep_cat = case_when(\n    lep_r_value == 0 ~ 0,\n    lep_r_value &lt;3 ~ 1,\n   (lep_r_value &gt;= 3 & lep_r_value &lt; 6) ~ 2,\n   (lep_r_value &gt;= 6 & lep_r_value &lt; 9) ~ 3,\n   lep_r_value &gt;= 9 ~ 4\n  ))\n\nwrite_csv(lep_cat, \"data/lep_category_mukey.csv\")\n\n# quick tabulation of number mukeys in each group\nlep_cat_desc &lt;- c(\"Zero\", \"Low\", \"Moderate\", \"High\", \"Very High\")\n\nlep_cat %&gt;% \n  group_by(lep_cat) %&gt;% \n  summarise(n = n(), \n            .groups = \"drop\") %&gt;% \n  mutate(lep_desc = lep_cat_desc) %&gt;% \n  select(lep_desc, n, lep_cat)\n\n\n  \n\n\n\n\n7.11.1 Explore LEP = 0\n\nSee also Section 8.6.4\nThere are 117 MUKEYs with LEP = 0.\nThere are 75 unique MUNAMEs with LEP = 0\nAfter exploring maps of LEP on 2022-09-19, Nic and I decided to set anything with LEP = 0 to 0.5, which keeps it in the lowest part of the “low” range. When we looked at these values across the state, they were discontinuous across counties, and the total area with LEP=0 is small. Suspect these aren’t\n\n\nmukey_lep_zero &lt;- m %&gt;% \n  filter(lep_r_value == 0) %&gt;% \n  select(mukey)\n\n# how many MUKEYs have LEP = 0?\nnrow(mukey_lep_zero)\n\n[1] 117\n\n# are the MUNAMES a clue to why LEP=0?\ndf_mukey_lep_zero &lt;- left_join(mukey_lep_zero, mukey_details, by = \"mukey\") %&gt;% \n  select(mukey, muname, mukind)\n\ndf_mukey_lep_zero\n\n\n  \n\n\n# 22 of the 75 unique map unit NAMES contain \"muck\"\nunique(df_mukey_lep_zero$muname) %&gt;% str_detect(string = ., pattern = \"muck\") %&gt;% \n  sum()\n\n[1] 22\n\n# 30 of the 75 unique map unit NAMES contain \"sand\"\nunique(df_mukey_lep_zero$muname) %&gt;% str_detect(string = ., pattern = \"sand\") %&gt;% \n  sum()\n\n[1] 30\n\n# these munames don't contain \"sand\" or \"muck\"\ndf_mukey_lep_zero %&gt;% \n  filter(!str_detect(muname, \"muck\"),\n         !str_detect(muname, \"sand\"))\n\n\n  \n\n\n\nLet’s take a look at the texture classes:\n\n# rows are added because we have some MUKEYs with \n# multiple components \nlepzero_cokey &lt;- left_join(df_mukey_lep_zero, cmp_lookup, by = \"mukey\")\n\nlepzero_cmp &lt;- left_join(lepzero_cokey, cmp_slab, by = c(\"cokey\"))\n\ncmp_tax &lt;- cmp_details %&gt;% select(mukey, cokey, contains(\"tax\"))\n\n\nlz &lt;- left_join(lepzero_cmp, cmp_tax, by = \"cokey\")\n\nlepzero_ssc &lt;- lz %&gt;% \n  select(cokey, sandtotal_r_value, silttotal_r_value, claytotal_r_value) %&gt;% \n  mutate(texcl = aqp::ssc_to_texcl(sand = lz$sandtotal_r_value,\n                                   clay = lz$claytotal_r_value))\n\nlepzero_ssc %&gt;% \n  group_by(texcl) %&gt;% \n  count() %&gt;% \n  mutate(lab_ypos = n+2) %&gt;% \n  ggplot() + \n  geom_col(aes(x = reorder(texcl, n), y = n)) +\n  geom_text(aes(x = texcl, y = lab_ypos, label = n)) +\n  theme_bw() +\n  xlab(\"Texture Class\") + \n  ylab(\"Number MUKEYs\") + \n  ggtitle(\"MUKEYs with LEP=0\")\n\n\n\n\n\n\n\n\nAnd also look at the soil taxonomy:\n\n# great groups\nlz %&gt;% \n  group_by(taxgrtgroup) %&gt;% \n  count() %&gt;% \n  arrange(desc(n))\n\n\n  \n\n\n# subgroups\nlz %&gt;% \n  group_by(taxsubgrp) %&gt;% \n  count() %&gt;% \n  arrange(desc(n))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#ksat",
    "href": "08-var-transformations.html#ksat",
    "title": "7  Variable Transformations",
    "section": "7.12 Ksat",
    "text": "7.12 Ksat\nWould pick log10 or ln here.\n\nplot_transformations(var = ksat_r_value,\n                     df = m,\n                     nbins = 15)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "08-var-transformations.html#awc",
    "href": "08-var-transformations.html#awc",
    "title": "7  Variable Transformations",
    "section": "7.13 AWC",
    "text": "7.13 AWC\nLog10 or ln\n\nplot_transformations(var = awc_r_value,\n                     df = m,\n                     nbins = 20)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variable Transformations</span>"
    ]
  },
  {
    "objectID": "09-plot-ec-lep.html",
    "href": "09-plot-ec-lep.html",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "",
    "text": "8.1 Overview\nThis chapter serves two main purposes:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MAP EC and LEP with `{terra}`</span>"
    ]
  },
  {
    "objectID": "09-plot-ec-lep.html#overview",
    "href": "09-plot-ec-lep.html#overview",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "",
    "text": "visualize distributions of EC and LEP values, to help make a decision about how to deal with unlikely values (LEP = 0) or very low / agronomically unimportant values (EC between 0-1).\nDo a test run of the raster reclass process with {terra}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MAP EC and LEP with `{terra}`</span>"
    ]
  },
  {
    "objectID": "09-plot-ec-lep.html#load-packages-and-data",
    "href": "09-plot-ec-lep.html#load-packages-and-data",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.2 Load packages and data",
    "text": "8.2 Load packages and data\nI’ll be using the {terra} package, which is for working with raster data. When I started this project, I created the initial AOI raster in QGIS and modified in ArcGIS, but am hoping to do more in R so it can be documented as part of this workflow.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(terra)\n\n# gssurgo raster, clipped to AOI and simple MUKEYs to reduce size\nr &lt;- rast(\"../MapunitRaster_10m_Clip1_and_Reclass/Reclass_tif1.tif\")\n\n# allows us to translate our \"new\" (shorter) MUKEYs to the\n# originals that match up with the rest of the db\naoi_mu &lt;- read.delim(\"data/gSSURGO_MN/mukey_new_crosswalk.txt\", sep = \",\") %&gt;% \n  select(MUKEY, MUKEY_New, Count)\n\nI created these keys for assigned EC category and LEP category in Chapter 7 .\n\n# to make a thematic EC map\nec_cat &lt;- read_csv(\"data/ec_category_mukey.csv\") %&gt;%\n  full_join(., aoi_mu, by = c(\"mukey\" = \"MUKEY\")) %&gt;% \n  mutate(ec_cat = case_when(\n    is.na(ec_cat) ~ 5,\n    TRUE ~ ec_cat\n  ))\n\nRows: 6912 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): mukey, ec_r_value, ec_cat\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlep_cat &lt;- read_csv(\"data/lep_category_mukey.csv\")%&gt;% \n  full_join(., aoi_mu, by = c(\"mukey\" = \"MUKEY\")) %&gt;% \n  mutate(lep_cat = case_when(\n    is.na(lep_cat) ~ 5,\n    TRUE ~ lep_cat\n  ))\n\nRows: 6912 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): mukey, lep_r_value, lep_cat\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MAP EC and LEP with `{terra}`</span>"
    ]
  },
  {
    "objectID": "09-plot-ec-lep.html#spatraster-object",
    "href": "09-plot-ec-lep.html#spatraster-object",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.3 SpatRaster object",
    "text": "8.3 SpatRaster object\nLook at how our raster object appears in R.\n\nr\n\nclass       : SpatRaster \ndimensions  : 61033, 47668, 1  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : -91855, 384825, 2278555, 2888885  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD_1983_Albers \nsource      : Reclass_tif1.tif \nname        : Reclass_tif1 \nmin value   :            0 \nmax value   :         7861",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MAP EC and LEP with `{terra}`</span>"
    ]
  },
  {
    "objectID": "09-plot-ec-lep.html#tables-to-re-classify-cell-values",
    "href": "09-plot-ec-lep.html#tables-to-re-classify-cell-values",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.4 Tables to re-classify cell values",
    "text": "8.4 Tables to re-classify cell values\nFirst, try making a thematic map with the EC categories I loaded above. Will use terra::classify() to assign new values to my raster cells. Need to supply a two-column matrix for the reclass, with “from” (first column) and “to” (second column) values.\n\n# 7999 is the missing data value\nec_mx &lt;- ec_cat %&gt;% \n  select(MUKEY_New, ec_cat) %&gt;% \n  add_row(MUKEY_New = 7999, ec_cat = 5) %&gt;% \n  as.matrix()\n\nI’m going to do one for LEP too:\n\n# 7999 is the missing data value\nlep_mx &lt;- lep_cat %&gt;% \n  select(MUKEY_New, lep_cat) %&gt;% \n  add_row(MUKEY_New = 7999, lep_cat = 5) %&gt;% \n  as.matrix()",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MAP EC and LEP with `{terra}`</span>"
    ]
  },
  {
    "objectID": "09-plot-ec-lep.html#raster-reclass-process",
    "href": "09-plot-ec-lep.html#raster-reclass-process",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.5 Raster reclass process",
    "text": "8.5 Raster reclass process\n!! Note that I have set this chunk option eval: false to make sure this doesn’t run again when I render the project (because it takes so long). In my _quarto.yml I have the execute option freeze: auto set, which typically would mean that code is only re-run if I have changed the source. I want to avoid that behavior here, because I’m fiddling around with the source, but don’t need this test to run again.\nAlso, it is important to supply the filename argument here to write the file. Otherwise, it will throw an error related to “insufficient disk space”, because terra is trying to save the reclassed raster as a temp file. For more info, read this issue terra’s github repo.\nRecall that 1 byte = 8 bits = 2^8 (256) unique numbers, so the datatype = \"INT2U\" below means “integer, 2 bytes, unsigned”. Back when I was dealing with the big raster size due to MUKEY values (see Section 1.6.3 ), I reclassed the raster in ArcGIS to allow us to use 16-bit encoding. 16-bit would be 2 bytes so I think I should be able to stick with that for saving this EC raster.\n\n###### RECLASS for EC -------------------------------\n\n# this took a LONG time (50 minutes??)\n\nstart_reclass &lt;- Sys.time()\n\nr_reclass &lt;- classify(x = r,\n                      rcl = ec_mx, \n                      filename = \"D:/big-files-backup/ch03-sh-groups/ec_reclass.tif\",\n                      datatype = \"INT2U\", \n                      overwrite = TRUE)\n\nend_reclass &lt;- Sys.time() \n\nstart_reclass - end_reclass\n\n###### RECLASS for LEP -------------------------------\n\nstart_reclass &lt;- Sys.time()\n\nr_reclass &lt;- classify(x = r,\n                      rcl = lep_mx, \n                      filename = \"D:/big-files-backup/ch03-sh-groups/lep_reclass.tif\",\n                      datatype = \"INT2U\", \n                      overwrite = TRUE)\n\nend_reclass &lt;- Sys.time() \n\nstart_reclass - end_reclass\n\n\n8.5.1 A note about file size\nAfter I saved this new raster, I checked the file size and it came to 0.1070863 GB (not bad!). I’d like to figure out how to make it go faster (if possible), but right now that’s not my first priority.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MAP EC and LEP with `{terra}`</span>"
    ]
  },
  {
    "objectID": "09-plot-ec-lep.html#visualize-re-classed-rasters",
    "href": "09-plot-ec-lep.html#visualize-re-classed-rasters",
    "title": "8  MAP EC and LEP with {terra}",
    "section": "8.6 Visualize re-classed rasters",
    "text": "8.6 Visualize re-classed rasters\n\n8.6.1 Read in .tifs\nReminder of what EC cell values mean:\n\n1: EC is 0\n2: EC is &gt;0, but &lt;1\n3: EC = 1\n4: EC &gt; 1\n5: NA value (meaning we are not including that area)\n\nNote the min and max values here (1 and 5). This means that I successfully reclassed all my raster values.\n\nec_rast &lt;- rast(\"D:/big-files-backup/ch03-sh-groups/ec_reclass.tif\")\n\nec_rast\n\nclass       : SpatRaster \ndimensions  : 61033, 47668, 1  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : -91855, 384825, 2278555, 2888885  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD_1983_Albers \nsource      : ec_reclass.tif \nname        : Reclass_tif1 \nmin value   :            1 \nmax value   :            5 \n\n\nReminder of what LEP cell values mean:\n\n0: LEP = 0\n1: Low LEP &lt;3\n2: Moderate LEP &gt;=3 & &lt;6\n3: High LEP&gt;= 6 & &lt;9\n4: Very high LEP &gt;= 9\n5: NA value (meaning we are not including that area)\n\n\nlep_rast &lt;- rast(\"D:/big-files-backup/ch03-sh-groups/lep_reclass.tif\")\n\nlep_rast\n\nclass       : SpatRaster \ndimensions  : 61033, 47668, 1  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : -91855, 384825, 2278555, 2888885  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD_1983_Albers \nsource      : lep_reclass.tif \nname        : Reclass_tif1 \nmin value   :            0 \nmax value   :            5 \n\n\n\n\n8.6.2 Plot troubleshooting\nHad issues with getting the colors to work in this plot, this gis.stackexchange thread was helpful.\nI had to update my version of terra, I had 1.4.XX, the method for doing colors described at the link above requires 1.5.50 or higher.\nWhen I run this interactively, I get error messages (below), but the plot still appears. This seems to be a known issue, potentially related to garbage collection. Can follow this issue for more details, last updated 2022-09-20 (very recent). This is the error:\nError in x$.self$finalize() : attempt to apply non-function\n\n\n8.6.3 EC Plot\nNic and I explored these rasters in QGIS on 2022-09-19, and decided that anything with EC &gt;0 but &lt;1 should be set to 0. As you can see on the plots below, there are some weird artefacts in SE Minnesota that are showing up with measured EC values, even though surrounding areas are all 0. Because there are parts of NW Minnesota that also have EC values in this range, it’s possible that this decision will create some weirdness in that part of the map. Check this out in the final clustering outputs.\n\nmycols &lt;- data.frame(values = c(1:5), cols = hcl.colors(5, palette = \"viridis\"))\n\ncoltab(ec_rast) &lt;- mycols\n\nplot(ec_rast, col = mycols, plg=list(legend = c(\"EC=0\", \"EC&gt;0 & &lt;1\", \"EC=1\", \"EC&gt;1\", \"Not Incl\")), main = \"EC Categories\")\n\n\n\n\n\n\n\n\nAnd a plot to highlight just the areas where EC will be set to zero:\n\nlowec_col_only &lt;- data.frame(values = c(1:5),\n                             cols = c(\"#4B0055\",\"#FDE333\", \"#4B0055\", \"#4B0055\", \"#4B0055\"))\n\nplot(ec_rast, col = lowec_col_only, plg=list(legend = c(\"EC=0\", \"EC&gt;0 & &lt;1\", \"EC=1\", \"EC&gt;1\", \"Not Incl\")), main = \"EC values to set to 0\")\n\n\n\n\n\n\n\n\n\n\n8.6.4 LEP Plot\n\nlepcols &lt;- data.frame(values = c(0:5), cols = hcl.colors(6, palette = \"Zissou 1\"))\n\ncoltab(lep_rast) &lt;- lepcols\n\n\nplot(lep_rast, col = lepcols, plg=list(legend = c(\"LEP=0\", \"LEP&lt;3\", \"LEP&gt;=3 & &lt;6\", \"LEP&gt;=6 & &lt;9\", \"LEP &gt;=9\", \"Not incl\")), main = \"LEP Categories\")\n\n\n\n\n\n\n\n\nAnd a plot to highlight only the areas with LEP = 0, these will be set to 0.5 (the low end of the lowest category). We made this decision because the LEP=0 areas are discontinuous across counties, and not very prevalent across the state. Suspect this is not a measured value.\n\nlepz_col_only &lt;- data.frame(values = c(0:5),\n                             cols = c(\"#FDE333\",\"#4B0055\", \"#4B0055\", \"#4B0055\", \"#4B0055\", \"#4B0055\"))\n\nplot(lep_rast, col = lepz_col_only, plg=list(legend = c(\"LEP=0\", \"LEP&lt;3\", \"LEP&gt;=3 & &lt;6\", \"LEP&gt;=6 & &lt;9\", \"LEP &gt;=9\", \"Not incl\")), main = \"LEP values to set to 0.5\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MAP EC and LEP with `{terra}`</span>"
    ]
  },
  {
    "objectID": "10-additional-data-prep.html",
    "href": "10-additional-data-prep.html",
    "title": "9  Additional Data Preparation",
    "section": "",
    "text": "9.1 Anthroportic Udorthents",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Additional Data Preparation</span>"
    ]
  },
  {
    "objectID": "10-additional-data-prep.html#anthroportic-udorthents",
    "href": "10-additional-data-prep.html#anthroportic-udorthents",
    "title": "9  Additional Data Preparation",
    "section": "",
    "text": "9.1.1 Identify MUKEYs to exclude\n\nExclude taxsubgrp == Anthroportic Udorthent\n\nThis covers some of the pits (see next bullet), and all of the MUNAME containing “dump”. It also covers everything with “landfill” in the MUNAME\n\nExclude MUNAMEs that contain “Pit” or “pit”, but keep “pitted”, as below\n\nThis covers pits that are not classified as Anthroportic Udorthents\n\n\n\n# look at anything that includes \"Anthro\" in the subgroup\nanthro &lt;- mu_detail %&gt;% \n  filter(str_detect(taxsubgrp, \"Anthro\")) %&gt;% \n  select(muname, taxsubgrp, muacres, mukey, cokey)\n\nanthro \n\n\n  \n\n\n# all landfills in our \"include\" dataset \n# are classified as Anthroportic Udorthents \nmu_detail %&gt;% filter(str_detect(muname, \"landfill\")) %&gt;% \n  select(muname, taxsubgrp, taxgrtgroup, muacres, mukey, cokey)\n\n\n  \n\n\n# not all munames that include \"pit\" (gravel or sand)\n# include classification info, some have taxsubgrp \n# entered as anthroportic udorthent and some don't \n# this means we will also want to exclude based on \n# some version of \"pit\" in the muname, but careful not to exclude # \"pitted\"\npits &lt;- mu_detail %&gt;% \n  filter(str_detect(muname, \"Pit\") |\n           str_detect(muname, \"pit\"),\n         !str_detect(muname, \"pitted\")) %&gt;% \n  select(muname, taxsubgrp, taxgrtgroup, muacres, mukey, cokey) \n\npits\n\n\n  \n\n\n# it appears that all munames that include \"dump\"\n# are classified as anthroportic udorthents \ndumps &lt;- mu_detail %&gt;% \n  filter(str_detect(muname, \"Dump\") |\n           str_detect(muname, \"dump\")) %&gt;% \n  select(muname, taxsubgrp, muacres, mukey, cokey) \n\ndumps\n\n\n  \n\n\n# look at the other Udorthents to be sure\n# excluding the Anthroportic Udorthents here\n# b/c they are already on the cut list\nudorthents &lt;- mu_detail %&gt;% \n  filter(taxgrtgroup == \"Udorthents\",\n         !str_detect(taxsubgrp, \"Anthroportic\")) %&gt;% \n  #safer to do this because sometimes udorthent is singular, sometimes plural\n  select(muname, taxsubgrp, taxgrtgroup, muacres, mukey, cokey) \n\n# these all seem fine to keep\nudorthents\n\n\n  \n\n\n\n\n\n9.1.2 Compile list of MUKEYs to exclude\n\nanthro_mukeys &lt;- anthro %&gt;% pull(mukey)\n\npit_mukeys &lt;- pits %&gt;% \npull(mukey)\n\n# the \"all\" list contains duplicates, \n# keep only unique mukeys\nexcl_mukeys_all &lt;- c(anthro_mukeys, pit_mukeys)\nexcl_mukeys_unique &lt;- unique(excl_mukeys_all)\n\n\n\n9.1.3 Exclude\n\nmu_no_anthro &lt;- mu %&gt;% \n  filter(!mukey %in% excl_mukeys_unique)\n\nWe started with 6912 MUKEYs, after excluding the Anthroportic Udorthents and pits, we have 6873 MUKEYs, a difference of 39 .",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Additional Data Preparation</span>"
    ]
  },
  {
    "objectID": "10-additional-data-prep.html#update-ec-values",
    "href": "10-additional-data-prep.html#update-ec-values",
    "title": "9  Additional Data Preparation",
    "section": "9.2 Update EC Values",
    "text": "9.2 Update EC Values\nAfter reviewing maps of the EC values across the state in Chapter 8 , we decided to set all EC values that were &gt;0 and &lt;1 to 0.\n\nmu_ec_update &lt;- mu_no_anthro %&gt;% \n  mutate(ec_r_value = case_when(\n    (ec_r_value &gt; 0 & ec_r_value &lt; 1) ~ 0,\n    TRUE ~ ec_r_value\n  ))\n  \n# how many mukeys were set to zero with the above rule?\nec_set_zero &lt;- mu_no_anthro %&gt;% \n  filter(ec_r_value &gt; 0, ec_r_value &lt; 1) %&gt;% \n  pull(mukey) %&gt;% \n  unique() %&gt;% \n  length()\n\nThere were 205 MUKEYs with EC value set to zero using the above rule.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Additional Data Preparation</span>"
    ]
  },
  {
    "objectID": "10-additional-data-prep.html#update-lep-values",
    "href": "10-additional-data-prep.html#update-lep-values",
    "title": "9  Additional Data Preparation",
    "section": "9.3 Update LEP Values",
    "text": "9.3 Update LEP Values\nThere are a small number of map units, covering a small area, that have LEP=0. Set these to 0.5 (based on the spatial pattern, we suspect these aren’t truly measured 0 values).\n\nmu_lep_update &lt;- mu_ec_update %&gt;% \n  mutate(lep_r_value = case_when(\n    lep_r_value == 0 ~ 0.5,\n    TRUE ~ lep_r_value\n  ))\n\n# how many mukeys were set to 0.5 with the above rule?\nlep_update_mukeys &lt;- mu_ec_update %&gt;% \n  filter(lep_r_value == 0) %&gt;% \n  pull(mukey) %&gt;% \n  unique() %&gt;% \n  length()\n\nThere were 117 affected by the above rule.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Additional Data Preparation</span>"
    ]
  },
  {
    "objectID": "10-additional-data-prep.html#sec-fen",
    "href": "10-additional-data-prep.html#sec-fen",
    "title": "9  Additional Data Preparation",
    "section": "9.4 High carbonates?",
    "text": "9.4 High carbonates?\nIn ?sec-clust-full , I noticed some of our MUKEYs have REALLY high carbonates for topsoil (&gt;20% ). Look into these a little more, is it possible these are data entry errors?\nThe one with 70% carbonates in particular seems goofy. I wonder if this is a calcareous fen? Do we want to include it? Represents ~700 acres across the state. I mapped this and it appears to be mostly in Steele County (SE MN).\nMost of the others that have carbonates in the 20-30% range are rims on depressions in till plains or lake plains. So that seems reasonable given the setting.\n\nhighcarb &lt;- mu_lep_update %&gt;% \n  filter(caco3_r_value &gt; 20)\n\nhighcarb %&gt;% \n  ggplot() + \n  geom_point(aes(x = ph1to1h2o_r_value, y = caco3_r_value)) + \n  theme_bw()\n\n\n\n\n\n\n\nhc_mukeys &lt;- highcarb %&gt;% pull(mukey)\n\n# clicked through these, to see if any seemed \n# like data entry errors\nmu_detail %&gt;% \n  filter(mukey %in% hc_mukeys) %&gt;% \n  select(mukey, caco3_r_value, \n         muname, muacres, compname, geomdesc, contains(\"tax\")) \n\n\n  \n\n\n# this is the really high one (Marsh) \nmu_detail %&gt;% filter(muname == \"Marsh\")\n\n\n  \n\n\n# I want to plot these Marsh areas in QGIS. What \n# is the short MUKEY? \nmu_detail %&gt;% \n  filter(muname == \"Marsh\") %&gt;% \n  left_join(., cwalk, by = c(\"mukey\" = \"MUKEY\")) %&gt;% \n  select(mukey, MUKEY_New)\n\n\n  \n\n\n\nGiven the very small area, and extreme carbonate values, I’m going to drop this one MUKEY (428275 in SSURGO; 2682 in my reclass).\n\ndropmarsh &lt;- mu_lep_update %&gt;% \n  filter(mukey != 428275)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Additional Data Preparation</span>"
    ]
  },
  {
    "objectID": "10-additional-data-prep.html#save-dataset",
    "href": "10-additional-data-prep.html#save-dataset",
    "title": "9  Additional Data Preparation",
    "section": "9.5 Save dataset",
    "text": "9.5 Save dataset\nThis dataset is now ready for modeling.\n\nwrite_csv(dropmarsh, \"data/clean_mu_weighted_soil_props.csv\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Additional Data Preparation</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html",
    "href": "11-implement-kmeans.html",
    "title": "10  Implement k-means",
    "section": "",
    "text": "10.1 Overview\nIn this section, I will use the cleaned dataset created in the last chapter to build a k-means pipeline that runs a model for a range of different cluster sizes (k).\nAfter fitting the models, I review some common metrics for determining best cluster sizes, and save the model objects for further investigation in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#setup",
    "href": "11-implement-kmeans.html#setup",
    "title": "10  Implement k-means",
    "section": "10.2 Setup",
    "text": "10.2 Setup\n\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glue)\nlibrary(tidyclust)\nlibrary(factoextra) # trying fviz_nbclust(), which gives elbow, silhouette, and gap statistic\nlibrary(hopkins)\nlibrary(fpc)\n\n\nd &lt;- read_csv(\"./data/clean_mu_weighted_soil_props.csv\") %&gt;% \n  select(-contains(\"comp_pct\"))\n\nold_names &lt;- colnames(d)\n\nnew_names &lt;- str_replace_all(old_names, \"_r_value\", \"\")\n\ncolnames(d) &lt;- new_names\n\nReminder of what the data look like:\n\nhead(d)\n\n\n  \n\n\n\nReminder of the transformations I chose to improve variable distributions and make them normal-ish.\n\nSquare root: clay, carbonates\nLog10: organic matter, cec, lep, ksat, awc\nCube (^3): bulk density\nNone: ec, ph",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#pre-process-data-recipe",
    "href": "11-implement-kmeans.html#pre-process-data-recipe",
    "title": "10  Implement k-means",
    "section": "10.3 Pre-process data (recipe)",
    "text": "10.3 Pre-process data (recipe)\nThis is something specific to the tidymodels modeling workflow, and I like it because it’s very explicit about how variables are transformed (pre-processed) prior to initializing the model.\nHere I apply some transformations to achieve more normal distributions, and then standardize (step_normalize) by subtracting the mean and dividing by 1 sd.\n\nrec_spec &lt;-   recipe(~., data = d) %&gt;% \n    update_role(mukey, new_role = \"ID\") %&gt;% \n  # note this is log10 (the default is ln)\n    step_log(om, cec7, ksat, awc, lep, base = 10) %&gt;% \n    step_mutate(dbthirdbar = dbthirdbar^3) %&gt;% \n    step_sqrt(claytotal, caco3) %&gt;% \n    step_normalize(all_numeric_predictors())\n\nrec_spec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 10\nID:         1\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: om, cec7, ksat, awc, lep\n\n\n• Variable mutation for: dbthirdbar^3\n\n\n• Square root transformation on: claytotal, caco3\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n\n10.3.1 Check that it worked\nAlthough I did make plots of the data distributions after transforming in Chapter 7 , I wanted to do it again here as a check that the pre-processing that I am specifying is working as intended. Here, I do that with two functions from {recipes} :\n\nprep() estimates a pre-processing recipe. It takes my input dataset (“training set”) , and estimates the parameters (model inputs), reporting back on the specific operations and missing data\nbake() to return the training data (because I set new_data to NULL)\n\n\n# retain argument here tells prep to keep \n# the pre-processed training data \n# note this can make the final recipe size large, \n# so this is not the recipe object I probably want to use\n# in my list col below\ncheck_prep &lt;- prep(rec_spec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \ncheck_prepped_df &lt;- bake(check_prep, new_data = NULL)\n\nhead(check_prepped_df)\n\n\n  \n\n\n# save the pre-processed data for making a \n# correlation matrix\n\nwrite_csv(check_prepped_df, \"./data/data_preprocessed_all_var.csv\")\n\nOK, now making a plot of the transformed variables to take a look at their distributions:\n\ncheck_prepped_df %&gt;% \n  select(-mukey) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"var\", values_to = \"value\")  %&gt;% \n  ggplot(aes(x = value)) + \n  geom_histogram(bins =25) + \n  facet_wrap(vars(var), scales = \"free\") + \n  theme_bw() + \n  ggtitle(\"Distributions of transformed and standardized vars\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#sec-mod-opt",
    "href": "11-implement-kmeans.html#sec-mod-opt",
    "title": "10  Implement k-means",
    "section": "10.4 Set model options",
    "text": "10.4 Set model options\nDo I need to think about the initializer here? Based on what I was reading in the book chapter by Tan et al., 2018 (“Cluster Analysis: Basic Concepts and Algorithms”), it sounds like using kmeans++ for the initialization could results in better clustering (lower SSE). See page 543. See also this paper referenced in the clusterR documentation about seeding (initialization).\nstats::kmeans() the default seems to be Hartigan-Wong method, which uses random partition for the initialization. From the {tidyclust} documentation:\n\nThe observations are assigned to a cluster uniformly at random. The centroid of each cluster is computed, and these are used as the initial centers.\n\nI guess the other benefit of the Hartigan-Wong method is that it results in more consistent human-verified clusters (again, per tidyclust documentation listed above). Could read this blog post for a deeper dive into these methods if needed.\nClusterR::KMeans_rcpp() uses the Lloyd/Forgy method (per the tidyclust docs, this info wasn’t easy to find in the ClusterR docs)\nFor now I’m going with stats::kmeans, and setting the nstart to 10. Because the random initial configuration (random starting centroids) can have an impact on the final clusters, it sounds like it’s a good idea to do multiple starts, and then let kmeans return the best one (using lowest within cluster SSE as the metric for “best”)\n\n# writing a custom function here so I can be explicit \n# about the options I'm choosing, and also use within the \n# list-col framework I set up with map() below. \nkm_spec &lt;- function(nclust){\n  \n  tidyclust::k_means(num_clusters = nclust) %&gt;%\n    parsnip::set_engine(engine = \"stats\",\n               nstart = 10, # 1 is default, &gt;1 recommended\n               algorithm = \"Hartigan-Wong\", # H-W is default\n               iter.max = 20) # default is 10, wasn't always enough\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#set-up-data-structure",
    "href": "11-implement-kmeans.html#set-up-data-structure",
    "title": "10  Implement k-means",
    "section": "10.5 Set up data structure",
    "text": "10.5 Set up data structure\nHere I set up a dataframe that will catch my modeling results in list columns of the different model objects and return values. The first column I define specifies the range of different cluster sizes (k) that we will try.\n\ntry_clusts &lt;- c(2:20)\n\nkm_df &lt;- data.frame(n_clust = try_clusts)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#specify-model-for-each-value-of-k",
    "href": "11-implement-kmeans.html#specify-model-for-each-value-of-k",
    "title": "10  Implement k-means",
    "section": "10.6 Specify model (for each value of k)",
    "text": "10.6 Specify model (for each value of k)\nFor each unique value of k (2-20), this returns a model specification object (in the kmeans_spec column) based on the custom function I wrote above. The model specification has all the options set about how we want the algorithm to run (methods, number of starts, etc.). We need a different one for each value of k.\nThe kmeans_wflow column here holds our workflow objects. These objects combine our model specification (from kmeans_spec) with the data recipe (preprocessor) we made above (rec_spec, is same for all models).\n\n# for each unique value of clusters (2:20), returns a model\n# specification (kmeans_spec) and a workflow (kmeans_wflow) \n# note that the workflow \nkm_df &lt;- km_df %&gt;%\n  mutate(\n    kmeans_spec = map(n_clust, ~ km_spec(nclust = .x)),\n    kmeans_wflow = map(kmeans_spec,\n                       ~ workflow(\n                         preprocessor = rec_spec, spec = .x\n                       ))\n  )\n\n# our current data structure\nhead(km_df, n=3L )\n\n\n  \n\n\n# take a look at an example workflow\nkm_df$kmeans_wflow[3]\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_mutate()\n• step_sqrt()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = nclust\n\nEngine-Specific Arguments:\n  nstart = 10\n  algorithm = Hartigan-Wong\n  iter.max = 20\n\nComputational engine: stats",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#fit-the-models",
    "href": "11-implement-kmeans.html#fit-the-models",
    "title": "10  Implement k-means",
    "section": "10.7 Fit the models",
    "text": "10.7 Fit the models\nAll the steps above were related to specifying different aspects of this model. Now we can actually fit the models.\nSome troubleshooting here:\n\nStarted by specifying tidyclust::fit() but something weird was happening where my step_normalize() wasn’t included in the pre-processor recipe when I looked at the fitted model object.\nIf I specify parsnip::fit() , then step_normalize() is included and the values of the cluster centroids are in the expected ranges (centered, scaled).\nI also tried this without explicitly specifying the package (so just fit() ) and it worked as expected.\n\n\n# make a quiet version of fit(), so we can capture results \n# and any warning messages from the models \n# see troubleshooting notes below\nquiet_fit &lt;- purrr::quietly(.f = parsnip::fit)\n\nset.seed(4) # for reproducibility \nkm_fit_df &lt;- km_df %&gt;%\n  mutate(km_result = map(.x = kmeans_wflow,\n                      .f = quiet_fit,\n       # data comes after .f b/c not vectorized over            \n                      data = d),\n       km_fit = map(km_result, ~pluck(.x, 'result')),\n       warn = map(km_fit, ~pluck(.x, 'warnings')),\n       msg = map(km_fit, ~pluck(.x, 'messages')),\n       n_iter = map_dbl(km_fit, \n                      ~pluck(.x, 'fit', 'fit', 'fit', 'iter' ))) \n         \n\n# check out current data structure\nhead(km_fit_df, n = 3L)\n\n\n  \n\n\n# don't need anymore, cleaning up\nrm(km_df)\n\n\n10.7.1 Notes about model fit troubleshooting\nIf I set.seed(123) and run kmeans with max.iter=10, get warnings about ‘no convergence at 10 iterations’, for k = 17 and k = 20. Changed to max.iter=20 and ran again, this time no convergence warnings.\nAs a result of this experience, I added some additional columns to the km_fit_df object using {purrr} ’s function quietly() so I could capture warnings and messages that would otherwise only appear in the console (and are hard to trace when I’m iterating through all these models at once). This blog post was very helpful for an example of how to do this.\nIn an earlier troubleshooting attempt, I was trying to see if the warning messages were stored anywhere in the fitted {tidyclust} model object? It seemed like maybe they would have been in ifault, but those values were all 0, even when I had model convergence warnings. In that case I tried indexing into the fitted model objects with map(km_fit, ~pluck(.x, 'result', 'fit', 'fit', 'fit', 'ifault' ))\n\n\n10.7.2 View messages & warnings\nWe can look at any warnings or messages from the modeling process:\n\nkm_fit_df %&gt;% \n  select(n_clust, warn, msg, n_iter)\n\n\n  \n\n\n\n\n\n10.7.3 Look at one fit object\nAs an example, these are what the fitted objects look like.\nNOTE the clustering vector here is using the cluster numbers directly from kmeans(). tidyclust assigns names like “Cluster_1”, “Cluster_2” etc. , but the numbers do NOT necessarily match with what kmeans() returns. The CLUSTERINGS are the same, but the numbers are not necessarily so. So 2 in this “Clustering Vector” below is NOT necessarily equal to tidyclust “Cluster_2” that you might get by using the extract_cluster_assignment function. To keep things consistent, I’m always using the cluster names assigned by tidyclust.\n\nexamp_fit &lt;- km_fit_df$km_result[[4]][['result']]\n\nexamp_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_log()\n• step_mutate()\n• step_sqrt()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-means clustering with 5 clusters of sizes 479, 2019, 1832, 1065, 1477\n\nCluster means:\n   claytotal         om       cec7 dbthirdbar         ec  ph1to1h2o      caco3\n1  0.5174494  0.3745308  0.3208957 -0.6376723  3.3283137  1.1660203  1.0228662\n2 -0.3814723 -0.3465904 -0.2722727  0.3236714 -0.2571456 -0.5186223 -0.5279703\n3  0.7722551  0.6366879  0.8147342 -0.5735785 -0.2463628 -0.2631583 -0.5750294\n4 -1.5118222 -1.1990889 -1.6374004  1.0687407 -0.2271428 -0.7273531 -0.5026188\n5  0.4858861  0.4272060  0.4382179 -0.2948278 -0.2585251  1.1816599  1.4656479\n         lep       ksat        awc\n1  0.4284214 -0.5251382  0.2167607\n2 -0.4841584  0.2713584  0.0553920\n3  0.6939805 -0.7350181  0.5806865\n4 -0.9544845  1.5123302 -1.7551062\n5  0.3503424 -0.3794245  0.3992591\n\nClustering vector:\n   [1] 4 4 4 2 2 4 4 4 4 4 2 4 4 2 4 3 2 2 4 4 2 2 4 4 2 4 4 4 4 4 2 2 4 4 4 4 4\n  [38] 2 2 2 4 4 4 4 4 2 4 4 4 4 4 2 2 2 3 5 5 2 3 3 3 3 3 1 4 4 4 4 5 5 3 3 4 4\n  [75] 5 4 5 3 2 2 3 3 3 1 5 4 4 4 4 4 4 4 3 4 4 5 2 4 4 4 3 3 5 5 5 5 4 4 2 2 2\n [112] 4 4 4 3 5 4 4 4 4 5 3 3 3 3 4 4 4 4 2 2 4 2 4 4 2 4 2 5 2 4 4 4 4 4 4 5 4\n [149] 4 4 5 2 2 3 1 2 1 4 2 2 2 4 5 5 5 5 2 2 4 4 4 2 2 4 4 2 2 5 5 5 2 3 2 2 3\n [186] 5 5 3 3 3 2 2 5 3 3 2 2 2 4 4 4 4 4 4 4 2 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 5\n [223] 5 3 4 4 3 3 3 5 4 4 4 3 3 4 5 2 3 3 4 2 5 5 2 3 4 3 3 1 4 4 4 2 1 2 4 4 5\n [260] 4 5 4 4 4 4 2 4 4 2 4 4 4 4 4 4 4 4 2 2 2 2 4 4 2 2 4 4 4 2 4 4 4 4 2 2 2\n [297] 2 2 4 4 4 4 2 4 2 4 4 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4\n [334] 2 2 2 1 3 5 5 1 1 1 1 5 1 5 5 3 3 5 5 3 1 5 1 3 5 5 1 2 5 3 5 5 2 1 5 5 3\n [371] 2 2 3 2 2 3 3 1 2 3 4 4 5 5 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 5 5 3 3 3 5 5 5\n [408] 5 5 3 2 2 3 3 3 3 2 2 2 2 3 3 3 2 2 5 5 5 3 2 3 2 2 2 3 3 3 4 4 4 3 3 4 2\n [445] 2 3 3 3 3 3 5 3 3 3 3 5 4 3 4 4 2 3 3 3 3 2 3 3 3 5 5 3 3 3 3 3 5 3 3 3 3\n [482] 5 3 2 2 2 5 2 2 3 3 2 3 3 3 5 2 3 3 3 5 3 3 5 5 5 3 3 3 5 3 3 3 3 3 3 3 5\n [519] 2 2 5 5 5 3 3 3 3 3 4 4 2 2 3 3 3 2 2 3 3 2 2 5 3 2 5 5 2 4 2 5 5 1 2 4 2\n [556] 2 5 2 5 3 5 5 5 5 2 2 5 5 5 5 5 3 5 2 5 2 3 3 5 2 5 5 5 3 5 3 5 5 5 2 2 2\n [593] 3 3 5 5 5 3 3 5 2 4 5 3 3 2 3 3 5 3 4 2 2 3 5 2 5 2 3 3 3 3 3 3 5 5 5 5 5\n [630] 2 3 3 3 3 2 2 2 2 3 2 2 2 5 3 3 3 2 2 2 2 3 3 3 3 2 5 5 5 4 4 4 4 2 2 2 2\n [667] 3 5 2 2 2 2 2 2 2 2 2 2 3 2 2 2 4 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 2 2 2 2 4\n [704] 4 2 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 2 4 4 4 5 3 2 4 4 2 4 4 2 4\n [741] 4 4 4 4 2 4 4 2 4 4 5 5 5 5 5 2 5 2 2 4 5 3 2 3 2 5 3 5 5 3 2 3 3 2 3 3 5\n [778] 5 1 5 5 5 5 5 5 2 2 2 2 5 3 3 5 3 3 2 3 5 3 3 5 5 5 5 5 5 5 5 2 3 5 5 5 5\n [815] 3 5 5 5 5 5 5 5 5 2 2 2 3 3 2 2 3 3 3 5 3 3 5 5 2 2 2 2 2 4 5 5 5 3 1 3 3\n [852] 3 1 5 5 1 5 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 4 4 1 3 1 1 4 4 4 1 2 2 4 2\n [889] 2 5 1 3 3 5 3 5 3 5 1 5 5 3 3 1 4 1 2 5 5 1 3 4 5 5 5 5 5 1 1 1 2 1 1 2 1\n [926] 1 1 1 1 1 3 2 2 3 4 2 1 1 1 3 3 5 3 4 4 4 4 3 3 1 5 5 1 1 1 4 4 5 4 4 2 2\n [963] 2 3 5 3 5 4 4 4 3 3 5 5 2 2 4 4 5 2 3 4 2 4 4 4 2 5 5 2 4 4 4 4 3 3 3 1 4\n[1000] 4 2 3 1 2 1 2 4 5 4 4 4 4 4 5 2 2 5 5 2 2 2 2 5 3 3 3 5 5 5 2 5 2 2 3 5 5\n[1037] 5 5 3 3 5 3 5 5 5 2 2 5 3 3 5 3 4 4 5 3 3 3 3 3 3 3 5 3 2 5 5 3 3 3 5 2 2\n[1074] 5 3 3 2 3 5 3 3 5 5 5 2 2 5 5 5 5 3 3 3 3 3 3 3 3 5 5 5 3 5 5 3 3 5 5 5 2\n[1111] 2 2 3 2 3 3 5 5 5 3 3 5 3 3 3 4 5 3 3 3 4 4 4 4 4 5 4 2 4 4 4 4 4 4 4 4 4\n[1148] 4 4 4 3 4 4 4 4 2 4 4 4 2 3 2 4 4 4 4 4 4 4 4 3 2 5 2 4 3 2 3 2 2 4 4 4 4\n[1185] 4 4 4 2 2 2 2 2 4 2 4 4 4 3 3 2 2 2 2 2 2 4 4 4 3 3 4 4 4 2 2 3 3 3 3 2 2\n\n...\nand 162 more lines.\n\n\nA nicer way to look at the results is by accessing specific parts of the fitted model object, as below.\n\n# some basic model metrics\nglance(examp_fit)\n\n\n  \n\n\n# centroid data (transformed/standardized scale)\ncentroids &lt;- tidyclust::extract_centroids(examp_fit)\ncentroids\n\n\n  \n\n\n# helpful to add to future plots for examining indiv. clusters\nclust_stat &lt;- tidyclust::sse_within(examp_fit)\nclust_stat",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#sec-mod-metrics",
    "href": "11-implement-kmeans.html#sec-mod-metrics",
    "title": "10  Implement k-means",
    "section": "10.8 Model metrics",
    "text": "10.8 Model metrics\nSee also section 7.5 in the Chapter by Tan et al. for more about cluster evaluation.\n\n10.8.1 Extract metrics\n\nmetrics_df &lt;- km_fit_df %&gt;%\n  mutate(\n    # tot_sse = total sum of squared error\n    tot_sse = map_dbl(km_fit, ~ sse_total_vec(.x)),\n    # tot_wss = sum of within-cluster sse\n    tot_wss = map_dbl(km_fit, ~sse_within_total_vec(.x)),\n    # sse ratio = wss / total sse, \n    sse_ratio = map_dbl(km_fit, ~sse_ratio_vec(.x))\n    )\n\nrm(km_fit_df)\n\nmetrics_simple &lt;- metrics_df %&gt;% \n  select(n_clust, tot_sse, tot_wss, sse_ratio)\n\nmetrics_simple\n\n\n  \n\n\n\n\n\n10.8.2 Plot Total WSS\nNot a clear “elbow” here, although by the time we get to 10-11 it does seem to be leveling off.\n\nmetrics_simple %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Compare values of k: looking for elbow\")\n\n\n\n\n\n\n\nmetrics_simple %&gt;% \n  filter(n_clust %in% c(2:12)) %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:12)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Zoom in a bit: looking for elbow\")\n\n\n\n\n\n\n\n\n\n\n10.8.3 Average Silhouette\nFrom the {tidyclust} documentation:\n\nAnother common measure of cluster structure is called the silhouette. The silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\n\nIn principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\nSee also pg. 581 in Tan2018 Chap 7 Cluster Analysis: Basic Concepts and Algorithms\n\nprepped_rec &lt;- prep(rec_spec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \nbaked_df &lt;- bake(prepped_rec, new_data = NULL) %&gt;% \n  select(-mukey) \n\ndists &lt;- baked_df %&gt;% as.matrix() %&gt;% dist(method = \"euclidean\")\n\nsilh_df &lt;- metrics_df %&gt;% \n  mutate(avg_sil = map_dbl(km_fit, \n                       tidyclust::silhouette_avg_vec,\n                       dists = dists),\n         indiv_sil = map(km_fit, \n                         tidyclust::silhouette,\n                         dists = dists))\n\nindiv_sil_df &lt;- silh_df %&gt;% select(n_clust, indiv_sil) %&gt;% \n  unnest(indiv_sil) %&gt;% \n  mutate(across(.cols = c(cluster, neighbor),\n                .fns = as.character))\n\nwrite_csv(indiv_sil_df, \"data/kmeans_points_silhouettes.csv\")\n\nrm(metrics_df)  \nrm(dists)\nrm(prepped_rec)\n\nHigher silhouette is better (means observations are closer to their centroids than to other observations). Seems to suggest that 4, 6, 10-12 would be OK (those are local maxima), but not greater than 12.\n\nsilh_df %&gt;% \n  ggplot(aes(x = n_clust, y = avg_sil)) +\n  geom_point() + \n  geom_line() + \n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  ggtitle(\"Overall Average Silhouette\") +\n  labs(subtitle = \"Higher is better, possible values [-1,1]\")\n\n\n\n\n\n\n\n\n\n\n10.8.4 Not used: Gap statistic\nFor fviz_nbclust(), first couple times running this, got Warning: Quick-TRANSFER stage steps exceeded maximum… Looking online, this seems to be a problem with the model not converging. I added some arguments here that are passed on to kmeans(), to make sure that the algorithm settings here match what I run above, including set.seed()\nContinued to get warnings, even though I’m using all the same settings as I use for kmeans up above. Not sure why this is, but I’m not going to spend any more time on it right now. Maybe see if getting the gap statistic through NbClust works better? (Later note: NbClust won’t be a good option either, I can’t alter important kmeans() settings in NbClust). Expect it will take a long time either way, consider running this in a separate script and pulling in the results.\n\nset.seed(4)\nfviz_gap_stat(x = baked_df, \n             FUNcluster = kmeans,\n             method = c(\"gap_stat\"),\n             k.max = 10, # only considering 2-10 clusters\n             nboot = 50, # default is 100\n             verbose = TRUE, \n             iter.max = 20, # passed to kmeans\n             nstart = 10 # passed to kmeans\n             )\n\n\n\n10.8.5 Calinski-Harabasz index\nNot used: {NbClust} , using {fpc} instead.\n\nFor Calinski-Harabasz index, higher values are better\nRealized after setting this up with NbClust that I don’t have the option to pass additional arguments to the kmeans function here. So I can’t make the algorithm settings exactly match my main clustering pipeline above (where I implement k-means using tidyclust and the tidymodels framework, and where I save the results for further analysis). This is a problem because I know from my original tests that I need to change the iter.max value to avoid non-convergence issues, and I also want to change nstart because nstart &gt;1 is typically known to be best practice (find citation for this).\n\n\n# keeping this here as a record, but I\"m NOT USING this function for the C-H index. \n\nnbc_indices &lt;- NbClust::NbClust(data = baked_df,\n                 distance = \"euclidean\",\n                 method = \"kmeans\",\n                 min.nc = 2,\n                 max.nc = 20,\n                 index = \"ch\") # Calinski and Harabasz\n\n# enframe turns a named vector into a dataframe\nch_index_vals &lt;- enframe(nbc_indices$All.index) %&gt;% \n  mutate(name = as.integer(name)) %&gt;% \n  rename(n_clust = name)\n\nTrying a different implementation of the Calinski-Harabasz index from the {fpc} package. This is preferred to the above approach, where I originally used the NbClust function from {NbClust} package because I can give this function my clustering generated above (NbClust does its own run of kmeans but I can’t customize it to keep it consistent with the model settings I use above).\n\n# calinhara wants an observations/variables matrix\n# as first argument (as opposed to a distance matrix)\nobsvar_mx &lt;- as.matrix(baked_df)\n\n# function to extract and modify tidyclust clusters\n# into a integer vector, which I will pass to calinhara()\ncreate_clust_vec &lt;- function(fit_obj){\n  \n  extract_cluster_assignment(fit_obj) %&gt;% \n    pull(.cluster) %&gt;% \n    str_replace(., \"Cluster_\", \"\") %&gt;% \n    as.integer()\n  \n}\n\n# apply function to extract clusterings as integer vectors\n# map to get a c-h index value for every value of k (2-20)\nch_metrics &lt;- silh_df %&gt;%\n  select(n_clust, km_fit) %&gt;%\n  mutate(\n    clustering_vec = map(km_fit, create_clust_vec),\n    ch_index = map_dbl(clustering_vec,\n                       ~ fpc::calinhara(x = obsvar_mx,\n                                        clustering = .x)\n    ))\n\nch_metrics %&gt;% \n  ggplot(aes(x = n_clust, y = ch_index)) + \n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  ylab(\"Calinski-Harabasz index\") + \n  ggtitle(\"Calinski-Harabasz\") +\n  labs(subtitle = \"Higher is better\") +\n  scale_x_continuous(breaks = c(2:20))\n\n\n\n\n\n\n\n\n\n\n10.8.6 Hopkins Statistic\nUsing the {hopkins} package for this. Citations included in the package documentation (also cite Tan et al., 2019 who give an example of using this for evaluating kmeans clusters).\n\nHopkins, B. and Skellam, J.G., 1954. A new method for determining the type of distribution of plant individuals. Annals of Botany, 18(2), pp.213-227.\nCross, G. R., and A. K. Jain. (1982). Measurement of clustering tendency. Theory and Application of Digital Control. Pergamon, 1982. 315-320.\n\nAnd a third citation, helpful illustrations:\n\nLawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of Chemical Information and Computer Sciences, 30(1), 36–41. https://doi.org/10.1021/ci00065a010\n\nApparently {factoextra} also has a Hopkins statistic, try that here too. (It takes a very long time to run, but returns 0.93, similar to 0.99 returned by hopkins::hopkins()\n\nset.seed(4)\nhstat &lt;- hopkins(X = baked_df,\n                 # default, number of rows to sample from the df\n                 m = ceiling(nrow(baked_df)/10), \n                 # default, dimension of the data\n                 d = ncol(baked_df),\n                 # default, kth nearest neighbor to find\n                 k = 1) \nhstat\n\n[1] 0.9999999\n\nhopkins.pval(x = hstat,\n             # this is the default for hopkins() above\n             n = ceiling(nrow(baked_df)/10)) \n\n[1] 0\n\n# commenting out because it takes a very long time to run\n# factoextra::get_clust_tendency(data = baked_df,\n#                                n = 687, \n#                                graph = FALSE)\n\n\n\n10.8.7 WSS and Silhouette metrics on one plot\n\n#| echo: false\n\nsil_totwss &lt;- silh_df %&gt;% \n  select(n_clust, avg_sil, tot_wss, tot_sse, sse_ratio)\n\nch &lt;- ch_metrics %&gt;% \n  select(n_clust, ch_index)\n\nmet_combined &lt;- left_join(sil_totwss, ch, by = \"n_clust\")\n\nwrite_csv(met_combined, \"data/kmeans_cluster_metrics.csv\")\n\nmet2 &lt;- met_combined %&gt;% \n  pivot_longer(cols = -c('n_clust'), names_to = \"metric\",\n               values_to = \"value\")\n\nmet2 %&gt;% \n  ggplot(aes(x = n_clust, y = value)) + \n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(2:20)) + \n  facet_wrap(vars(metric), ncol = 1, scales = \"free\") +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#save-model-fits",
    "href": "11-implement-kmeans.html#save-model-fits",
    "title": "10  Implement k-means",
    "section": "10.9 Save model fits",
    "text": "10.9 Save model fits\nWill save these as Rdata so I can call them up and investigate the cluster centroids more closely in the next chapter.\n\nmods &lt;- silh_df %&gt;% \n  select(n_clust, km_fit)\n\nsave(mods, file = \"data/fitted_kmeans_mods.RData\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#save-cluster-assignments",
    "href": "11-implement-kmeans.html#save-cluster-assignments",
    "title": "10  Implement k-means",
    "section": "10.10 Save cluster assignments",
    "text": "10.10 Save cluster assignments\nFor each version of the model (each value of k, different numbers of clusters), a MUKEY is assigned to a specific cluster. Here, I’m pulling that data, shaping it into one dataframe (one row per MUKEY, cluster assingments in separate columns). I’m also adding back in the soil property data so we can use this in the next step when evaluating different cluster sizes.\n\nclust_assign_df &lt;- mods %&gt;% \n  mutate(clust_assign = map(km_fit, ~augment(.x, new_data = d)),\n         mukey_clust = map(clust_assign, ~select(.x, mukey, .pred_cluster)))\n\n\nassign_mukey_df &lt;- clust_assign_df %&gt;% \n  select(n_clust, mukey_clust) %&gt;% \n  unnest(mukey_clust) %&gt;% pivot_wider(names_from = n_clust, values_from = .pred_cluster, names_prefix = \"k_\")\n\n\nclust_props &lt;- full_join(d, assign_mukey_df, by = \"mukey\")\n\nwrite_csv(clust_props, \"data/mukey_cluster_assignments_and_soilprops.csv\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "11-implement-kmeans.html#note---raster-reclass-step",
    "href": "11-implement-kmeans.html#note---raster-reclass-step",
    "title": "10  Implement k-means",
    "section": "10.11 Note - raster reclass step",
    "text": "10.11 Note - raster reclass step\nA key step that happens between this chapter and the following ones is carried out in R/cluster_reclass_rast.R. This is a script that takes information about the map units and their assigned clusters from the models we just generated and saves that info to separate rasters (one for each version of the model). This allows us to visualize how these different cluster models look.\nI put this step in a separate script because the raster reclassing process takes quite a while (35-40 minutes per raster on my computer).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Implement k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html",
    "href": "22-pca-kmeans.html",
    "title": "11  Do PCA before k-means",
    "section": "",
    "text": "11.1 Overview\nAfter lots of reading during the drafting phase of methods & results for my original k-means analysis, I found many authors in soil science, climate/atomspheric science, and geochemistry who use principal components analysis (PCA) for data reduction prior to doing k-means. The argument for doing this is that when you have highly correlated variables), including them all basically gives more weight to the correlated variables. If they are highly correlated, they contain most of the same “information”, and by including both variables you are giving that “information” more weight in the calculation of (dis)similarity matrix (Euclidean distances) that we ultimately use for clustering.\nMy plan is to do PCA, run k-means again, and compare the results with my original k-means analysis. I think this would be a valuable thing to add to my paper, as we can make an argument for whether one method or another might be more generalizable for others who want to use this technique for creating similar conceptual clusters from a regional set of soil data (presumably with a slightly different set of variables given the specific context).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#implementation",
    "href": "22-pca-kmeans.html#implementation",
    "title": "11  Do PCA before k-means",
    "section": "11.2 Implementation",
    "text": "11.2 Implementation\nThere is an easy way to do PCA as a pre-processing step in the {tidymodels} framework I’ve been doing using the step_pca() function. Under the hood, this uses stats::prcomp() to do the PCA.\nMuch of this code will be similar to Chapter 10, but with some additional exploratory plots as I think about how to incorporate this information in my manuscript.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#setup",
    "href": "22-pca-kmeans.html#setup",
    "title": "11  Do PCA before k-means",
    "section": "11.3 Setup",
    "text": "11.3 Setup\n\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glue)\nlibrary(tidyclust)\nlibrary(factoextra) # trying fviz_nbclust(), which gives elbow, silhouette, and gap statistic\nlibrary(hopkins)\nlibrary(fpc)\nlibrary(ggforce)\nlibrary(gt)\n\n\nd &lt;- readr::read_csv(\"./data/clean_mu_weighted_soil_props.csv\") %&gt;% \n  select(-contains(\"comp_pct\"))\n\nold_names &lt;- colnames(d)\n\nnew_names &lt;- stringr::str_replace_all(old_names, \"_r_value\", \"\")\n\ncolnames(d) &lt;- new_names\n\n# this is a dataframe of the results from my first k-means\n# run using same methods as Devine et al. Loading it so I \n# can use the colors in my PCA plots to get a qualitative \n# sense of whether we \"see\" similar clusters after PCA\nclust_membership &lt;- readr::read_csv(\"data/mukey_cluster_assignments_and_soilprops.csv\") %&gt;% \n  select(mukey, k_6)\n\nd &lt;- dplyr::left_join(d, clust_membership, by = \"mukey\")\n\nReminder of what the data look like:\n\nhead(d)\n\n\n  \n\n\n\nReminder of the transformations I chose to improve variable distributions and make them normal-ish.\n\nSquare root: clay, carbonates\nLog10: organic matter, cec, lep, ksat, awc\nCube (^3): bulk density\nNone: ec, ph",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#pre-process-data-build-recipe",
    "href": "22-pca-kmeans.html#pre-process-data-build-recipe",
    "title": "11  Do PCA before k-means",
    "section": "11.4 Pre-process data (build recipe)",
    "text": "11.4 Pre-process data (build recipe)\n\napply some transformations to achieve more normal distributions,\nthen standardize (step_normalize) by subtracting the mean and dividing by 1 sd\nthen PCA\n\n\nrec_spec &lt;- recipe(~., data = d) %&gt;% \n    update_role(mukey, new_role = \"ID\") %&gt;% \n    update_role(k_6, new_role = \"cluster_k6\") %&gt;% \n  # note this is log10 (the default is ln)\n    step_log(om, cec7, ksat, awc, lep, base = 10) %&gt;% \n    step_mutate(dbthirdbar = dbthirdbar^3) %&gt;% \n    step_sqrt(claytotal, caco3) %&gt;% \n    step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors(), num_comp = 10)\n\nrec_spec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor:  10\ncluster_k6:  1\nID:          1\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: om, cec7, ksat, awc, lep\n\n\n• Variable mutation for: dbthirdbar^3\n\n\n• Square root transformation on: claytotal, caco3\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n• PCA extraction with: all_numeric_predictors()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#functions-to-run-pca-visualize-results",
    "href": "22-pca-kmeans.html#functions-to-run-pca-visualize-results",
    "title": "11  Do PCA before k-means",
    "section": "11.5 Functions to run PCA, visualize results",
    "text": "11.5 Functions to run PCA, visualize results\nThis is modified from the “Tidy models with R” book, section 16.5 “Feature Extraction Techniques”.\nI removed the “dat” argument because I’m using the dataset that is already in my recipe. I also set new_data = NULL in bake as a reminder of this. In prep, it is default to have retain=TRUE, but again I’m explicitly typing it as a reminder to myself of what the defaults / where the data is coming from.\n\nrun_pca &lt;- function(recipe){\n  \n  recipe %&gt;% \n    # here, prep estimates the added PCA step\n    prep(retain = TRUE) %&gt;%\n    # Process the data (new_data=NULL means use data in recipe)\n    bake(new_data = NULL) \n  \n  \n}\n\n# Create the scatterplot matrix\nplot_validation_results &lt;- function(pca_df) {\n  \n  pca_df %&gt;%\n    ggplot(aes(\n      x = .panel_x,\n      y = .panel_y,\n      color = k_6,\n      fill = k_6\n    )) +\n    geom_point(alpha = 0.4, size = 0.5) +\n    geom_autodensity(alpha = .3) +\n    facet_matrix(vars(-c(mukey, k_6)), layer.diag = 2) +\n    scale_color_brewer(palette = \"Dark2\") +\n    scale_fill_brewer(palette = \"Dark2\")\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#principal-components-analysis-pca",
    "href": "22-pca-kmeans.html#principal-components-analysis-pca",
    "title": "11  Do PCA before k-means",
    "section": "11.6 Principal Components Analysis (PCA)",
    "text": "11.6 Principal Components Analysis (PCA)\n\n11.6.1 Visualize PCA results\n\npca_results &lt;- run_pca(rec_spec)\n\nwrite_csv(pca_results, \"data/pca_scores.csv\")\n\n# just plotting the first 5 PCs so we can actually seem them\n\npca_results %&gt;% \nselect(mukey, k_6, PC01, PC02, PC03, PC04, PC05, PC06) %&gt;% \nplot_validation_results() +\n  ggtitle(\"PCA pairwise plots: colors indicate k_6 clusters from original k-means\")\n\n\n\n\n\n\n\n\n\n\n11.6.2 Extract loadings\nTook me FOREVER to figure out why I wasn’t able to extract my loadings with the example code from the step_pca documentation here using tidy(prepped_rec, number = 2, type = \"coef\"). After much frustration, I figured out that the “number” argument needs to correspond to the PCA step in the tidied dataframe of my prepped recipe… in all the documentation this number is 2, so I kept trying that and getting my bulk density mutation step. In my case, the number is 5.\n\npca_prep &lt;- prep(rec_spec)\n\n# 'type = \"coef\"' here gets variables loadings per component\npca_loadings &lt;- tidy(pca_prep, 5, type = \"coef\")\n\nwrite_csv(pca_loadings, \"data/pca_loadings.csv\")\n\n\n\n\n\n\n\n\n\n\n\nltab &lt;- loadings_dat %&gt;% \n  select(terms, value, component) %&gt;% \n  pivot_wider(names_from = component, \n              values_from = value,\n              names_prefix = \"PC\") %&gt;% \n  gt(rowname_col = \"terms\") %&gt;%\n  tab_stubhead(label = \"term\") %&gt;% \n  fmt_number(\n    columns  = contains(\"PC\"),\n    decimals = 2\n  ) %&gt;% \n  sub_missing() %&gt;% \n  tab_header(title = \"PC Loadings\")\n\ngtsave(ltab, filename = \"figs/pc_loadings_table.docx\")\n\n\n\n11.6.3 Extract variance\nNote the number of rows in the pca_var dataframe: 40. We have 4 different terms calculated for each component:\n\nvariance\ncumulative variance\npercent variance\ncumulative percent variance\n\n\npca_var &lt;- tidy(pca_prep, 5, type = \"variance\")\n\nwrite_csv(pca_var, \"data/pca_variance.csv\")\n\nFirst we can look at % variance explained by each component:\n\npca_var %&gt;% \n  filter(terms == \"percent variance\") %&gt;% \n  ggplot(aes(x = component, y = value)) +\n  geom_col() + \n  geom_text(aes(x = component, y = value+2, label = round(value, 1)), color = \"red\") +\n  theme_bw() +\n  ylab(\"Percent variation explained\") +\n  scale_x_continuous(breaks = c(1:10))\n\n\n\n\n\n\n\n\nNext, we can look at cumulative % explained\n\npca_var %&gt;% \n  filter(terms == \"cumulative percent variance\") %&gt;% \n  ggplot(aes(x = component, y = value)) +\n  geom_col() + \n  geom_text(aes(x = component, y = value+2, label = round(value, 1)), color = \"red\") +\n  theme_bw() +\n  ylab(\"Cumulative percent variation explained\") +\n  scale_x_continuous(breaks = c(1:10))\n\n\n\n\n\n\n\n\nAnd now a more traditional scree plot:\n\npca_var %&gt;% \n  filter(terms == \"variance\") %&gt;% \n  ggplot(aes(x = component, y = value)) +\n  geom_point() + \n  geom_line() + \n  scale_x_continuous(breaks = c(1:10)) +\n  theme_minimal() +\n  ylab(\"Variance (Eigenvalue?)\")\n\n\n\n\n\n\n\n\n\n\n11.6.4 Select PCs to keep\nFor this round, I am keeping the first 5 PCs. This will always be a somewhat subjective decision, something noted by both Jolliffe & Cadima (2016) and the helpfully detailed climate zone paper by Fovell & Fovell 1993 that walks through their process and comparison of 3 vs. 5 PCs for their modeling scenario.\nKeeping the first 5 PCs accounts for 90% of the variation in my dataset. It also happens to be the number at which all of my original variables have been loaded on at least one PC (PC5 is the first time we see AWC loaded at all).\nMight be interesting, like Fovell & Fovell (1993), to do an additional version with 7 PCs. That gets us at &gt;95% variation accounted for.\n\ndat_for_kmeans &lt;- pca_results %&gt;% \n  select(mukey, PC01, PC02, PC03, PC04, PC05)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#sec-mod-opt",
    "href": "22-pca-kmeans.html#sec-mod-opt",
    "title": "11  Do PCA before k-means",
    "section": "11.7 Set model options",
    "text": "11.7 Set model options\n\n# writing a custom function here so I can be explicit \n# about the options I'm choosing, and also use within the \n# list-col framework I set up with map() below. \nkm_spec &lt;- function(nclust){\n  \n  tidyclust::k_means(num_clusters = nclust) %&gt;%\n    parsnip::set_engine(engine = \"stats\",\n               nstart = 10, # 1 is default, &gt;1 recommended\n               algorithm = \"Hartigan-Wong\", # H-W is default\n               iter.max = 20) # default is 10, wasn't always enough\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#set-up-data-structure-and-kmeans-recipe",
    "href": "22-pca-kmeans.html#set-up-data-structure-and-kmeans-recipe",
    "title": "11  Do PCA before k-means",
    "section": "11.8 Set up data structure and kmeans recipe",
    "text": "11.8 Set up data structure and kmeans recipe\nHere I set up a dataframe that will catch my modeling results in list columns of the different model objects and return values. The first column I define specifies the range of different cluster sizes (k) that we will try.\nI also set up my recipe here, which is much simpler compared to the original version because all of my data has been processed already before running it through k-means. Per the reading I did on 2023-01-11, especially Green & Krieger (1995) and Schaffer & Green (1998), I’m not further standardizing my component scores.\n\ntry_clusts &lt;- c(2:20)\n\nkm_df &lt;- data.frame(n_clust = try_clusts)\n\nkm_rec &lt;- recipe(~., data = dat_for_kmeans) %&gt;% \n  update_role(mukey, new_role = \"ID\")\n\nkm_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 5\nID:        1",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#specify-model-for-each-value-of-k",
    "href": "22-pca-kmeans.html#specify-model-for-each-value-of-k",
    "title": "11  Do PCA before k-means",
    "section": "11.9 Specify model (for each value of k)",
    "text": "11.9 Specify model (for each value of k)\nFor each unique value of k (2-20), this returns a model specification object (in the kmeans_spec column) based on the custom function I wrote above. The model specification has all the options set about how we want the algorithm to run (methods, number of starts, etc.). We need a different one for each value of k.\nThe kmeans_wflow column here holds our workflow objects. These objects combine our model specification (from kmeans_spec) with the data recipe (preprocessor) we made above (rec_spec, is same for all models).\n\n# for each unique value of clusters (2:20), returns a model\n# specification (kmeans_spec) and a workflow (kmeans_wflow) \n# note that the workflow \nkm_df &lt;- km_df %&gt;%\n  mutate(\n    kmeans_spec = map(n_clust, ~ km_spec(nclust = .x)),\n    kmeans_wflow = map(kmeans_spec,\n                       ~ workflow(\n                         preprocessor = km_rec, spec = .x\n                       ))\n  )\n\n# our current data structure\nhead(km_df, n=3L )\n\n\n  \n\n\n# take a look at an example workflow\nkm_df$kmeans_wflow[3]\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = nclust\n\nEngine-Specific Arguments:\n  nstart = 10\n  algorithm = Hartigan-Wong\n  iter.max = 20\n\nComputational engine: stats",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#fit-the-models",
    "href": "22-pca-kmeans.html#fit-the-models",
    "title": "11  Do PCA before k-means",
    "section": "11.10 Fit the models",
    "text": "11.10 Fit the models\nAll the steps above were related to specifying different aspects of this model. Now we can actually fit the models.\nSome troubleshooting here:\n\nStarted by specifying tidyclust::fit() but something weird was happening where my step_normalize() wasn’t included in the pre-processor recipe when I looked at the fitted model object.\nIf I specify parsnip::fit() , then step_normalize() is included and the values of the cluster centroids are in the expected ranges (centered, scaled).\nI also tried this without explicitly specifying the package (so just fit() ) and it worked as expected.\n\n\n# make a quiet version of fit(), so we can capture results \n# and any warning messages from the models \n# see troubleshooting notes below\nquiet_fit &lt;- purrr::quietly(.f = parsnip::fit)\n\nset.seed(4) # for reproducibility \nkm_fit_df &lt;- km_df %&gt;%\n  mutate(km_result = map(.x = kmeans_wflow,\n                      .f = quiet_fit,\n       # data comes after .f b/c not vectorized over            \n                      data = dat_for_kmeans),\n       km_fit = map(km_result, ~pluck(.x, 'result')),\n       warn = map(km_fit, ~pluck(.x, 'warnings')),\n       msg = map(km_fit, ~pluck(.x, 'messages')),\n       n_iter = map_dbl(km_fit, \n                      ~pluck(.x, 'fit', 'fit', 'fit', 'iter' ))) \n         \n\n# check out current data structure\nhead(km_fit_df, n = 3L)\n\n\n  \n\n\n# don't need anymore, cleaning up\nrm(km_df)\n\n\n11.10.1 View messages & warnings\nWe can look at any warnings or messages from the modeling process:\n\nkm_fit_df %&gt;% \n  select(n_clust, warn, msg, n_iter)\n\n\n  \n\n\n\n\n\n11.10.2 Look at one fit object\nAs an example, these are what the fitted objects look like.\nNOTE the clustering vector here is using the cluster numbers directly from kmeans(). tidyclust assigns names like “Cluster_1”, “Cluster_2” etc. , but the numbers do NOT necessarily match with what kmeans() returns. The CLUSTERINGS are the same, but the numbers are not necessarily so. So 2 in this “Clustering Vector” below is NOT necessarily equal to tidyclust “Cluster_2” that you might get by using the extract_cluster_assignment function. To keep things consistent, I’m always using the cluster names assigned by tidyclust.\n\nexamp_fit &lt;- km_fit_df$km_result[[4]][['result']]\n\nexamp_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-means clustering with 5 clusters of sizes 1463, 431, 1809, 1096, 2073\n\nCluster means:\n        PC01       PC02        PC03         PC04       PC05\n1  1.5136139 -1.1675498 -0.04287291 -1.011715005 -0.1041855\n2  1.7443425 -2.6232190 -0.74284903  2.312509274 -0.1599810\n3  1.6349352  1.1528420 -0.02833620  0.175076730  0.2152876\n4 -3.7357281 -0.3166354 -0.15275027 -0.001684125  0.4859401\n5 -0.8825222  0.5307642  0.29019078  0.081321539 -0.3379982\n\nClustering vector:\n   [1] 4 4 4 5 5 4 4 4 4 4 5 4 4 5 4 3 5 5 4 4 5 5 4 4 5 4 4 4 4 4 5 5 4 4 4 4 4\n  [38] 5 5 5 4 4 4 4 4 5 4 4 4 4 4 5 5 5 3 1 1 5 3 3 3 3 3 2 4 4 4 4 1 1 3 3 4 4\n  [75] 1 4 1 5 5 5 3 3 3 2 1 4 4 4 4 4 4 4 3 4 4 1 5 4 4 4 3 3 1 1 1 1 4 4 5 5 5\n [112] 4 4 4 3 1 4 4 4 4 1 3 3 3 3 4 4 4 4 5 5 4 4 4 4 5 4 5 1 5 4 4 4 4 4 4 1 4\n [149] 4 4 1 5 5 3 2 5 2 4 5 5 5 4 1 1 1 1 5 5 4 4 4 5 5 4 4 5 5 1 1 1 5 3 5 5 3\n [186] 1 1 3 3 3 5 5 1 3 5 5 5 5 4 4 4 4 4 4 4 5 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1\n [223] 1 3 4 4 3 3 3 1 4 4 4 3 3 4 1 5 3 3 4 5 1 1 5 3 4 3 3 2 4 4 4 5 2 5 4 4 1\n [260] 4 1 4 4 4 4 5 4 4 5 4 4 4 4 4 4 4 4 5 5 5 5 4 4 5 5 4 4 4 5 4 4 4 4 5 5 5\n [297] 5 5 4 4 4 4 5 4 5 4 4 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4\n [334] 5 5 5 2 3 1 1 2 2 2 2 1 2 1 1 3 3 1 1 3 2 1 3 3 1 1 2 5 1 3 1 1 5 2 1 1 3\n [371] 5 5 3 5 5 3 3 2 5 3 4 4 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 1 3 3 3 1 1 1\n [408] 1 1 3 5 5 3 3 3 3 5 5 5 5 3 5 3 5 5 1 1 1 3 5 3 5 5 5 3 3 3 4 4 4 3 3 4 5\n [445] 5 3 3 3 3 3 1 3 3 3 3 1 4 3 4 4 5 3 3 3 3 5 3 3 3 1 1 3 3 3 3 3 1 3 3 3 3\n [482] 1 3 5 5 5 1 5 5 3 3 5 3 3 3 1 5 3 3 3 1 3 3 1 1 1 3 3 3 1 3 3 3 3 3 3 3 1\n [519] 5 5 1 1 1 3 3 3 3 3 4 4 5 5 3 3 3 5 5 3 3 5 5 1 3 5 1 1 5 4 5 1 1 2 5 4 5\n [556] 5 1 5 1 5 1 1 1 1 5 5 1 1 1 1 1 3 1 5 1 5 3 3 1 5 1 1 1 3 1 3 1 1 1 5 5 5\n [593] 3 3 1 1 1 3 3 1 5 4 1 3 3 5 3 3 1 3 4 5 5 5 1 5 1 5 3 3 3 3 3 3 1 1 1 1 1\n [630] 5 3 3 3 3 5 5 5 5 3 5 5 5 1 3 3 3 5 5 5 5 3 3 3 3 5 1 1 1 4 4 4 4 5 5 5 5\n [667] 5 1 5 5 5 5 5 5 5 5 5 5 3 5 5 5 4 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 5 5 5 5 4\n [704] 4 5 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4 4 5 4 4 4 1 5 5 4 4 5 4 4 5 4\n [741] 4 4 4 4 5 4 4 5 4 4 1 1 1 1 1 5 1 5 5 4 1 3 5 3 5 1 3 1 1 3 5 3 3 5 3 3 1\n [778] 1 2 1 1 1 1 1 1 5 5 4 4 1 3 3 1 3 3 5 3 1 3 3 1 1 1 1 5 1 1 1 5 3 1 1 1 1\n [815] 3 1 1 1 1 1 1 1 1 5 5 5 3 3 5 5 3 3 3 1 3 3 1 1 5 5 5 5 5 4 1 1 1 3 2 3 3\n [852] 3 2 1 1 2 1 2 2 3 3 3 2 2 2 2 5 2 2 2 2 2 2 5 4 4 2 5 2 2 4 4 4 2 5 5 4 5\n [889] 5 1 3 3 3 1 3 1 3 1 2 1 1 3 3 2 4 2 5 1 1 3 3 4 1 1 1 1 1 2 2 2 5 2 2 5 2\n [926] 2 2 2 2 2 3 5 5 3 4 5 2 2 2 3 3 1 3 4 4 4 4 3 3 2 1 1 2 2 2 4 4 1 4 4 5 5\n [963] 5 3 1 3 1 4 4 4 3 3 1 1 5 5 4 4 1 5 3 4 5 4 4 4 5 1 1 5 4 4 4 4 3 3 3 2 4\n[1000] 4 5 3 2 5 2 5 4 1 4 4 4 4 4 1 5 5 1 1 5 5 5 5 1 3 3 3 1 1 1 5 1 5 5 3 1 1\n[1037] 1 1 3 3 1 5 1 1 1 5 5 1 3 3 3 3 4 4 1 3 3 3 3 3 3 3 1 5 5 1 1 3 3 3 1 5 5\n[1074] 1 3 3 5 3 1 3 3 1 1 1 5 5 1 1 1 1 3 3 3 3 3 3 3 3 1 1 1 3 1 1 3 3 1 1 1 5\n[1111] 5 5 3 5 3 3 1 1 1 3 3 1 3 3 3 4 1 3 3 3 4 4 4 4 4 1 4 5 4 4 4 4 4 4 4 4 4\n[1148] 4 4 4 3 4 4 4 4 5 4 4 4 5 3 5 4 4 4 4 4 4 4 4 3 5 1 5 4 3 5 3 5 5 4 4 4 4\n[1185] 4 4 4 5 5 5 5 5 4 5 4 4 4 5 3 5 5 5 5 4 4 4 4 4 3 3 4 4 4 5 5 3 3 5 3 5 5\n[1222] 3 3 1 5 5 4 4 4 5 5 3 5 5 5 5 1 1 5 5 5 5 5 5 3 3 3 3 1 5 5 5 5 5 5 4 4 4\n[1259] 1 5 5 5 5 5 5 5 5 4 4 1 1 1 4 3 5 3 5 5 5 4 5 5 5 5 3 3 3 5 5 5 3 5 5 5 5\n[1296] 4 4 4 4 1 3 3 3 3 3 5 5 5 1 5 5 5 3 5 3 1 1 4 4 4 5 5 4 5 2 2 5 3 1 3 5 1\n[1333] 1 3 3 1 3 1 3 1 1 1 3 2 1 5 5 1 1 1 1 1 1 5 5 5 5 5 4 5 5 5 5 5 5 5 5 5 5\n[1370] 3 3 1 3 1 3 3 4 4 4 4 5 5 5 5 5 3 1 3 3 5 3 3 3 3 4 5 5 5 5 5 5 5 1 5 5 3\n[1407] 3 3 3 3 3 1 1 1 1 3 1 3 3 1 5 1 1 3 1 1 3 1 1 5 5 3 3 3 1 3 3 3 3 3 3 1 1\n\n...\nand 156 more lines.\n\n\nA nicer way to look at the results is by accessing specific parts of the fitted model object, as below.\n\n# some basic model metrics\nglance(examp_fit)\n\n\n  \n\n\n# centroid data (transformed/standardized scale)\ncentroids &lt;- tidyclust::extract_centroids(examp_fit)\ncentroids\n\n\n  \n\n\n# helpful to add to future plots for examining indiv. clusters\nclust_stat &lt;- tidyclust::sse_within(examp_fit)\nclust_stat",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#sec-mod-metrics",
    "href": "22-pca-kmeans.html#sec-mod-metrics",
    "title": "11  Do PCA before k-means",
    "section": "11.11 Model metrics",
    "text": "11.11 Model metrics\nSee also section 7.5 in the Chapter by Tan et al. for more about cluster evaluation.\n\n11.11.1 Extract metrics\n\nmetrics_df &lt;- km_fit_df %&gt;%\n  mutate(\n    # tot_sse = total sum of squared error\n    tot_sse = map_dbl(km_fit, ~ sse_total_vec(.x)),\n    # tot_wss = sum of within-cluster sse\n    tot_wss = map_dbl(km_fit, ~sse_within_total_vec(.x)),\n    # sse ratio = wss / total sse, \n    sse_ratio = map_dbl(km_fit, ~sse_ratio_vec(.x))\n    )\n\nrm(km_fit_df)\n\nmetrics_simple &lt;- metrics_df %&gt;% \n  select(n_clust, tot_sse, tot_wss, sse_ratio)\n\nmetrics_simple\n\n\n  \n\n\nrm(pca_prep)\nrm(rec_spec)\n\n\n\n11.11.2 Plot Total WSS\nNot a clear “elbow” here, although by the time we get to 10-11 it does seem to be leveling off.\n\nmetrics_simple %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Compare values of k: looking for elbow\")\n\n\n\n\n\n\n\nmetrics_simple %&gt;% \n  filter(n_clust %in% c(2:12)) %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(1:12)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Zoom in a bit: looking for elbow\")\n\n\n\n\n\n\n\n\n\n\n11.11.3 Average Silhouette\nFrom the {tidyclust} documentation:\n\nAnother common measure of cluster structure is called the silhouette. The silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\n\nIn principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\nSee also pg. 581 in Tan2018 Chap 7 Cluster Analysis: Basic Concepts and Algorithms\n\nprepped_rec &lt;- prep(km_rec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \nbaked_df &lt;- bake(prepped_rec, new_data = NULL) %&gt;% \n  select(-mukey) \n\ndists &lt;- baked_df %&gt;% as.matrix() %&gt;% dist(method = \"euclidean\")\n\nsilh_df &lt;- metrics_df %&gt;% \n  mutate(avg_sil = map_dbl(km_fit, \n                       tidyclust::silhouette_avg_vec,\n                       dists = dists),\n         indiv_sil = map(km_fit, \n                         tidyclust::silhouette,\n                         dists = dists))\n\nindiv_sil_df &lt;- silh_df %&gt;% select(n_clust, indiv_sil) %&gt;% \n  unnest(indiv_sil) %&gt;% \n  mutate(across(.cols = c(cluster, neighbor),\n                .fns = as.character))\n\nwrite_csv(indiv_sil_df, \"data/pca_kmeans_point_silhouettes.csv\")\n\nrm(metrics_df)  \nrm(dists)\nrm(prepped_rec)\n\nHigher silhouette is better (means observations are closer to their centroids than to other observations). Seems to suggest that 4, 6, 8, 11 would be OK\n\nsilh_df %&gt;% \n  ggplot(aes(x = n_clust, y = avg_sil)) +\n  geom_point() + \n  geom_line() + \n  theme_bw() +\n  scale_x_continuous(breaks = c(1:20)) +\n  ggtitle(\"Overall Average Silhouette\") +\n  labs(subtitle = \"Higher is better, possible values [-1,1]\")\n\n\n\n\n\n\n\n\nCan also plot the individual silhouettes. For each clustering (model version), we have a silhouette value per observation in the dataset (n=6872). We also have the closest “neighbor” cluster, or the cluster that specific observation would belong to if its home cluster didn’t exist.\nHere’s an example of this data for the k=6 clustering. The\n\nneighbor_counts &lt;- indiv_sil_df %&gt;% \n  group_by(n_clust, cluster, neighbor) %&gt;% \n  count()  %&gt;% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c\"),\n         neighbor = str_replace(neighbor, \"Cluster_\", \"c\"))\n\nk6_neighbor_counts &lt;- neighbor_counts %&gt;% \n  filter(n_clust == 6)\n\n\nindiv_sil_df %&gt;% \n  mutate(across(.cols = c(cluster, neighbor),\n         ~str_replace(.x, \"Cluster_\", \"c\"))) %&gt;% \n  filter(n_clust == 6) %&gt;% \n  ggplot() + \n  geom_boxplot(aes(x = neighbor, y = sil_width)) +\n    geom_point(aes(x = neighbor, y = sil_width),\n             position = position_jitter(width = 0.1),\n             alpha = 0.2,\n             color = \"pink\") + \n  geom_text(data = k6_neighbor_counts,\n            aes(x = neighbor, y = 0.7, label = n),\n            color = \"blue\") +\n  facet_wrap(vars(cluster), scales = \"free_x\") +\n  theme_bw() +\n  ggtitle(\"k=6 silhouettes\")\n\n\n\n\n\n\n\n\n\nclust_sil_avgs &lt;- indiv_sil_df %&gt;% \n  group_by(cluster,\n           n_clust) %&gt;% \n  summarise(mean_sil = mean(sil_width),\n            sd_sil = sd(sil_width), \n            .groups = \"drop\")\n\nclust_sil_avgs %&gt;% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c0\"),\n         cluster = case_when(\n           cluster %in% c(\"c010\", \"c011\", \"c012\", \"c013\", \"c014\", \"c015\", \"c016\",\n                          \"c017\", \"c018\", \"c019\", \"c020\") ~ str_replace(cluster, \"c0\", \"c\"),\n           TRUE ~ cluster\n         )) %&gt;% \n  filter(n_clust %in% c(6:12)) %&gt;% \n  ggplot() +\n  geom_col(aes(y = cluster, x = mean_sil)) +\n  facet_wrap(vars(n_clust), scales = \"free_y\") + \n  ggtitle(\"Average silhouette width per cluster for k=6-12\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n11.11.4 Not used: Gap statistic\nFor fviz_nbclust(), first couple times running this, got Warning: Quick-TRANSFER stage steps exceeded maximum… Looking online, this seems to be a problem with the model not converging. I added some arguments here that are passed on to kmeans(), to make sure that the algorithm settings here match what I run above, including set.seed()\nContinued to get warnings, even though I’m using all the same settings as I use for kmeans up above. Not sure why this is, but I’m not going to spend any more time on it right now. Maybe see if getting the gap statistic through NbClust works better? (Later note: NbClust won’t be a good option either, I can’t alter important kmeans() settings in NbClust). Expect it will take a long time either way, consider running this in a separate script and pulling in the results.\n\nset.seed(4)\nfviz_gap_stat(x = baked_df, \n             FUNcluster = kmeans,\n             method = c(\"gap_stat\"),\n             k.max = 10, # only considering 2-10 clusters\n             nboot = 50, # default is 100\n             verbose = TRUE, \n             iter.max = 20, # passed to kmeans\n             nstart = 10 # passed to kmeans\n             )\n\n\n\n11.11.5 Calinski-Harabasz index\nNot used: {NbClust} , using {fpc} instead.\n\nFor Calinski-Harabasz index, higher values are better\nRealized after setting this up with NbClust that I don’t have the option to pass additional arguments to the kmeans function here. So I can’t make the algorithm settings exactly match my main clustering pipeline above (where I implement k-means using tidyclust and the tidymodels framework, and where I save the results for further analysis). This is a problem because I know from my original tests that I need to change the iter.max value to avoid non-convergence issues, and I also want to change nstart because nstart &gt;1 is typically known to be best practice (find citation for this).\n\n\n# keeping this here as a record, but I\"m NOT USING this function for the C-H index. \n\nnbc_indices &lt;- NbClust::NbClust(data = baked_df,\n                 distance = \"euclidean\",\n                 method = \"kmeans\",\n                 min.nc = 2,\n                 max.nc = 20,\n                 index = \"ch\") # Calinski and Harabasz\n\n# enframe turns a named vector into a dataframe\nch_index_vals &lt;- enframe(nbc_indices$All.index) %&gt;% \n  mutate(name = as.integer(name)) %&gt;% \n  rename(n_clust = name)\n\nTrying a different implementation of the Calinski-Harabasz index from the {fpc} package. This is preferred to the above approach, where I originally used the NbClust function from {NbClust} package because I can give this function my clustering generated above (NbClust does its own run of kmeans but I can’t customize it to keep it consistent with\n\n# calinhara wants an observations/variables matrix\n# as first argument (as opposed to a distance matrix)\nobsvar_mx &lt;- as.matrix(baked_df)\n\n# function to extract and modify tidyclust clusters\n# into a integer vector, which I will pass to calinhara()\ncreate_clust_vec &lt;- function(fit_obj){\n  \n  extract_cluster_assignment(fit_obj) %&gt;% \n    pull(.cluster) %&gt;% \n    str_replace(., \"Cluster_\", \"\") %&gt;% \n    as.integer()\n  \n}\n\n# apply function to extract clusterings as integer vectors\n# map to get a c-h index value for every value of k (2-20)\nch_metrics &lt;- silh_df %&gt;%\n  select(n_clust, km_fit) %&gt;%\n  mutate(\n    clustering_vec = map(km_fit, create_clust_vec),\n    ch_index = map_dbl(clustering_vec,\n                       ~ fpc::calinhara(x = obsvar_mx,\n                                        clustering = .x)\n    ))\n\nch_metrics %&gt;% \n  ggplot(aes(x = n_clust, y = ch_index)) + \n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  ylab(\"Calinski-Harabasz index\") + \n  ggtitle(\"Calinski-Harabasz\") +\n  labs(subtitle = \"Higher is better\") +\n  scale_x_continuous(breaks = c(2:20))\n\n\n\n\n\n\n\n\n\n\n11.11.6 Hopkins Statistic\nUsing the {hopkins} package for this. Citations included in the package documentation (also cite Tan et al., 2019 who give an example of using this for evaluating kmeans clusters).\n\nHopkins, B. and Skellam, J.G., 1954. A new method for determining the type of distribution of plant individuals. Annals of Botany, 18(2), pp.213-227.\nCross, G. R., and A. K. Jain. (1982). Measurement of clustering tendency. Theory and Application of Digital Control. Pergamon, 1982. 315-320.\n\nAnd a third citation, helpful illustrations:\n\nLawson, R. G., & Jurs, P. C. (1990). New index for clustering tendency and its application to chemical problems. Journal of Chemical Information and Computer Sciences, 30(1), 36–41. https://doi.org/10.1021/ci00065a010\n\nApparently {factoextra} also has a Hopkins statistic, try that here too. (It takes a very long time to run, but returns 0.93, similar to 0.99 returned by hopkins::hopkins()\n\nset.seed(4)\nhstat &lt;- hopkins(X = baked_df,\n                 # default, number of rows to sample from the df\n                 m = ceiling(nrow(baked_df)/10), \n                 # default, dimension of the data\n                 d = ncol(baked_df),\n                 # default, kth nearest neighbor to find\n                 k = 1) \nhstat\n\n[1] 0.9999673\n\nhopkins.pval(x = hstat,\n             # this is the default for hopkins() above\n             n = ceiling(nrow(baked_df)/10)) \n\n[1] 0\n\n# below gives 0.9331899 as the result\n# which agrees with above\n# commenting out because it takes a very long time to run\n# factoextra::get_clust_tendency(data = baked_df,\n#                                n = 687, \n#                                graph = FALSE)\n\n\n\n11.11.7 WSS and Silhouette metrics on one plot\n\n#| echo: false\n\nsil_totwss &lt;- silh_df %&gt;% \n  select(n_clust, avg_sil, tot_wss, tot_sse, sse_ratio)\n\nch &lt;- ch_metrics %&gt;% \n  select(n_clust, ch_index)\n\nmet_combined &lt;- left_join(sil_totwss, ch, by = \"n_clust\")\n\nwrite_csv(met_combined, \"data/pca_kmeans_cluster_metrics.csv\")\n\nmet2 &lt;- met_combined %&gt;% \n  pivot_longer(cols = -c('n_clust'), names_to = \"metric\",\n               values_to = \"value\")\n\nmet2 %&gt;% \n  ggplot(aes(x = n_clust, y = value)) + \n  geom_point() + \n  geom_line() +\n  scale_x_continuous(breaks = c(2:20)) + \n  facet_wrap(vars(metric), ncol = 1, scales = \"free\") +\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#save-model-fits",
    "href": "22-pca-kmeans.html#save-model-fits",
    "title": "11  Do PCA before k-means",
    "section": "11.12 Save model fits",
    "text": "11.12 Save model fits\nWill save these as Rdata so I can call them up and investigate the cluster centroids more closely in the next chapter.\n\npca_mods &lt;- silh_df %&gt;% \n  select(n_clust, km_fit)\n\nsave(pca_mods, file = \"data/fitted_pca_kmeans_mods.RData\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "22-pca-kmeans.html#save-cluster-assignments",
    "href": "22-pca-kmeans.html#save-cluster-assignments",
    "title": "11  Do PCA before k-means",
    "section": "11.13 Save cluster assignments",
    "text": "11.13 Save cluster assignments\nFor each version of the model (each value of k, different numbers of clusters), a MUKEY is assigned to a specific cluster. Here, I’m pulling that data, shaping it into one dataframe (one row per MUKEY, cluster assingments in separate columns). I’m also adding back in the soil property data so we can use this in the next step when evaluating different cluster sizes.\n\nclust_assign_df &lt;- pca_mods %&gt;% \n  mutate(clust_assign = map(km_fit, ~augment(.x, new_data = dat_for_kmeans)),\n         mukey_clust = map(clust_assign, ~select(.x, mukey, .pred_cluster)))\n\n\nassign_mukey_df &lt;- clust_assign_df %&gt;% \n  select(n_clust, mukey_clust) %&gt;% \n  unnest(mukey_clust) %&gt;% pivot_wider(names_from = n_clust, values_from = .pred_cluster, names_prefix = \"k_\")\n\n# drop k_6 column from d (it was from orig k-means clusters,\n# used above for viz purposes only)\n\nsoil_props &lt;- d %&gt;% select(-k_6) \n\nclust_props &lt;- full_join(soil_props, assign_mukey_df, by = \"mukey\")\n\nwrite_csv(clust_props, \"data/pca_mukey_cluster_assignments_and_soilprops.csv\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Do PCA before k-means</span>"
    ]
  },
  {
    "objectID": "23-pca-kmeans-finalists.html",
    "href": "23-pca-kmeans-finalists.html",
    "title": "12  Clustering Finalists",
    "section": "",
    "text": "12.1 Overview\nExploring some of the clustering results from the version of the models where I did PCA as a data pre-processing step. Visualizing some of the results here.\n(“reduced”) PCA before k-means: silhouette suggests 4, 6, 8, 11. C-H index 4-8 would be similar, 9-11 would be similar.\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(stringr)\nlibrary(gtsummary)\nlibrary(terra)\n# input files for spatial exploration of clusters \n\n# generated in 22-pca-kmeans.qmd\nprop1 &lt;- read_csv(\"data/pca_mukey_cluster_assignments_and_soilprops.csv\")\n\nk4all &lt;- rast(\"D:/big-files-backup/ch03-sh-groups/clust4_pca.tif\")\nk6all &lt;- rast(\"D:/big-files-backup/ch03-sh-groups/clust6_pca.tif\")\nk8all &lt;- rast(\"D:/big-files-backup/ch03-sh-groups/clust8_pca.tif\")\nk11all &lt;- rast(\"D:/big-files-backup/ch03-sh-groups/clust11_pca.tif\")\n# input files for taxonomic exploration of clusters \n\n# generated ch7 map unit aggregation\nmu &lt;- read_csv(\"data/mu_weighted_soil_props.csv\")\n\ncmp_lookup &lt;- read_csv(\"data/key_cokey_mukey_complete_cases_include.csv\")\n\ncmp_details &lt;- read_csv(\"data/component_list.csv\")\n\nmunames &lt;- read_csv(\"data/target_mapunit_table.csv\") %&gt;% \n  select(mukey, muname, muacres)\n\n# more rows than mu b/c there are multiple cmps\n# included in some mus\nmu_cmp &lt;- left_join(mu, cmp_lookup, by = \"mukey\")\n\n# adds munames and muacres\nmu_cmp_nm &lt;- left_join(mu_cmp, munames, by = \"mukey\")\n\n# one row for each component (there can be multiple components\n# for an included mukey), now includes munames and component details\nmu_detail &lt;- left_join(mu_cmp_nm, cmp_details, by = c(\"cokey\",\"mukey\"))\n\n# for translating MUKEY from gSSURGO to my shortened version\ncwalk &lt;- aoi_mu &lt;- read.delim(\"data/gSSURGO_MN/mukey_new_crosswalk.txt\", sep = \",\") %&gt;% \n  select(MUKEY, MUKEY_New, Count)\nCluster_0  Cluster_1  Cluster_2  Cluster_3  Cluster_4  Cluster_5  Cluster_6 \n \"#FFF8DC\"  \"#FF5A5F\"  \"#FFB400\"  \"#007A87\"  \"#8CE071\"  \"#7B0051\"  \"#00D1C1\" \n Cluster_7  Cluster_8  Cluster_9 Cluster_10 Cluster_11 Cluster_12 \n \"#FFAA91\"  \"#B4A76C\"  \"#9CA299\"  \"#565A5C\"  \"#00A04B\"  \"#E54C20\"",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Clustering Finalists</span>"
    ]
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#clusters",
    "href": "23-pca-kmeans-finalists.html#clusters",
    "title": "12  Clustering Finalists",
    "section": "12.2 4 clusters",
    "text": "12.2 4 clusters\n\n12.2.1 Summary table\n\ncreate_summary_table(kcol = \"k_4\", prop_df = prop1)\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 2,0221\nCluster_2, N = 2,8211\nCluster_3, N = 1,5441\nCluster_4, N = 4851\n\n\n\n\nclaytotal\n9 (4)\n24 (8)\n24 (8)\n25 (12)\n\n\nom\n2.27 (1.19)\n5.87 (10.03)\n5.56 (4.31)\n4.80 (1.65)\n\n\ncec7\n9 (4)\n24 (19)\n22 (10)\n22 (10)\n\n\ndbthirdbar\n1.47 (0.10)\n1.30 (0.19)\n1.31 (0.20)\n1.28 (0.09)\n\n\nec\n0.01 (0.08)\n0.00 (0.03)\n0.00 (0.00)\n1.28 (0.52)\n\n\nph1to1h2o\n6.46 (0.49)\n6.60 (0.35)\n7.54 (0.31)\n7.53 (0.47)\n\n\ncaco3\n0.4 (1.4)\n0.2 (0.8)\n9.4 (5.0)\n7.8 (6.2)\n\n\nlep\n1.19 (0.47)\n2.88 (1.75)\n2.89 (1.71)\n3.53 (3.10)\n\n\nksat\n53 (36)\n9 (6)\n11 (9)\n12 (15)\n\n\nawc\n0.13 (0.03)\n0.21 (0.04)\n0.20 (0.03)\n0.19 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\n12.2.2 Violin plots\n\nplot_cluster_violins(kcol = k_4, claytotal, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_4, om, prop_df = prop1) + scale_y_log10()\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_4, ph1to1h2o, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_4, caco3, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_4, ec, prop_df = prop1)\n\n\n\n\n\n\n\n\n\n\n12.2.3 Maps\n\nk4all_colors &lt;- create_mapcol_df(4)\n\nplot(k4all, col = k4all_colors, plg=list(legend = c(\"C0\", \"C1\", \"C2\", \"C3\", \"C4\")), main = \"k=4 all variables\")\n\n\n\n12.2.4 Taxonomy\n\n\n\n\n\n\n\n\n\n\n\n12.2.5 Visualize clusters in 2D\n\nprop1 %&gt;% \n  ggplot() + \n  geom_point(aes(x = claytotal, y = ph1to1h2o, color = k_4)) +\n  theme_bw()\n\n\n\n\n\n\n\nprop1 %&gt;% \n  ggplot() + \n  geom_point(aes(x = claytotal, y = om, color = k_4)) +\n  theme_bw() + \n  scale_y_log10()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Clustering Finalists</span>"
    ]
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#clusters-1",
    "href": "23-pca-kmeans-finalists.html#clusters-1",
    "title": "12  Clustering Finalists",
    "section": "12.3 6 clusters",
    "text": "12.3 6 clusters\n\n12.3.1 Summary table\n\ncreate_summary_table(kcol = \"k_6\", prop_df = prop1)\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 1,1291\nCluster_2, N = 2,1581\nCluster_3, N = 1,6641\nCluster_4, N = 1,4361\nCluster_5, N = 4201\nCluster_6, N = 651\n\n\n\n\nclaytotal\n7 (2)\n16 (4)\n30 (8)\n24 (7)\n23 (11)\n7 (4)\n\n\nom\n1.83 (0.99)\n3.20 (1.53)\n5.32 (2.00)\n5.35 (3.11)\n4.80 (1.71)\n69.79 (15.21)\n\n\ncec7\n7 (2)\n15 (4)\n26 (6)\n22 (7)\n20 (9)\n143 (28)\n\n\ndbthirdbar\n1.50 (0.10)\n1.40 (0.09)\n1.29 (0.11)\n1.31 (0.19)\n1.28 (0.09)\n0.27 (0.15)\n\n\nec\n0.01 (0.10)\n0.00 (0.02)\n0.04 (0.20)\n0.00 (0.00)\n1.33 (0.54)\n0.00 (0.00)\n\n\nph1to1h2o\n6.41 (0.53)\n6.51 (0.38)\n6.73 (0.35)\n7.56 (0.32)\n7.63 (0.39)\n6.61 (0.42)\n\n\ncaco3\n0.5 (1.5)\n0.3 (1.1)\n0.4 (1.2)\n9.8 (4.9)\n8.9 (5.8)\n1.5 (2.4)\n\n\nlep\n1.08 (0.50)\n1.50 (0.54)\n4.21 (1.94)\n2.74 (1.46)\n3.00 (2.62)\n0.59 (0.31)\n\n\nksat\n73 (36)\n18 (11)\n6 (3)\n11 (9)\n14 (16)\n24 (5)\n\n\nawc\n0.11 (0.02)\n0.18 (0.03)\n0.20 (0.02)\n0.19 (0.02)\n0.19 (0.03)\n0.40 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\n12.3.2 Violin plots\n\nplot_cluster_violins(kcol = k_6, claytotal, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_6, om, prop_df = prop1) + scale_y_log10()\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_6, ph1to1h2o, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_6, caco3, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_6, ec, prop_df = prop1)\n\n\n\n\n\n\n\n\n\n\n12.3.3 Maps\n\nk6all_colors &lt;- create_mapcol_df(6)\n\nplot(k6all, col = k6all_colors, plg=list(legend = c(\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\")), main = \"k=6 all variables\")\n\n\n\n12.3.4 Taxonomy\n\n\n\n\n\n\n\n\n\n\n\n12.3.5 Visualize clusters in 2D\n\nprop1 %&gt;% \n  ggplot() + \n  geom_point(aes(x = claytotal, y = ph1to1h2o, color = k_6)) +\n  theme_bw()\n\n\n\n\n\n\n\nprop1 %&gt;% \n  ggplot() + \n  geom_point(aes(x = claytotal, y = om, color = k_6)) +\n  theme_bw() + \n  scale_y_log10()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Clustering Finalists</span>"
    ]
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#clusters---all-variables-included",
    "href": "23-pca-kmeans-finalists.html#clusters---all-variables-included",
    "title": "12  Clustering Finalists",
    "section": "12.4 8 clusters - all variables included",
    "text": "12.4 8 clusters - all variables included\n\n12.4.1 Summary table\n\ncreate_summary_table(kcol = \"k_8\", prop_df = prop1)\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 9131\nCluster_2, N = 1,6291\nCluster_3, N = 1,3511\nCluster_4, N = 6281\nCluster_5, N = 9711\nCluster_6, N = 3821\nCluster_7, N = 9331\nCluster_8, N = 651\n\n\n\n\nclaytotal\n6 (2)\n22 (4)\n12 (3)\n30 (6)\n34 (7)\n22 (9)\n19 (5)\n7 (4)\n\n\nom\n1.66 (0.82)\n3.77 (1.79)\n2.94 (1.35)\n7.32 (3.34)\n6.03 (1.83)\n4.74 (1.40)\n3.90 (1.88)\n69.79 (15.21)\n\n\ncec7\n6 (2)\n19 (4)\n12 (3)\n28 (7)\n30 (6)\n19 (8)\n17 (4)\n143 (28)\n\n\ndbthirdbar\n1.50 (0.10)\n1.35 (0.07)\n1.44 (0.09)\n1.21 (0.24)\n1.26 (0.13)\n1.28 (0.09)\n1.39 (0.08)\n0.27 (0.15)\n\n\nec\n0.01 (0.11)\n0.01 (0.11)\n0.00 (0.03)\n0.01 (0.12)\n0.08 (0.27)\n1.36 (0.56)\n0.00 (0.00)\n0.00 (0.00)\n\n\nph1to1h2o\n6.38 (0.51)\n6.51 (0.32)\n6.53 (0.43)\n7.66 (0.25)\n6.81 (0.35)\n7.67 (0.36)\n7.45 (0.35)\n6.61 (0.42)\n\n\ncaco3\n0.3 (1.2)\n0.1 (0.5)\n0.3 (1.1)\n10.9 (4.9)\n0.5 (1.3)\n9.4 (5.7)\n8.4 (4.8)\n1.5 (2.4)\n\n\nlep\n1.06 (0.51)\n2.32 (0.95)\n1.26 (0.46)\n3.91 (1.81)\n5.05 (2.16)\n2.58 (1.95)\n2.00 (0.92)\n0.59 (0.31)\n\n\nksat\n81 (34)\n10 (3)\n27 (14)\n6 (3)\n4 (3)\n15 (16)\n14 (11)\n24 (5)\n\n\nawc\n0.10 (0.02)\n0.21 (0.02)\n0.16 (0.03)\n0.20 (0.02)\n0.19 (0.02)\n0.19 (0.03)\n0.19 (0.03)\n0.40 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\n\n12.4.2 Violin plots\n\nplot_cluster_violins(kcol = k_8, claytotal, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_8, om, prop_df = prop1) + scale_y_log10()\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_8, ph1to1h2o, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_8, caco3, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_8, ec, prop_df = prop1)\n\n\n\n\n\n\n\n\n\n\n12.4.3 Maps\n\nk8all_colors &lt;- create_mapcol_df(8)\n\nplot(k8all, col = k8all_colors, plg=list(legend = c(\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\")), main = \"k=8 all variables\")\n\n\n\n12.4.4 Taxonomy",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Clustering Finalists</span>"
    ]
  },
  {
    "objectID": "23-pca-kmeans-finalists.html#clusters---all-variables",
    "href": "23-pca-kmeans-finalists.html#clusters---all-variables",
    "title": "12  Clustering Finalists",
    "section": "12.5 11 clusters - all variables",
    "text": "12.5 11 clusters - all variables\n\n12.5.1 Summary table\n\ncreate_summary_table(kcol = \"k_11\", prop_df = prop1)\n\n\n\n\n\n\n\n\nCharacteristic\nCluster_1, N = 4991\nCluster_10, N = 2801\nCluster_11, N = 651\nCluster_2, N = 4701\nCluster_3, N = 1,5311\nCluster_4, N = 4731\nCluster_5, N = 9031\nCluster_6, N = 6181\nCluster_7, N = 9191\nCluster_8, N = 2041\nCluster_9, N = 9101\n\n\n\n\nclaytotal\n6 (2)\n18 (7)\n7 (4)\n7 (2)\n22 (3)\n13 (3)\n12 (3)\n30 (6)\n33 (6)\n35 (12)\n20 (5)\n\n\nom\n1.28 (0.64)\n4.71 (1.44)\n69.79 (15.21)\n2.39 (1.05)\n3.61 (1.42)\n4.52 (2.55)\n2.39 (0.84)\n7.36 (3.35)\n6.06 (1.74)\n4.92 (1.91)\n3.85 (1.81)\n\n\ncec7\n5 (1)\n16 (6)\n143 (28)\n8 (2)\n19 (3)\n14 (6)\n12 (3)\n28 (7)\n29 (5)\n29 (8)\n18 (4)\n\n\ndbthirdbar\n1.55 (0.06)\n1.28 (0.09)\n0.27 (0.15)\n1.43 (0.10)\n1.36 (0.07)\n1.34 (0.08)\n1.48 (0.07)\n1.20 (0.24)\n1.27 (0.13)\n1.26 (0.09)\n1.39 (0.08)\n\n\nec\n0.00 (0.00)\n1.45 (0.60)\n0.00 (0.00)\n0.03 (0.16)\n0.00 (0.00)\n0.01 (0.08)\n0.00 (0.00)\n0.00 (0.00)\n0.00 (0.00)\n1.05 (0.22)\n0.00 (0.00)\n\n\nph1to1h2o\n6.11 (0.36)\n7.80 (0.21)\n6.61 (0.42)\n6.85 (0.44)\n6.51 (0.32)\n6.66 (0.37)\n6.40 (0.42)\n7.66 (0.25)\n6.78 (0.34)\n7.16 (0.48)\n7.46 (0.35)\n\n\ncaco3\n0.0 (0.5)\n11.3 (5.1)\n1.5 (2.4)\n1.3 (2.2)\n0.1 (0.5)\n0.3 (1.0)\n0.2 (0.7)\n10.9 (5.0)\n0.4 (1.1)\n3.1 (4.0)\n8.5 (4.8)\n\n\nlep\n1.08 (0.53)\n1.83 (0.97)\n0.59 (0.31)\n1.05 (0.50)\n2.34 (0.92)\n0.98 (0.53)\n1.41 (0.33)\n3.84 (1.74)\n4.77 (1.73)\n5.90 (3.45)\n2.03 (0.93)\n\n\nksat\n88 (37)\n18 (18)\n24 (5)\n72 (27)\n9 (2)\n23 (13)\n26 (12)\n6 (3)\n4 (3)\n4 (4)\n14 (10)\n\n\nawc\n0.10 (0.02)\n0.19 (0.03)\n0.40 (0.03)\n0.11 (0.01)\n0.21 (0.02)\n0.18 (0.03)\n0.15 (0.02)\n0.20 (0.02)\n0.19 (0.02)\n0.18 (0.03)\n0.19 (0.03)\n\n\n\n1 Mean (SD)\n\n\n\n\n\n\n\n\n\n\nrank_clusters(k_11, claytotal, prop1)\n\n\n  \n\n\nrank_clusters(k_11, ph1to1h2o, prop1)\n\n\n  \n\n\nrank_clusters(k_11, om, prop1)\n\n\n  \n\n\nrank_clusters(k_11, caco3, prop1)\n\n\n  \n\n\n\n\n\n12.5.2 Violin plots\n\nplot_cluster_violins(kcol = k_11, claytotal, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_11, om, prop_df = prop1) + scale_y_log10()\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_11, ph1to1h2o, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_11, caco3, prop_df = prop1)\n\n\n\n\n\n\n\nplot_cluster_violins(kcol = k_11, ec, prop_df = prop1)\n\n\n\n\n\n\n\n\n\n\n12.5.3 Taxonomy",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Clustering Finalists</span>"
    ]
  },
  {
    "objectID": "24-compare-kmeans.html",
    "href": "24-compare-kmeans.html",
    "title": "13  Comparing k-means scenarios",
    "section": "",
    "text": "13.1 Which clusters are we comparing?\nOriginal k-means: silhouette suggests 6, 9-11. C-H suggests 6, 11. Pairwise comparisons suggest 6, 8, or 11\nPCA before k-means: silhouette suggests 4, 6, 8, 11. C-H 4-8 would be similar, 9-11 would be similar",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparing k-means scenarios</span>"
    ]
  },
  {
    "objectID": "24-compare-kmeans.html#compare-model-metrics",
    "href": "24-compare-kmeans.html#compare-model-metrics",
    "title": "13  Comparing k-means scenarios",
    "section": "13.2 Compare model metrics",
    "text": "13.2 Compare model metrics\n\norig_metrics &lt;- read_csv(\"data/kmeans_cluster_metrics.csv\") %&gt;% \n  mutate(version = \"original\")\n\npca_metrics &lt;- read_csv(\"data/pca_kmeans_cluster_metrics.csv\") %&gt;% \n  mutate(version = \"pca\")\n\nall_metrics &lt;- bind_rows(orig_metrics, pca_metrics)\n\n\n13.2.1 Sum of within-cluster SSE\nIt doesn’t make sense to plot these two on the same graph, because the errors are in different units (z-score for the original, PCA score for the PCA).\n\n\n\n\n\n\n\n\n\n\n\n13.2.2 Average Silhouette\nFrom the {tidyclust} documentation:\n\nAnother common measure of cluster structure is called the silhouette.\nThe silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\nIn principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\n\n\n\n\n\n\n\n\n\n\n\n13.2.3 Comparing Calinski-Harabasz index\nHigher values are better. This is also known as the “Variance Ratio Criterion,”, which is how Calinski & Harabasz refer to it in their 1974 paper introducing it. The paper title is “A dendrite method for cluster analysis”\nNice summary from PyShark:\n\n“The Calinski-Harabasz index (also known as the Variance Ratio Criterion) is calculated as a ratio of the sum of inter-cluster dispersion and the sum of intra-cluster dispersion for all clusters (where the dispersion is the sum of squared distances).\nA high CH means better clustering since observations in each cluster are closer together (more dense), while clusters themselves are further away from each other (well separated).”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparing k-means scenarios</span>"
    ]
  },
  {
    "objectID": "24-compare-kmeans.html#jaccard-similarity-coefficient",
    "href": "24-compare-kmeans.html#jaccard-similarity-coefficient",
    "title": "13  Comparing k-means scenarios",
    "section": "13.3 Jaccard similarity coefficient",
    "text": "13.3 Jaccard similarity coefficient\nSo what am I comparing? Visually, I’d like to make an alluvial plot or Sankey-type plot to see how the two compare (and I did this, see PDFS in _refs/. But I’d also like to compare membership between the different clusterings. In Lasantha et al., (2022), they use the Jaccard similarity coefficient.\n\n“The Jaccard similarity coefficient is the ratio between the intersection and union of two sets; it has values ranging from zero for non-intersection to one for exact similarity. This is index is widely used in the evaluation of similarity in clustering in addition to applications such as image recognition and text analysis” Lasantha et al., 2022\n\nThis appears to be the citation for the original coefficient:\nP. Jaccard, “The distribution of the flora in the alpine zone zone” New Phytologist, vol. 11, no. 2, pp. 37–50, 1912.\nFrom Tan et al. (2018): “… the simple matching coefficient, which is known as the Rand statistic in this context, and the Jaccard coefficient are two of the most frequently used cluster validity measures.”\nAdditional relevant Jaccard citation:\nHennig, C. (2007). Cluster-wise assessment of cluster stability. Computational Statistics & Data Analysis, 52(1), 258–271. https://doi.org/10.1016/j.csda.2006.11.025\nTan, P.-N., Steinbach, M., Karpatne, A., & Kumar, V. (2018). Introduction to data mining (2nd ed.). Pearson.\nEquation:\nJaccard coefficient = f11 / (f01 + f10 + f11)\n\n\n\n\nSame Cluster\nDifferent Cluster\n\n\n\n\nSame Class\nf11\nf10\n\n\nDifferent Class\nf01\nf00\n\n\n\nSo in words, when we are comparing to sets of classes (like two different clusterings produced by k-means), the Jaccard coefficient is telling us the ratio of : objects that are in both sets (the intersection) divided by the union (total number of objects in both sets, subtracting the number they share)\nApparently the {vegan} package can do this with the vegdist() function, method = 'jaccard' .\nThe thing I didn’t understand at first was that the Jaccard index is for use on a specific pair of clusters. So I need to do this pairwise for all the clusters in the original and PCA versions\n\n13.3.1 Example from online tutorial\nJaccard index example from UC Riverside GEN 242 Course\n\nsource(\"R/cindex_tgirke.R\") \n\nlibrary(cluster)\n\ny &lt;- matrix(rnorm(5000), 1000, 5, dimnames=list(paste(\"g\", 1:1000, sep=\"\"), paste(\"t\", 1:5, sep=\"\")))\n\nclarax &lt;- clara(y, 49)\n\n# length = 1000, 49 classes (numeric 1:49)\nclV1 &lt;- clarax$clustering\n\nclarax &lt;- clara(y, 50)\n\nclV2 &lt;- clarax$clustering \n\nci &lt;- cindex(clV1=clV1, clV2=clV2, self=FALSE, minSZ=1, method=\"jaccard\")\n\n\n\nci[2:3] # Returns Jaccard index and variables used to compute it\n\n$variables\n   a    b    c \n8867 3402 2988 \n\n$Jaccard_Index\n[1] 0.5811759\n\n  n_intersect &lt;- length(intersect(clV1, clV2))\n  \n  jac_index &lt;- n_intersect/(length(clV1) + length(clV2) - n_intersect)\n\n\n\n13.3.2 My Jaccard function\nThis was a helpful blog post (Jaccard Index is quite simple to calculate).\nRecall that what I want here is the MUKEYs. That allows us to calculate an intersection (so how many MUKEYs appear in both clusters?).\n\njaccard &lt;- function(orig_clust, pca_clust, dat, orig_col, pca_col){\n  \n  orig_sym &lt;- rlang::sym(orig_col)\n  pca_sym &lt;- rlang::sym(pca_col)\n  \n  orig_members &lt;- d %&gt;% \n    filter(!!orig_sym == orig_clust) %&gt;% \n    pull(mukey)\n  \n  pca_members &lt;- d %&gt;% \n    filter(!!pca_sym == pca_clust) %&gt;% \n    pull(mukey)\n  \n  n_intersect &lt;- length(intersect(orig_members, pca_members))\n  \n  jac_index &lt;- n_intersect/(length(orig_members) + length(pca_members) - n_intersect)\n  \n  return(jac_index)\n}\n\nTry it for k=6, calculating the Jaccard similarity coefficient for Cluster 1 from the original model set and Cluster 1 from the PCA model set.\nTo see if the result I’m getting makes sense, I’m looking at the alluvial diagram I made comparing original clusters and the PCA clusters. What I see is that Cluster 1 from the original version is split between Cluster 1 and Cluster 2 in the PCA version, with roughly 2/3 of the MUKEYs shared between Cluster 1 (original) and Cluster 1 (PCA). So it makes sense that the value below is 0.68.\n\njaccard(orig_clust = \"c1\", \n        pca_clust = \"p1\",\n        dat = d,\n        orig_col = \"k_6\",\n        pca_col = \"pk_6\"\n        )\n\n[1] 0.6801205",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparing k-means scenarios</span>"
    ]
  },
  {
    "objectID": "24-compare-kmeans.html#calculate-jaccard-similarity-coefficients",
    "href": "24-compare-kmeans.html#calculate-jaccard-similarity-coefficients",
    "title": "13  Comparing k-means scenarios",
    "section": "13.4 Calculate Jaccard similarity coefficients",
    "text": "13.4 Calculate Jaccard similarity coefficients\nI think it makes the most sense to do this with three dataframes, one for each value of K we are interested in evaluating. So that would be: 6, 8, 11.\n\n13.4.1 Set up data structure\n\n# data structure for k=6\nclust_orig_k6 &lt;- unique(d$k_6)\nclust_pca_k6 &lt;- unique(d$pk_6)\n\nk6_pairs &lt;- tidyr::crossing(k6_orig = clust_orig_k6,\n            k6_pca = clust_pca_k6)\n\n# data structure for k=8\nclust_orig_k8 &lt;- unique(d$k_8)\nclust_pca_k8 &lt;- unique(d$pk_8)\n\nk8_pairs &lt;- tidyr::crossing(k8_orig = clust_orig_k8,\n            k8_pca = clust_pca_k8)\n\n# data structure for k=11\nclust_orig_k11 &lt;- unique(d$k_11)\nclust_pca_k11 &lt;- unique(d$pk_11)\n\nk11_pairs &lt;- tidyr::crossing(k11_orig = clust_orig_k11,\n            k11_pca = clust_pca_k11)\n\n\n\n13.4.2 Map over Jaccard function\n\njac_k6 &lt;- k6_pairs %&gt;% \n  mutate(jaccard_coef = map2_dbl(.x = k6_orig,\n                                 .y = k6_pca,\n                                 .f = jaccard,\n                                 dat = d,\n                                 orig_col = \"k_6\",\n                                 pca_col = \"pk_6\"),\n         k = \"k_6\") %&gt;% \n  rename(orig = k6_orig,\n         pca = k6_pca)\n\njac_k8 &lt;- k8_pairs %&gt;% \n  mutate(jaccard_coef = map2_dbl(.x = k8_orig,\n                                 .y = k8_pca,\n                                 .f = jaccard,\n                                 dat = d,\n                                 orig_col = \"k_8\",\n                                 pca_col = \"pk_8\"),\n         k = \"k_8\") %&gt;% \n  rename(orig = k8_orig,\n         pca = k8_pca)\n\n  \n  \njac_k11 &lt;- k11_pairs %&gt;% \n  mutate(jaccard_coef = map2_dbl(.x = k11_orig,\n                                 .y = k11_pca,\n                                 .f = jaccard,\n                                 dat = d,\n                                 orig_col = \"k_11\",\n                                 pca_col = \"pk_11\"), \n         k = \"k_11\") %&gt;% \n  rename(orig = k11_orig,\n         pca = k11_pca)\n\n\njac_all &lt;- bind_rows(jac_k6, jac_k8, jac_k11)\n\nwrite_csv(jac_all, \"data/jaccard_coefficients_k6_k8_k11.csv\")\n\n\n\n13.4.3 Visualize Results\n\n\nWarning: Since gt v0.9.0, the `colors` argument has been deprecated.\n• Please use the `fn` argument instead.\nThis warning is displayed once every 8 hours.\n\n\n\n\n\n\n\n\n\n\nPCA-6 Clusters\n\n\np1\np2\np3\np4\np5\np6\n\n\n\n\nFull-6 Clusters\nc1\n0.680\n0.162\n0.000\n0.000\n0.000\n0.000\n\n\nc2\n0.000\n0.565\n0.181\n0.017\n0.003\n0.000\n\n\nc3\n0.000\n0.007\n0.000\n0.925\n0.000\n0.000\n\n\nc4\n0.000\n0.000\n0.621\n0.009\n0.013\n0.000\n\n\nc5\n0.000\n0.000\n0.000\n0.000\n0.936\n0.000\n\n\nc6\n0.000\n0.001\n0.000\n0.000\n0.000\n0.970\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA-8 Clusters\n\n\np1\np2\np3\np4\np5\np6\np7\np8\n\n\n\n\nFull-8 Clusters\nc1\n0.993\n0.000\n0.001\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nc2\n0.000\n0.951\n0.008\n0.000\n0.012\n0.000\n0.001\n0.000\n\n\nc3\n0.002\n0.002\n0.973\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nc4\n0.000\n0.000\n0.000\n0.989\n0.000\n0.002\n0.000\n0.000\n\n\nc5\n0.000\n0.005\n0.000\n0.003\n0.952\n0.000\n0.000\n0.000\n\n\nc6\n0.000\n0.002\n0.000\n0.000\n0.000\n0.982\n0.000\n0.000\n\n\nc7\n0.000\n0.000\n0.000\n0.001\n0.000\n0.000\n0.997\n0.000\n\n\nc8\n0.000\n0.001\n0.000\n0.000\n0.000\n0.000\n0.000\n0.970\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA-11 Clusters\n\n\np1\np10\np11\np2\np3\np4\np5\np6\np7\np8\np9\n\n\n\n\nFull-11 Clusters\nc1\n0.902\n0.000\n0.000\n0.032\n0.000\n0.000\n0.015\n0.000\n0.000\n0.000\n0.000\n\n\nc10\n0.000\n0.000\n0.970\n0.000\n0.000\n0.004\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nc11\n0.000\n0.000\n0.000\n0.000\n0.000\n0.171\n0.000\n0.000\n0.000\n0.000\n0.000\n\n\nc2\n0.001\n0.000\n0.000\n0.845\n0.000\n0.017\n0.013\n0.000\n0.000\n0.000\n0.000\n\n\nc3\n0.000\n0.000\n0.000\n0.000\n0.938\n0.029\n0.004\n0.000\n0.009\n0.000\n0.001\n\n\nc4\n0.000\n0.000\n0.000\n0.007\n0.003\n0.217\n0.703\n0.000\n0.000\n0.000\n0.000\n\n\nc5\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.987\n0.001\n0.000\n0.000\n\n\nc6\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.003\n0.969\n0.000\n0.000\n\n\nc7\n0.000\n0.002\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.990\n0.000\n\n\nc8\n0.000\n0.000\n0.000\n0.000\n0.000\n0.016\n0.000\n0.001\n0.000\n0.000\n0.973\n\n\nc9\n0.000\n0.993\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.002\n0.000",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparing k-means scenarios</span>"
    ]
  },
  {
    "objectID": "26-notes-pca6-8.html",
    "href": "26-notes-pca6-8.html",
    "title": "14  Comparing PCA 6 & PCA 8 Models",
    "section": "",
    "text": "14.1 Overview\nMy plan is to make maps of the k=6-8 clusterings (PCA version) to help explain differences between them, and potentially provide practical / interpretive evidence for why we might choose one over another. My gut feeling is that k=8 makes sense because it breaks up some of the biggest cluster groups (can see this in the alluvial plot comparing the PCA k_6 with the PCA k_8",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing PCA 6 & PCA 8 Models</span>"
    ]
  },
  {
    "objectID": "26-notes-pca6-8.html#alluvial-plot",
    "href": "26-notes-pca6-8.html#alluvial-plot",
    "title": "14  Comparing PCA 6 & PCA 8 Models",
    "section": "14.2 Alluvial plot",
    "text": "14.2 Alluvial plot\nThe three largest clusters (k_6) are:\n\nClust 2 (n=2158)\nClust 3 (n=1664)\nClust 4 (n=1436)\n\nMoving from PCA k_6 to PCA k_8 versions of the model, the big differences we see are that the 3 largest clusters (6-2, 6-3, 6-4) split to create to two additional, intermediate clusters (8-3 and 8-2). An additional result of this split is that we have a smaller cluster (n=971) of the highest clay mukeys remaining in 8-5.\n\n~ 100 mukeys from the 6-1 (coarsest) and ~1200 mukeys from 6-2 (coarse-loamy) split off to form 8-3.\nThe remaining ~900 mukeys from 6-2 combine with ~500 mukeys from 6-3 to form 8-2, leaving the remaining ~1000 mukeys with the highest clay from 6-3 to become 8-5",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing PCA 6 & PCA 8 Models</span>"
    ]
  },
  {
    "objectID": "26-notes-pca6-8.html#k6",
    "href": "26-notes-pca6-8.html#k6",
    "title": "14  Comparing PCA 6 & PCA 8 Models",
    "section": "14.3 k=6",
    "text": "14.3 k=6",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing PCA 6 & PCA 8 Models</span>"
    ]
  },
  {
    "objectID": "26-notes-pca6-8.html#k8",
    "href": "26-notes-pca6-8.html#k8",
    "title": "14  Comparing PCA 6 & PCA 8 Models",
    "section": "14.4 k=8",
    "text": "14.4 k=8\nGoing from k=6 to k=8, the main difference is",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Comparing PCA 6 & PCA 8 Models</span>"
    ]
  },
  {
    "objectID": "31-demo-weighted-centroids.html",
    "href": "31-demo-weighted-centroids.html",
    "title": "15  Find map unit weighted centroids",
    "section": "",
    "text": "15.1 Set-up and load data\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(glue)\nRecall that per the gSSURGO documentation the gSSURGO raster (from which my reclassed raster was derived) uses the USA Contiguous Albers Equal Area Conic USGS version coordinate system with a horizontal datum of NAD 1983.\ncty &lt;- sf::st_read('data/mn_county/mn_county_boundaries.shp')\n\nredwood &lt;- cty %&gt;% filter(CTY_NAME == \"Redwood\")\nspat_redwood &lt;- vect(x = redwood)\n\n# gssurgo raster, clipped to AOI and simple MUKEYs to reduce size\nr &lt;- raster::raster(\"data/gSSURGO_MN/MapunitRaster_10m_Clip1_and_Reclass/MapunitRaster_10m_Clip1_and_Reclass/Reclass_tif1.tif\")\n  \n\ncrs(raster::raster(\"data/gSSURGO_MN/MapunitRaster_10m_Clip1_and_Reclass/MapunitRaster_10m_Clip1_and_Reclass/Reclass_tif1.tif\"))",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Find map unit weighted centroids</span>"
    ]
  },
  {
    "objectID": "31-demo-weighted-centroids.html#tests",
    "href": "31-demo-weighted-centroids.html#tests",
    "title": "15  Find map unit weighted centroids",
    "section": "15.2 Tests",
    "text": "15.2 Tests\n\ntest1 &lt;- raster::rasterToPoints(x = r,\n                       fun = function(x){x==1},\n                       progress = \"text\")\n\n\ntest1_named &lt;- data.frame(test1) %&gt;%\n  rename(lon = x,\n         lat = y,\n         mukey_short = Reclass_tif1)\n\ntest_centroid &lt;- test1_named %&gt;%\n  summarise(lon = mean(lon),\n            lat = mean(lat))\n\ntest_vect &lt;- vect(test1_named, geom = c(\"lon\", \"lat\"), crs = \"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\")\n\ncentroid_vect &lt;- vect(test_centroid, geom = c(\"lon\", \"lat\"), crs = \"+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs\")\n\nplot(r)\npoints(test_vect)\npoints(centroid_vect, col = c(\"red\"))",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Find map unit weighted centroids</span>"
    ]
  },
  {
    "objectID": "31-demo-weighted-centroids.html#raster-to-points-function",
    "href": "31-demo-weighted-centroids.html#raster-to-points-function",
    "title": "15  Find map unit weighted centroids",
    "section": "15.3 Raster to points function",
    "text": "15.3 Raster to points function\n\nsave_mu_points_from_rast &lt;- function(rast_obj = r, mukey_chunk) {\n  \n  pts &lt;- raster::rasterToPoints(\n    x = rast_obj,\n    fun = function(x) {\n      x %in% mukey_chunk\n    },\n    progress = \"text\"\n  )\n  \n  \n  pts_named &lt;- data.frame(pts) %&gt;%\n    rename(lon = x,\n           lat = y,\n           mukey_short = Reclass_tif1) %&gt;%\n    group_by(mukey_short) %&gt;%\n    summarise(across(\n      .cols = c(\"lat\", \"lon\"),\n      .fns = list(mean = mean),\n      .names = \"{.fn}_{.col}\"\n    ), n = n())\n  \n  fname &lt;- glue(\"{min(mukey_chunk)}_{max(mukey_chunk)}\")\n  \n  write_csv(\n    pts_named,\n    glue(\n      \"data/mu_points_from_rast/{fname}_mukey_short_points.csv\"\n    )\n  )\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Find map unit weighted centroids</span>"
    ]
  },
  {
    "objectID": "31-demo-weighted-centroids.html#find-centroids",
    "href": "31-demo-weighted-centroids.html#find-centroids",
    "title": "15  Find map unit weighted centroids",
    "section": "15.4 Find centroids",
    "text": "15.4 Find centroids\nNote I have eval set to false below, this takes a little while to run so I don’t want it to redo every time I do a new build.\n\nall &lt;- c(1:7861)\nchunk_length &lt;- 200\n\nsp &lt;- split(x = all, f = ceiling(seq_along(all) / chunk_length))\n\n\npurrr::map(\n  .x = sp,\n  .f = ~ save_mu_points_from_rast(mukey_chunk = .x),\n  .progress = TRUE\n)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Find map unit weighted centroids</span>"
    ]
  },
  {
    "objectID": "31-demo-weighted-centroids.html#combine-centroid-data",
    "href": "31-demo-weighted-centroids.html#combine-centroid-data",
    "title": "15  Find map unit weighted centroids",
    "section": "15.5 Combine centroid data",
    "text": "15.5 Combine centroid data\nNow that we have the centroids for each map unit, want to combine into one data set. In the next chapter (32-sample-climate-vars.qmd) I will use these points to sample the climate data (30 year normals for mean annual precipitation, mean annual temperature).\nThis is set to eval: false too because it depends on the above code chunk.\n\nfpaths &lt;- list.files('data/mu_points_from_rast/', full.names = TRUE)\n\ndf_list &lt;- map(.x = fpaths, .f = read_csv)\n\ncentroids_all &lt;- bind_rows(df_list)\n\nhead(centroids_all)\n\nwrite_csv(centroids_all, 'data/mapunit_centroids_nad1983_epsg5070.csv')",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Find map unit weighted centroids</span>"
    ]
  },
  {
    "objectID": "32-sample-climate-vars.html",
    "href": "32-sample-climate-vars.html",
    "title": "16  Sample climate data at map-unit centroids",
    "section": "",
    "text": "16.1 Set-up\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(sf)\nlibrary(terra)\nLoading the Minnesota state boundary so I can use it when plotting later. Originally thought it would be a good idea to clip the climate rasters to the MN boundary, but then I realized that it was possible for my map unit centroids to fall OUTSIDE of the Minnesota border if their center of gravity was close to the border anyway. A better solution I think is to use terra::ext with a SpatVector of my centroid points and then use the resulting extent to clip the climate raster. That way I am guaranteed to have climate data for all the centroid points.\n# EPSG 4269 / NAD 83\nmn &lt;- st_read('data/cb_2018_us_state_500k/cb_2018_us_state_500k.shp') %&gt;% \n  filter(NAME == \"Minnesota\")\n\nReading layer `cb_2018_us_state_500k' from data source \n  `C:\\Users\\Hava\\backed_up\\Documents\\R\\ch03-sh-cluster-20240114\\data\\cb_2018_us_state_500k\\cb_2018_us_state_500k.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 56 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1489 ymin: -14.5487 xmax: 179.7785 ymax: 71.36516\nGeodetic CRS:  NAD83\n\nst_crs(mn)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n# make it a SpatVector \nmnspat &lt;- vect(mn)\nmn84 &lt;- project(x = mnspat, y = 'epsg:4326')\n\n\nggplot(data = mn) +\n  geom_sf()\n\n\n\n\n\n\n\nmn_cty &lt;- vect('_qgis_files/shp_bdry_counties_in_minnesota/mn_county_boundaries.shp')\n\nmn_cty84 &lt;- project(x = mn_cty, y = 'epsg:4326')",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sample climate data at map-unit centroids</span>"
    ]
  },
  {
    "objectID": "32-sample-climate-vars.html#set-up",
    "href": "32-sample-climate-vars.html#set-up",
    "title": "16  Sample climate data at map-unit centroids",
    "section": "",
    "text": "16.1.1 Load map unit centroids\nNote need to convert these to WGS 84 before we can sample the climate data.\n\ncentroid_df &lt;- read_csv('data/mapunit_centroids_nad1983_epsg5070.csv', show_col_types = FALSE) %&gt;% \n  rename(lon = mean_lon,\n         lat = mean_lat) %&gt;% \n  select(-n) %&gt;%\n  # because my short Mukeys are essentially row numbers (1-7859)\n  # adding an ID (row number) column here AND arranging by mukey_short\n  # is redundant. Having a separate ID column is important because\n  # it's conceivable that you might NOT have mukeys that line up exactly \n  # with row numbers, and then you wouldn't have a way to join the row ids\n  # returned by terra::extract back with the appropriate mukeys in \n  # this dataframe. I made this mistake the first time around (not sorting\n  #by mukey_short OR creating ID column) and got some weird climate results\n  arrange(mukey_short) %&gt;% \n  mutate(ID = row_number())\n\n# NAD 1983 / EPSG 5070\ncentroids_nad83 &lt;- vect(centroid_df, geom = c(\"lon\", \"lat\"), crs = 'epsg:5070')\n\n# WGS 84 = EPSG 4326\ncentroids_wgs84 &lt;- project(centroids_nad83, 'epsg:4326')\n\nplot(mn_cty84)\npoints(centroids_wgs84)\n\n\n\n\n\n\n\n\n\n\n16.1.2 Get extent of centroids\nWondering if I should create an extent that’s a little larger than this one? Seems possible that if any centroids fall close enough to the edge I won’t have the corresponding raster cell(s) in the extent. The docs for terra::crop back this up: “Note that the SpatRaster returned may not have exactly the same extent as the SpatExtent supplied because you can only select entire cells (rows and columns), and you cannot add new areas.”\nAfter a little more reading I found that terra already has a function to do exactly what I want (add a few rows/columns around the edges of my SpatExtent to make sure it covers what I need). That function is terra::extend.\n\nc_extent &lt;- terra::ext(centroids_wgs84)\n\n# plot the extent with centroids\nplot(c_extent, main = \"Extent with centroids\")\npoints(centroids_wgs84)\n\n\n\n\n\n\n\n# extend the extent by specified number rows/columns on each side\n# at first I was thinking about this in terms of raster cells,\n# but the extending happens in the same units (degress lon/lat)\n# as the extent itself. So below I am extending by 0.25 degrees\n# on each side of the rectangle\nc_extent_expanded &lt;- terra::extend(x = c_extent, y = c(0.25))\n\n# plot again\nplot(c_extent_expanded, main = \"Extent expanded +0.25 degrees, with map unit centroids\")\npoints(centroids_wgs84)\npolys(mn_cty84)\n\n\n\n\n\n\n\n\n\n\n16.1.3 Load climate data\nNet CDF files, NOAA US Climate Normals https://www.ncei.noaa.gov/products/land-based-station/us-climate-normals\n\nThe file has 85 layers in total (1 layer = 1 variable, like mlytavg_norm_2 which translates to “monthly mean temperature normals from monthly averages”, for February (month = 2)\nI believe the layer I want is anntavg_norm , which per the NOAA README for the monthly gridded normals is “Annual mean temperature normals from monthly normals”\n\nUnits are degrees Celsius\nValues should be rounded to the nearest tenth\n\n\n\n# if you want to see all the layers in this netCDF file, remove the \n# lyrs argument\ntemp &lt;- rast(\n  x = 'data/climate/tavg-1991_2020-monthly-normals-v1.0.nc',\n             lyrs = 'anntavg_norm')\n\n# WGS 84 / EPSG 4326\ntemp\n\nclass       : SpatRaster \ndimensions  : 596, 1385, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166666, 0.04166667  (x, y)\nextent      : -124.7083, -67, 24.5417, 49.37503  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : tavg-1991_2020-monthly-normals-v1.0.nc:anntavg_norm \nvarname     : anntavg_norm (Annual mean temperature normals from monthly normals) \nname        :   anntavg_norm \nunit        : degree_Celsius \n\nterra::plot(temp,\n     main = 'Annual Temperature Normal, Celsius, (1991-2020)')\n\n\n\n\n\n\n\n\n\nprecip &lt;- rast(x = 'data/climate/prcp-1991_2020-monthly-normals-v1.0.nc',\n               lyrs = 'annprcp_norm')\n\nprecip\n\nclass       : SpatRaster \ndimensions  : 596, 1385, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166666, 0.04166667  (x, y)\nextent      : -124.7083, -67, 24.5417, 49.37503  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : prcp-1991_2020-monthly-normals-v1.0.nc:annprcp_norm \nvarname     : annprcp_norm (Annual precipitation normals from monthly precipitation normal values) \nname        : annprcp_norm \nunit        :   millimeter \n\nplot(precip, main = \"Annual Precipitation Normal, mm (1991-2020)\")",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sample climate data at map-unit centroids</span>"
    ]
  },
  {
    "objectID": "32-sample-climate-vars.html#crop-climate-data-to-extent",
    "href": "32-sample-climate-vars.html#crop-climate-data-to-extent",
    "title": "16  Sample climate data at map-unit centroids",
    "section": "16.2 Crop climate data to extent",
    "text": "16.2 Crop climate data to extent\n\nprecip_crop &lt;- terra::crop(x = precip, y = c_extent_expanded)\n\ntemp_crop &lt;- terra::crop(x = temp, y = c_extent_expanded)\n\nplot(precip_crop, main = \"Annual precipitation normal (mm) 1991-2020\")\nlines(mn_cty84)\n\n\n\n\n\n\n\nplot(temp_crop, main = \"Annual temperature normal (deg. Celsius) 1991-2020\")\nlines(mn_cty84)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sample climate data at map-unit centroids</span>"
    ]
  },
  {
    "objectID": "32-sample-climate-vars.html#extract-sample-climate-variables-at-centroid-points",
    "href": "32-sample-climate-vars.html#extract-sample-climate-variables-at-centroid-points",
    "title": "16  Sample climate data at map-unit centroids",
    "section": "16.3 Extract (sample) climate variables at centroid points",
    "text": "16.3 Extract (sample) climate variables at centroid points\nWriting this a reminder to myself when I come back and wonder if I did all the CRS stuff right. The climate data’s original CRS is lon/lat WGS 84 (CRS84) (OGC:CRS84). The climate data is provided in as netCDF files (rasters). Because re-projecting rasters can result in loss of information, it’s better if I change the CRS of my POINTS (vector) to match the climate raster before extracting values. This is what I did.\nFrom the terra documentation:\n\nTransforming (projecting) raster data is fundamentally different from transforming vector data. Vector data can be transformed and back-transformed without loss in precision and without changes in the values. This is not the case with raster data. In each transformation the values for the new cells are estimated in some fashion. Therefore, if you need to match raster and vector data for analysis, you should generally transform the vector data.\n\nThis\n\ntemp_pts &lt;- terra::extract(x = temp_crop, y = centroids_wgs84)\n\nprecip_pts &lt;- terra::extract(x = precip_crop, y = centroids_wgs84)\n\n\n# missing any values?\ntemp_pts %&gt;% filter(is.na(anntavg_norm))\n\n\n  \n\n\nprecip_pts %&gt;% filter(is.na(annprcp_norm))\n\n\n  \n\n\n# first time around I did this join with \"mukey_short\" = \"ID\" and that \n# was a mistake because it turned out my original centroid_df\n# was NOT sorted by mukey (1:7859) so it was NOT the appropriate join\n# Noticed the issue when I mapped the extracted temp and precip values \n# of the centroids\nc_temp &lt;- left_join(centroid_df, temp_pts, by = c(\"ID\"))\ncentroids_clim &lt;- left_join(c_temp, precip_pts, by = c(\"ID\"))\n\n\ncentroids_clim\n\n\n  \n\n\n# get the WGS 84 lat/long points out of the spatvector object\n# really only doing this so I can make some quick plots \n# below \npts_wgs84 &lt;- as.data.frame(centroids_wgs84, geom = \"xy\")\n\nclim_plot_df &lt;- left_join(centroids_clim, pts_wgs84, by = c(\"ID\", \"mukey_short\")) %&gt;% \n  select(-c(lat, lon))\n\nclimsf &lt;- st_as_sf(clim_plot_df, coords = c(\"x\",\"y\"))\n\nclimsf &lt;- st_set_crs(climsf, 4326) \n\n\nggplot(data = climsf) +\n  geom_sf(aes(color = anntavg_norm)) +\n  scale_color_viridis_c() +\n  ggtitle(\"Map Unit Centroids - Temperature\") +\n  labs(subtitle = \"Annual Temperature Normal (deg. Celsius) 1991-2020\")\n\n\n\n\n\n\n\nggplot(data = climsf) +\n  geom_sf(aes(color = annprcp_norm)) +\n  scale_color_viridis_c() + \n  ggtitle(\"Map Unit Centroids - Precipitation\") +\n  labs(subtitle = \"Annual Precipitation Normal (mm) 1991-2020\")",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sample climate data at map-unit centroids</span>"
    ]
  },
  {
    "objectID": "32-sample-climate-vars.html#save-climate-data",
    "href": "32-sample-climate-vars.html#save-climate-data",
    "title": "16  Sample climate data at map-unit centroids",
    "section": "16.4 Save climate data",
    "text": "16.4 Save climate data\n\nwrite_csv(clim_plot_df, \"data/ann_normals_precip_temp_mukey_centroids.csv\")",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sample climate data at map-unit centroids</span>"
    ]
  },
  {
    "objectID": "32-sample-climate-vars.html#how-correlated-are-the-climate-variables",
    "href": "32-sample-climate-vars.html#how-correlated-are-the-climate-variables",
    "title": "16  Sample climate data at map-unit centroids",
    "section": "16.5 How correlated are the climate variables?",
    "text": "16.5 How correlated are the climate variables?\n\nc &lt;- read_csv(\"data/ann_normals_precip_temp_mukey_centroids.csv\")\n\nRows: 7861 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): mukey_short, ID, anntavg_norm, annprcp_norm, x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nc %&gt;% \n  select(anntavg_norm, annprcp_norm) %&gt;% \n  corrr::correlate()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sample climate data at map-unit centroids</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html",
    "href": "35-climate-clusters.html",
    "title": "17  Cluster map units based on climate",
    "section": "",
    "text": "17.1 Overview\nQuick outline of the process:",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#overview",
    "href": "35-climate-clusters.html#overview",
    "title": "17  Cluster map units based on climate",
    "section": "",
    "text": "Already have the data we need (MAP, MAT) from 31-demo-weighted-centroids.qmd and 32-sample-climate-vars.qmd , make some plots to show the distributions\nPre-process data (standardize), subtracting the mean and dividing by 1 sd\nPCA doesn’t seem necessary here because we only have 2 variables (MAT and MAP)\nSpecify model options, set up data structure and k-means recipe\nFit models (probably k = 2-5 is reasonable to start?)\nEvaluate results",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#set-up",
    "href": "35-climate-clusters.html#set-up",
    "title": "17  Cluster map units based on climate",
    "section": "17.2 Set up",
    "text": "17.2 Set up\n\nlibrary(workflows)\n\nWarning: package 'workflows' was built under R version 4.3.1\n\nlibrary(parsnip)\n\nWarning: package 'parsnip' was built under R version 4.3.2\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5      ✔ rsample      1.2.0 \n✔ dials        1.2.0      ✔ tune         1.1.2 \n✔ infer        1.0.6      ✔ workflowsets 1.0.1 \n✔ modeldata    1.3.0      ✔ yardstick    1.2.0 \n✔ recipes      1.0.10     \n\n\nWarning: package 'broom' was built under R version 4.3.2\n\n\nWarning: package 'dials' was built under R version 4.3.1\n\n\nWarning: package 'scales' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.2\n\n\nWarning: package 'modeldata' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.2\n\n\nWarning: package 'rsample' was built under R version 4.3.2\n\n\nWarning: package 'tune' was built under R version 4.3.2\n\n\nWarning: package 'workflowsets' was built under R version 4.3.2\n\n\nWarning: package 'yardstick' was built under R version 4.3.1\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(glue)\nlibrary(tidyclust)\n\nWarning: package 'tidyclust' was built under R version 4.3.1\n\nlibrary(factoextra) # trying fviz_nbclust(), which gives elbow, silhouette, and gap statistic\n\nWarning: package 'factoextra' was built under R version 4.3.2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(hopkins)\n\nWarning: package 'hopkins' was built under R version 4.3.2\n\nlibrary(fpc)\n\nWarning: package 'fpc' was built under R version 4.3.2\n\nlibrary(ggforce)\n\nWarning: package 'ggforce' was built under R version 4.3.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.2\n\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.3.1\n\n\nLinking to GEOS 3.11.2, GDAL 3.6.2, PROJ 9.2.0; sf_use_s2() is TRUE\n\nd &lt;- read_csv(\"data/ann_normals_precip_temp_mukey_centroids.csv\") %&gt;% \n  select(-ID) \n\nRows: 7861 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (6): mukey_short, ID, anntavg_norm, annprcp_norm, x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmukeys_include &lt;- read_csv(\"data/clean_mu_weighted_soil_props_with_sand.csv\")\n\nRows: 6872 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): mukey, claytotal_r_value, sandtotal_r_value, om_r_value, cec7_r_va...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# for translating MUKEY from gSSURGO to my shortened version\ncwalk &lt;- aoi_mu &lt;- read.delim(\"data/gSSURGO_MN/mukey_new_crosswalk.txt\", sep = \",\") %&gt;% \n  select(MUKEY, MUKEY_New)\n\nd_ids &lt;- left_join(d, cwalk, by = c(\"mukey_short\" = \"MUKEY_New\"))\n\nd_incl &lt;- d_ids %&gt;% \n  filter(MUKEY %in% mukeys_include$mukey) %&gt;% \n  rename(mukey = MUKEY)\n\nCheck out the MAP and MAT data:\n\nhead(d_incl)\n\n\n  \n\n\nggplot(data = d_incl) +\n  geom_histogram(aes(x = annprcp_norm)) +\n  ggtitle(\"Distribution of precipitation (mm) values\") +\n  xlab(\"Annual precipitation normal (mm)\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data = d_incl) +\n  geom_histogram(aes(x = anntavg_norm)) +\n  ggtitle(\"Distribution of temperature (degree Celsius) values\") +\n  xlab(\"Annual temperature normal (degrees C)\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# check for missing values\nd_incl %&gt;% \n  filter(is.na(annprcp_norm) |\n           is.na(anntavg_norm))",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#pre-process-data-build-recipe",
    "href": "35-climate-clusters.html#pre-process-data-build-recipe",
    "title": "17  Cluster map units based on climate",
    "section": "17.3 Pre-process data (build recipe)",
    "text": "17.3 Pre-process data (build recipe)\n\nrec_spec &lt;- recipe(~., data = d_incl) %&gt;%\n    update_role(mukey, new_role = \"ID_long\") %&gt;%\n  update_role(mukey_short, new_role = \"ID_short\") %&gt;% \n  update_role(c(x, y), new_role = \"lat_lon\") %&gt;% \n    step_normalize(all_numeric_predictors()) \n\nrec_spec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\nlat_lon:   2\nID_long:   1\nID_short:  1\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_numeric_predictors()\n\n# confirm all predictors are labelled with role = predictor,\n# other vars should have other roles so they aren't included\n# in the model.\nsummary(rec_spec)\n\n\n  \n\n\n\n\n# retain argument here tells prep to keep \n# the pre-processed training data \n# note this can make the final recipe size large, \n# so this is not the recipe object I probably want to use\n# in my list col below\ncheck_prep &lt;- prep(rec_spec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \ncheck_prepped_df &lt;- bake(check_prep, new_data = NULL)\n\nhead(check_prepped_df)\n\n\n  \n\n\n# save the pre-processed data in case I want to make some plots of \n# the distribution of the normalized (centered/scaled) variables\n\nwrite_csv(check_prepped_df, \"./data/data_preprocessed_only_climate_20240310.csv\")\n\n\nggplot(data = check_prepped_df) +\n  geom_histogram(aes(x = annprcp_norm)) +\n  ggtitle(\"Distribution of *centered & scaled* precipitation (mm) values\") +\n  xlab(\"Annual precipitation normal\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data = check_prepped_df) +\n  geom_histogram(aes(x = anntavg_norm)) +\n  ggtitle(\"Distribution of *centered & scaled* temperature (degree Celsius) values\") +\n  xlab(\"Annual temperature normal\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#set-model-options",
    "href": "35-climate-clusters.html#set-model-options",
    "title": "17  Cluster map units based on climate",
    "section": "17.4 Set model options",
    "text": "17.4 Set model options\n\n# writing a custom function here so I can be explicit \n# about the options I'm choosing, and also use within the \n# list-col framework I set up with map() below. \nkm_spec &lt;- function(nclust){\n  \n  tidyclust::k_means(num_clusters = nclust) %&gt;%\n    parsnip::set_engine(engine = \"stats\",\n               nstart = 10, # 1 is default, &gt;1 recommended\n               algorithm = \"Hartigan-Wong\", # H-W is default\n               iter.max = 20) # default is 10, wasn't always enough\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#set-up-data-structure",
    "href": "35-climate-clusters.html#set-up-data-structure",
    "title": "17  Cluster map units based on climate",
    "section": "17.5 Set up data structure",
    "text": "17.5 Set up data structure\nHere I set up a dataframe that will catch my modeling results in list columns of the different model objects and return values. The first column I define specifies the range of different cluster sizes (k) that we will try.\n\ntry_clusts &lt;- c(2:6)\n\nkm_df &lt;- data.frame(n_clust = try_clusts)\n\nkm_df",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#specify-model-for-each-value-of-k",
    "href": "35-climate-clusters.html#specify-model-for-each-value-of-k",
    "title": "17  Cluster map units based on climate",
    "section": "17.6 Specify model (for each value of k)",
    "text": "17.6 Specify model (for each value of k)\nFor each unique value of k (2-6), this returns a model specification object (in the kmeans_spec column) based on the custom function I wrote above. The model specification has all the options set about how we want the algorithm to run (methods, number of starts, etc.). We need a different one for each value of k.\nThe kmeans_wflow column here holds our workflow objects. These objects combine our model specification (from kmeans_spec) with the data recipe (preprocessor) we made above (rec_spec, is same for all models).\n\n# for each unique value of clusters (2:20), returns a model\n# specification (kmeans_spec) and a workflow (kmeans_wflow) \n# note that the workflow \nkm_df &lt;- km_df %&gt;%\n  mutate(\n    kmeans_spec = map(n_clust, ~ km_spec(nclust = .x)),\n    kmeans_wflow = map(kmeans_spec,\n                       ~ workflow(\n                         preprocessor = rec_spec, spec = .x\n                       ))\n  )\n\n# our current data structure\nhead(km_df)\n\n\n  \n\n\n# take a look at an example workflow\nkm_df$kmeans_wflow[3]\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: k_means()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK Means Cluster Specification (partition)\n\nMain Arguments:\n  num_clusters = nclust\n\nEngine-Specific Arguments:\n  nstart = 10\n  algorithm = Hartigan-Wong\n  iter.max = 20\n\nComputational engine: stats",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#fit-the-models",
    "href": "35-climate-clusters.html#fit-the-models",
    "title": "17  Cluster map units based on climate",
    "section": "17.7 Fit the models",
    "text": "17.7 Fit the models\nAll the steps above were related to specifying different aspects of this model. Now we can actually fit the models.\nSome troubleshooting here:\n\nStarted by specifying tidyclust::fit() but something weird was happening where my step_normalize() wasn’t included in the pre-processor recipe when I looked at the fitted model object.\nIf I specify parsnip::fit() , then step_normalize() is included and the values of the cluster centroids are in the expected ranges (centered, scaled).\nI also tried this without explicitly specifying the package (so just fit() ) and it worked as expected.\n\n\n# make a quiet version of fit(), so we can capture results \n# and any warning messages from the models \n# see troubleshooting notes below\nquiet_fit &lt;- purrr::quietly(.f = parsnip::fit)\n\nset.seed(4) # for reproducibility \nkm_fit_df &lt;- km_df %&gt;%\n  mutate(km_result = map(.x = kmeans_wflow,\n                      .f = quiet_fit,\n       # data comes after .f b/c not vectorized over            \n                      data = d_incl),\n       km_fit = map(km_result, ~pluck(.x, 'result')),\n       warn = map(km_fit, ~pluck(.x, 'warnings')),\n       msg = map(km_fit, ~pluck(.x, 'messages')),\n       n_iter = map_dbl(km_fit, \n                      ~pluck(.x, 'fit', 'fit', 'fit', 'iter' ))) \n         \n\n# check out current data structure\nhead(km_fit_df)\n\n\n  \n\n\n# don't need anymore, cleaning up\nrm(km_df)\n\n\n17.7.1 View messages & warnings\nWe can look at any warnings or messages from the modeling process:\n\nkm_fit_df %&gt;% \n  select(n_clust, warn, msg, n_iter)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#model-metrics",
    "href": "35-climate-clusters.html#model-metrics",
    "title": "17  Cluster map units based on climate",
    "section": "17.8 Model metrics",
    "text": "17.8 Model metrics\nSee also section 7.5 in the Chapter by Tan et al. for more about cluster evaluation.\n\n17.8.1 Extract metrics\n\nmetrics_df &lt;- km_fit_df %&gt;%\n  mutate(\n    # tot_sse = total sum of squared error\n    tot_sse = map_dbl(km_fit, ~ sse_total_vec(.x)),\n    # tot_wss = sum of within-cluster sse\n    tot_wss = map_dbl(km_fit, ~sse_within_total_vec(.x)),\n    # sse ratio = wss / total sse, \n    sse_ratio = map_dbl(km_fit, ~sse_ratio_vec(.x))\n    )\n\nrm(km_fit_df)\n\nmetrics_simple &lt;- metrics_df %&gt;% \n  select(n_clust, tot_sse, tot_wss, sse_ratio)\n\nmetrics_simple\n\n\n  \n\n\n\n\n\n17.8.2 Plot Total WSS\n\nmetrics_simple %&gt;% \n  ggplot(aes(x = n_clust, y = tot_wss)) +\n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  scale_x_continuous(breaks = c(2:6)) +\n  xlab(\"k (number clusters)\") +\n  ylab(\"sum of within-cluster sse\") +\n  ggtitle(\"Compare values of k: looking for elbow\")\n\n\n\n\n\n\n\n\n\n\n17.8.3 Average Silhouette\nFrom the {tidyclust} documentation:\n\nAnother common measure of cluster structure is called the silhouette. The silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\n\nIn principle, a large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\nSee also pg. 581 in Tan2018 Chap 7 Cluster Analysis: Basic Concepts and Algorithms\n\nprepped_rec &lt;- prep(rec_spec, retain=TRUE)\n\n# using NULL here for new_data b/c I want the \n# pre-processed training data \nbaked_df &lt;- bake(prepped_rec, new_data = NULL) %&gt;% \n  select(-mukey) \n\ndists &lt;- baked_df %&gt;% as.matrix() %&gt;% dist(method = \"euclidean\")\n\nsilh_df &lt;- metrics_df %&gt;% \n  mutate(avg_sil = map_dbl(km_fit, \n                       tidyclust::silhouette_avg_vec,\n                       dists = dists),\n         indiv_sil = map(km_fit, \n                         tidyclust::silhouette,\n                         dists = dists))\n\nindiv_sil_df &lt;- silh_df %&gt;% select(n_clust, indiv_sil) %&gt;% \n  unnest(indiv_sil) %&gt;% \n  mutate(across(.cols = c(cluster, neighbor),\n                .fns = as.character))\n\nwrite_csv(indiv_sil_df, \"data/kmeans_points_silhouettes_climate_only.csv\")\n\nrm(metrics_df)  \nrm(dists)\nrm(prepped_rec)\n\nHigher silhouette is better (means observations are closer to their centroids than to other observations).\n\nsilh_df %&gt;% \n  ggplot(aes(x = n_clust, y = avg_sil)) +\n  geom_point() + \n  geom_line() + \n  theme_bw() +\n  scale_x_continuous(breaks = c(2:6)) +\n  ggtitle(\"Overall Average Silhouette\") +\n  labs(subtitle = \"Higher is better, possible values [-1,1]\")\n\n\n\n\n\n\n\n\n\nclust_sil_avgs &lt;- indiv_sil_df %&gt;% \n  group_by(cluster,\n           n_clust) %&gt;% \n  summarise(mean_sil = mean(sil_width),\n            sd_sil = sd(sil_width), \n            .groups = \"drop\")\n\nclust_sil_avgs %&gt;% \n  mutate(cluster = str_replace(cluster, \"Cluster_\", \"c0\")) %&gt;% \n  ggplot() +\n  geom_col(aes(y = cluster, x = mean_sil)) +\n  facet_wrap(vars(n_clust), scales = \"free_y\") + \n  ggtitle(\"Average silhouette width per cluster for k=2-6\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n17.8.4 Calinski-Harabasz index\nNot used: {NbClust} , using {fpc} instead.\nUsing the implementation of the Calinski-Harabasz index from the {fpc} package. This is preferred to my original approach, where I used the NbClust function from {NbClust} package because I can give this function my clustering generated above (NbClust does its own run of kmeans but I can’t customize it to keep it consistent with\n\n# calinhara wants an observations/variables matrix\n# as first argument (as opposed to a distance matrix)\nobsvar_mx &lt;- as.matrix(baked_df)\n\n# function to extract and modify tidyclust clusters\n# into a integer vector, which I will pass to calinhara()\ncreate_clust_vec &lt;- function(fit_obj){\n  \n  extract_cluster_assignment(fit_obj) %&gt;% \n    pull(.cluster) %&gt;% \n    str_replace(., \"Cluster_\", \"\") %&gt;% \n    as.integer()\n  \n}\n\n# apply function to extract clusterings as integer vectors\n# map to get a c-h index value for every value of k (2-20)\nch_metrics &lt;- silh_df %&gt;%\n  select(n_clust, km_fit) %&gt;%\n  mutate(\n    clustering_vec = map(km_fit, create_clust_vec),\n    ch_index = map_dbl(clustering_vec,\n                       ~ fpc::calinhara(x = obsvar_mx,\n                                        clustering = .x)\n    ))\n\nch_metrics %&gt;% \n  ggplot(aes(x = n_clust, y = ch_index)) + \n  geom_point() + \n  geom_line() +\n  theme_bw() +\n  ylab(\"Calinski-Harabasz index\") + \n  ggtitle(\"Calinski-Harabasz\") +\n  labs(subtitle = \"Higher is better\") +\n  scale_x_continuous(breaks = c(2:6))\n\n\n\n\n\n\n\n\n\n\n17.8.5 WSS and Silhouette metrics on one plot",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#save-model-fits",
    "href": "35-climate-clusters.html#save-model-fits",
    "title": "17  Cluster map units based on climate",
    "section": "17.9 Save model fits",
    "text": "17.9 Save model fits\nWill save these as Rdata so I can call them up and investigate them further as needed.\n\nmods &lt;- silh_df %&gt;% \n  select(n_clust, km_fit)\n\nsave(mods, file = \"data/fitted_kmeans_mods_climate_only.RData\")",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#save-cluster-assignments",
    "href": "35-climate-clusters.html#save-cluster-assignments",
    "title": "17  Cluster map units based on climate",
    "section": "17.10 Save cluster assignments",
    "text": "17.10 Save cluster assignments\nFor each version of the model (each value of k, different numbers of clusters), a MUKEY is assigned to a specific cluster. Here, I’m pulling that data, shaping it into one dataframe (one row per MUKEY, cluster assignments in separate columns). I’m also adding back in the soil property data so we can use this in the next step when evaluating different cluster sizes.\n\nclust_assign_df &lt;- mods %&gt;% \n  mutate(clust_assign = map(km_fit, ~augment(.x, new_data = d_incl)),\n         mukey_clust = map(clust_assign, ~select(.x, mukey, .pred_cluster)))\n\n\nassign_mukey_df &lt;- clust_assign_df %&gt;% \n  select(n_clust, mukey_clust) %&gt;% \n  unnest(mukey_clust) %&gt;% pivot_wider(names_from = n_clust, values_from = .pred_cluster, names_prefix = \"k_\")\n\n\nclust_props &lt;- full_join(d_incl, assign_mukey_df, by = \"mukey\")\n\nwrite_csv(clust_props, \"data/mukey_cluster_assignments_and_props_climate_only.csv\")",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "35-climate-clusters.html#map-clusters",
    "href": "35-climate-clusters.html#map-clusters",
    "title": "17  Cluster map units based on climate",
    "section": "17.11 Map clusters",
    "text": "17.11 Map clusters\n\nclust_long &lt;- clust_props %&gt;% \n  select(-mukey) %&gt;% \n  pivot_longer(cols = contains(\"k_\"),\n               names_to = \"model_ver\",\n               values_to = \"cluster_assignment\")\n\nclimsf &lt;- st_as_sf(clust_long, coords = c(\"x\",\"y\"))\n\nclimsf &lt;- st_set_crs(climsf, 4326) \n\n\nggplot(data = climsf) +\n  geom_sf(aes(color = cluster_assignment)) +\n # scale_color_viridis_d() + \n  facet_wrap(vars(model_ver)) +\n  ggtitle(\"Map unit centroids color coded by climate cluster\")\n\n\n\n\n\n\n\n\n\nclust_long %&gt;% \n  group_by(model_ver, cluster_assignment) %&gt;% \n  summarise(n = n(),\n            min_temp = min(anntavg_norm),\n            max_temp = max(anntavg_norm),\n            mean_temp = mean(anntavg_norm),\n            min_prcp = min(annprcp_norm),\n            max_prcp = max(annprcp_norm),\n            mean_prcp = mean(annprcp_norm)) %&gt;% \n  mutate(across(.cols = contains(\"temp\"), .fns = ~round(.x, digits = 1)),\n         across(.cols = contains('prcp'), .fns = ~round(.x, digits = 0))) %&gt;% \n  gt() %&gt;% \n  tab_spanner(label = \"MAT normal (Celsius)\",\n              columns = contains(\"temp\")) %&gt;% \n  tab_spanner(label = \"MAP normal (mm)\",\n              columns = contains(\"prcp\")) %&gt;%\n  cols_label(min_temp = \"Min.\",\n             max_temp = \"Max.\",\n             mean_temp = \"Mean\",\n             min_prcp = \"Min.\",\n             max_prcp = \"Max.\", \n             mean_prcp = \"Mean\")\n\n`summarise()` has grouped output by 'model_ver'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\ncluster_assignment\nn\nMAT normal (Celsius)\nMAP normal (mm)\n\n\nMin.\nMax.\nMean\nMin.\nMax.\nMean\n\n\n\n\nk_2\n\n\nCluster_1\n2435\n3.5\n6.8\n5.0\n549\n747\n640\n\n\nCluster_2\n4437\n5.9\n8.4\n7.0\n645\n962\n808\n\n\nk_3\n\n\nCluster_1\n1750\n3.5\n5.8\n4.6\n549\n724\n630\n\n\nCluster_2\n2656\n5.3\n7.4\n6.6\n619\n792\n717\n\n\nCluster_3\n2466\n6.7\n8.4\n7.3\n774\n962\n866\n\n\nk_4\n\n\nCluster_1\n1123\n3.5\n5.2\n4.3\n549\n676\n608\n\n\nCluster_2\n1340\n4.8\n6.6\n5.7\n586\n747\n671\n\n\nCluster_3\n2261\n6.0\n7.6\n6.8\n641\n806\n742\n\n\nCluster_4\n2148\n6.7\n8.4\n7.3\n798\n962\n877\n\n\nk_5\n\n\nCluster_1\n1101\n3.5\n5.2\n4.3\n549\n673\n607\n\n\nCluster_2\n1307\n4.8\n6.5\n5.6\n586\n747\n669\n\n\nCluster_3\n1304\n6.9\n7.8\n7.4\n761\n866\n817\n\n\nCluster_4\n1266\n6.7\n8.4\n7.1\n863\n962\n909\n\n\nCluster_5\n1894\n5.9\n7.4\n6.7\n641\n786\n731\n\n\nk_6\n\n\nCluster_1\n867\n3.5\n5.0\n4.1\n549\n663\n597\n\n\nCluster_2\n919\n4.4\n5.8\n5.1\n580\n738\n672\n\n\nCluster_3\n711\n5.6\n6.9\n6.1\n616\n730\n653\n\n\nCluster_4\n1290\n6.9\n7.8\n7.4\n761\n866\n818\n\n\nCluster_5\n1266\n6.7\n8.4\n7.1\n863\n962\n909\n\n\nCluster_6\n1819\n5.7\n7.4\n6.7\n664\n786\n735\n\n\n\n\n\n\n\n\n\nclust_long %&gt;% \n  mutate(cluster_assignment = str_replace(cluster_assignment, \"Cluster_\", \"C\")) %&gt;% \n  ggplot() +\n  geom_boxplot(aes(x = cluster_assignment, y = anntavg_norm)) +\n  facet_wrap(vars(model_ver), axes = \"all_x\", drop = TRUE, scales = \"free_x\") +\n  ggtitle(\"Annual temperature normals by model version & cluster\") +\n  ylab(\"Annual temp. normal (degrees C)\")\n\n\n\n\n\n\n\nclust_long %&gt;% \n  mutate(cluster_assignment = str_replace(cluster_assignment, \"Cluster_\", \"C\")) %&gt;% \n  ggplot() +\n  geom_boxplot(aes(x = cluster_assignment, y = annprcp_norm)) +\n  facet_wrap(vars(model_ver), axes = \"all_x\", drop = TRUE, scales = \"free_x\") +\n  ggtitle(\"Annual precipitation normals by model version & cluster\") +\n  ylab(\"Annual precip normal (mm)\")",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Cluster map units based on climate</span>"
    ]
  },
  {
    "objectID": "19-sample-points.html",
    "href": "19-sample-points.html",
    "title": "18  Sample validation points",
    "section": "",
    "text": "18.1 Validation datasets\nThere are 3 main validation datasets that I currently have access to:\nlibrary(tidyverse)\nlibrary(terra)\n\n# points to validate\nncss_dat &lt;- read_csv(\"data/validation_data/NCSS-KSSL/validation_ncss_kssl_0-20cm_aggr.csv\")\n\n# ended up making these matrices so I can add ids as an \"attribute\" (atts) in the vect function \nncss_pts &lt;- ncss_dat %&gt;% \n  select(longitude_decimal_degrees, latitude_decimal_degrees) %&gt;% \n  rename(lon = longitude_decimal_degrees,\n         lat = latitude_decimal_degrees) %&gt;% \n  as.matrix()\n\ncig_dat &lt;- read_csv(\"data/cig_lab_data_all_20230301.csv\")\n\ncig_pts &lt;- cig_dat %&gt;% \n  select(lon, lat) %&gt;% \n  as.matrix()\n\n# I am using these dfs to see if setting sample ID as an attribute in the\n# SpatVector below will allow me to identify the points more readily after\n# extract()\ncig_ids &lt;- data.frame(sample_id = cig_dat$sample_id,\n                      row.names = NULL)\n\nncss_ids &lt;- data.frame(pedon_key = ncss_dat$pedon_key)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sample validation points</span>"
    ]
  },
  {
    "objectID": "19-sample-points.html#validation-datasets",
    "href": "19-sample-points.html#validation-datasets",
    "title": "18  Sample validation points",
    "section": "",
    "text": "NRCS SHI project (Jess). Validation completed by JB, saved in data &gt; validation data &gt; NCRS-SHI &gt; shi_site_allvar_validation_20221116.csv\nCIG field data\nNCSS data from KSSL. I completed the data cleaning and merging process with the script unqip_merge_ncss_data.R. The dataset (all props averaged to 0-20cm) is saved in data &gt; validation data &gt; NCSS-KSSL &gt; validation_ncss_kssl_0-20cm_aggr.csv",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sample validation points</span>"
    ]
  },
  {
    "objectID": "19-sample-points.html#reproject-points-to-nad-83-albers",
    "href": "19-sample-points.html#reproject-points-to-nad-83-albers",
    "title": "18  Sample validation points",
    "section": "18.2 Reproject points to NAD 83 Albers",
    "text": "18.2 Reproject points to NAD 83 Albers\nMy rasters are in NAD 1983 Albers projection (EPSG: 5070). My points are in WGS 84 (EPSG: 4326, they are lon/lat). I need to reproject the points before I extract the cluster assignments from the rasters I made. This involves two steps:\n\nTurn my 2-column dataframes (lon, lat) into SpatVector objects\nReproject the SpatVector objects to the correction CRS\n\nFrom the {terra} documentation: “You can use a data.frame to make a SpatVector of points; or a”geom” matrix to make a SpatVector of any supported geometry (see examples and geom)\n\n# turn the CIG and NCSS points dfs to SpatVectors\ncig_spat &lt;- vect(x = cig_pts,\n     type = \"points\",\n     # EPSG:4326 is WGS84 long/lat\n     crs = \"epsg:4326\",\n     atts = cig_ids\n     )\n\nncss_spat &lt;- vect(x = ncss_pts,\n                  type = \"points\",\n                  # EPSG:4326 is WGS84 long/lat\n                  crs = \"epsg:4326\",\n                  atts = ncss_ids)\n\n# reproject the CIG and NCSS points to NAD 83 (EPSG 5070)\ncig_reproj &lt;- project(x = cig_spat,\n                      y = \"epsg:5070\")\n\ncig_reproj\n\n class       : SpatVector \n geometry    : points \n dimensions  : 486, 1  (geometries, attributes)\n extent      : -83294.4, 263752.6, 2296221, 2843821  (xmin, xmax, ymin, ymax)\n coord. ref. : NAD83 / Conus Albers (EPSG:5070) \n names       : sample_id\n type        :     &lt;num&gt;\n values      :         1\n                       2\n                       3\n\nncss_reproj &lt;- project(x = ncss_spat,\n                    y = \"epsg:5070\")\n\nncss_reproj\n\n class       : SpatVector \n geometry    : points \n dimensions  : 49, 1  (geometries, attributes)\n extent      : -56529.67, 395664.5, 2374854, 2848762  (xmin, xmax, ymin, ymax)\n coord. ref. : NAD83 / Conus Albers (EPSG:5070) \n names       : pedon_key\n type        :     &lt;chr&gt;\n values      :   02N0123\n                 02N0796\n                 02N0797",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sample validation points</span>"
    ]
  },
  {
    "objectID": "19-sample-points.html#sample-rasters-extract-value",
    "href": "19-sample-points.html#sample-rasters-extract-value",
    "title": "18  Sample validation points",
    "section": "18.3 Sample Rasters (Extract value)",
    "text": "18.3 Sample Rasters (Extract value)\nNeed to review relevant function/s from {terra} for doing the raster sampling. Otherwise I know how to do it already in QGIS.\nLooks like I want the extract() function. “Sample” has a different meaning in {terra}, (taking a spatial sample / regular sample)\nImportant arguments:\n\nx is the SpatRaster\ny (in my case) is a SpatVector, re-projected above\nmethod should be “simple”, because I want the value of the cell the point falls into\nxy set to TRUE (return coordinates with results)\nID set to TRUE (return IDs, record numbers, of input SpatVector y)\n\n\n18.3.1 An example\nOK, got this working after some silly troubleshooting with attributes and CRS (had the wrong EPSG code for NAD 1983 Albers to start out).\n\n# start with the k=2 model\ntest_rast &lt;- rast(\"D:/big-files-backup/ch03-sh-groups/clust2_allvar.tif\")\n\next_df &lt;- extract(x = test_rast,\n        y = cig_reproj,\n        method = \"simple\",\n        xy = TRUE,\n        ID = TRUE)  \n\n# also try it with the NCSS points, just to see:\nextract(x = test_rast,\n        y = ncss_reproj,\n        method = \"simple\", \n        xy = TRUE,\n        ID = TRUE)\n\n\n  \n\n\n\n\n\n18.3.2 Finished raster extract process (CIG, NCSS)\nThe script R/extract_raster_values_at_validation_points.R implements the process described above. At the end of that script, I save a CSV file in wide format with the CIG and NCSS_KSSl validation data cig_ncss-kssl_allvar_validation_20221230.csv.\nThe next step is to combine the validation data, which identifies the cluster assignments, with the soil property data.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sample validation points</span>"
    ]
  },
  {
    "objectID": "19-sample-points.html#number-points-per-dataset-soil-property",
    "href": "19-sample-points.html#number-points-per-dataset-soil-property",
    "title": "18  Sample validation points",
    "section": "18.4 Number points per dataset & soil property",
    "text": "18.4 Number points per dataset & soil property\nIn the manuscript, I’d like to report how many points came from each of our three datasets. Here is some code to determine this. The best place to start is the dataset that I prepped for pairwise comparisons using R/prep_validation_data_for_pairwise_comparisons.R. It has NA values removed and the CIG points have been reduced to independent points only.\n\nval_dat &lt;-  read_csv(\"data/validation_data_clusters_and_soil_props.csv\") %&gt;% \n  mutate(proj_id = case_when(\n    str_detect(val_unit_id, \"CC$\") ~ \"NRCS-SHI\",\n    str_detect(val_unit_id, \"[:digit:]{3}$\") ~ \"KSSL\",\n    TRUE ~ \"CIG\"\n  ))\n\nRows: 146 Columns: 34\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): val_unit_id\ndbl (33): k_10, k_11, k_12, k_13, k_14, k_15, k_16, k_17, k_18, k_19, k_2, k...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# number of validation pts per project\nval_dat %&gt;% \n  group_by(proj_id) %&gt;% \n  count()\n\n\n  \n\n\n# number of validation points overall\nnrow(val_dat)\n\n[1] 146\n\n\nOK, so how many points do we have for each of the soil properties we are using to calculate cluster means?\n\nval_dat %&gt;% \n  select(val_unit_id, claytotal, dbthirdbar, ph1to1h2o, caco3, om_loi) %&gt;% \n  summarise(across(.cols = -val_unit_id,\n                   .fns = \\(xcol) sum(!is.na(xcol))))",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Sample validation points</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html",
    "href": "29-welch-pairwise.html",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "",
    "text": "19.1 Overview\nFirst did some data processing in prep_validation_data_for_pca_versions.R to prepare for the analyses I’m running in this chapter.\nDid some more reading on March 15, 2023 and decided a better way to deal with the issue of hetergeneous variance is to do Welch’s ANOVA followed by Games-Howell pairwise comparisons.\nSee Lakens et al. (2019) “Taking Parametric Assumptions Seriously: Arguments for the Use of Welch’s F-test instead of the Classical F-test in One-Way ANOVA” for more info on Welch’s F-test.\nSee Sauder & Demars (2019) plus their supplemental material on OSF for more about the Games-Howell test\nIn R, Wech’s ANOVA can be accomplished with oneway.test(…, var.equal = FALSE) , or {rstatix} has a wrapper function welch_anova_test().\nFor Games-Howell, {rstatix} has games_howell_test() documentation here.\nFrom the documentation (emphasis added):",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#overview",
    "href": "29-welch-pairwise.html#overview",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "",
    "text": "Performs Games-Howell test, which is used to compare all possible combinations of group differences when the assumption of homogeneity of variances is violated. This post hoc test provides confidence intervals for the differences between group means and shows whether the differences are statistically significant.\nThe test is based on Welch’s degrees of freedom correction and uses Tukey’s studentized range distribution for computing the p-values. The test compares the difference between each pair of means with appropriate adjustment for the multiple testing. So there is no need to apply additional p-value corrections.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#validation-points-per-cluster",
    "href": "29-welch-pairwise.html#validation-points-per-cluster",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.2 Validation points per cluster",
    "text": "19.2 Validation points per cluster\nHere I am illustrating how many independent validation points we have for each cluster assignment, for the different model options from k = 4, 6, 8.\nNote that clusters with only 1 member won’t be included in pairwise comparison b/c not possible to calculate variance with only 1 member…",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#soil-properties-for-pairwise-comparisons",
    "href": "29-welch-pairwise.html#soil-properties-for-pairwise-comparisons",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.3 Soil properties for pairwise comparisons",
    "text": "19.3 Soil properties for pairwise comparisons\nBecause our validation data points are coming from different projects / datasets, we don’t have exactly the same variables from each project. This is a reminder of which variables exist in the three sources we used for validation points:\n\nKSSL : clay, bulk density, lep, awc, ec, cec, pH, carbonates, organic matter, (est org C)\nCIG: clay, bulk density, pH, carbonates, organic matter, (est org C)\nSHI: clay, bulk density, pH, organic matter\n\nIn summary: all three datasets include bulk density, pH, organic matter, and clay, and KSSL and CIG include carbonates. So I think it makes sense to focus on plotting and doing pairwise comparisons with these variables specifically",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#example-pairwise-om-k6",
    "href": "29-welch-pairwise.html#example-pairwise-om-k6",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.4 Example pairwise: OM, k=6",
    "text": "19.4 Example pairwise: OM, k=6\nWorking out the steps I need to include in a function to do the pairwise comparisons.\n\n# test case k=6 version and organic matter\n\ntest_dat &lt;- val_dat %&gt;% \n  select(val_unit_id, k_6, om_loi, claytotal, source) %&gt;% \n  drop_na(om_loi)\n\n# note only 1 observation for clust_1, need to drop it\n# b/c can't calculate variance for the pairwise comparison w/ only 1 obs.\ntest_dat %&gt;% count(k_6)\n\n\n  \n\n\ntest_dat_mult &lt;- test_dat %&gt;% filter(k_6 != \"clust_1\")\n\n# plot to see distributions \ntest_dat_mult %&gt;% \n  ggplot(aes(x = k_6, y = om_loi)) + \n  geom_boxplot() + \n  geom_point() + \n  theme_bw()\n\n\n\n\n\n\n\ntest_lm &lt;- lm(formula = om_loi ~ k_6,\n   data = test_dat_mult)\n\n# look at some diagnostic plots for our model \n# note homogeneity of variance looks sketchy\nperformance::check_model(test_lm, check = c(\"normality\", \"homogeneity\", \"linearity\")) \n\n\n\n\n\n\n\n# runs Bartlett Test for homogeneity of variance\nperformance::check_homogeneity(test_lm) \n\nWarning: Variances differ between groups (Bartlett Test, p = 0.000).\n\n\nThis shows how I would do Games-Howell pairwise comparisons\n\ngh_om &lt;- games_howell_test(formula = om_loi ~ k_6,\n                  data = test_dat_mult)\n\ngh_om\n\n\n  \n\n\n# now we add a column \"comparison\" needed in order for \n# biostat::cld to make our \"compact letter display\" (letters\n# showing where we can/can't reject null that means are the same)\n# pairwise comps w/ Dunn's test \n\ngh_comp &lt;- gh_om %&gt;% \n  mutate(comparison = glue(\"{group1}-{group2}\"))\n\n\n#biostat::make_cld(p.adj ~ comparison, data = gh_comp)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#function-to-plot-model-checks",
    "href": "29-welch-pairwise.html#function-to-plot-model-checks",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.5 Function to plot model checks",
    "text": "19.5 Function to plot model checks\nI want to make some plots to check the assumptions of normally distributed residuals and homogeneity of variance. In chapter 27, where I show the non-parametric pairwise tests, I also have a section for running Levene’s test to check homogeneity of variance. However, since Welch’s ANOVA and Games-Howell make no assumptions about homogeneity variance, I’m not worried about running Levene’s test here.\n\n# working with these variables for the model checks\n# independent:  k= 4, 6, 8\n# dependent:  vars = om_loi, dbthirdbar,  ph1to1h2o, caco3, claytotal\n\ncheck_plots_anova &lt;- function(soil_var, k_opt, df, log_trans_adj = FALSE) {\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # checking if any clusters are represented by 1 or less data points. Want to drop these\n  \n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n  \n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[k_opt]])\n  \n  if (length(single_obs_clusters) == 0) {\n    dat_subset &lt;- dat_no_na\n    \n  } else{\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n    \n  }\n  \n  if(log_trans_adj){\n    \n    # note this is for CaCO3, need to add 1 to avoid\n    # zero values\n    f &lt;- paste0(\"log10(\", soil_var, \"+1)~\", k_opt)\n    \n  }else{\n    \n    f &lt;- paste0(soil_var, \" ~ \", k_opt)\n    \n  }\n  \n  mod &lt;- lm(formula = f,\n            data = dat_subset)\n  \n  check_plots &lt;- performance::check_model(mod, check = c(\"normality\", \"homogeneity\", \"linearity\"))  \n  \n  return(list(var = glue(\"{soil_var}\"),\n              plots = check_plots))\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#k4-model-checks",
    "href": "29-welch-pairwise.html#k4-model-checks",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.6 k=4 model checks",
    "text": "19.6 k=4 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_4\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\n\n\n\n\n# note we need to log10 transform here (+1) to meet the assumption of normally distributed residuals\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_4\", \n                  df = val_dat,\n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#k6-model-checks",
    "href": "29-welch-pairwise.html#k6-model-checks",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.7 k=6 model checks",
    "text": "19.7 k=6 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_6\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_6\", \n                  df = val_dat,\n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#k8-model-checks",
    "href": "29-welch-pairwise.html#k8-model-checks",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.8 k=8 model checks",
    "text": "19.8 k=8 model checks\n\ncheck_plots_anova(soil_var = \"claytotal\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nclaytotal\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"ph1to1h2o\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nph1to1h2o\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"dbthirdbar\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\ndbthirdbar\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"om_loi\",\n                  k_opt = \"k_8\", \n                  df = val_dat)\n\n$var\nom_loi\n\n$plots\n\n\n\n\n\n\n\n\ncheck_plots_anova(soil_var = \"caco3\",\n                  k_opt = \"k_8\", \n                  df = val_dat, \n                  log_trans_adj = TRUE)\n\n$var\ncaco3\n\n$plots",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#function-for-games-howell-pairwise-comparisons",
    "href": "29-welch-pairwise.html#function-for-games-howell-pairwise-comparisons",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.9 Function for Games-Howell pairwise comparisons",
    "text": "19.9 Function for Games-Howell pairwise comparisons\nArguments: soil property and k option (model version / number of clusters) and validation dataframe.\nReturns: all pairwise comparisons in a dataframe\n\ncompare_clust_pairwise &lt;- function(soil_var, k_opt, df = val_dat) {\n  ### prep data\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(k_opt),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # can't do pairwise t-tests with only 1 obs in a\n  # group, so need to filter those out\n  \n  n_obs_per_cluster &lt;- dat_no_na %&gt;%\n    count(.data[[k_opt]])\n  \n  single_obs_clusters &lt;- n_obs_per_cluster %&gt;%\n    filter(n &lt;= 1) %&gt;%\n    pull(.data[[k_opt]])\n  \n  if (length(single_obs_clusters) == 0) {\n    dat_subset &lt;- dat_no_na\n    \n  } else{\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[k_opt]] %in% single_obs_clusters))\n    \n  }\n  \n  ### perform pairwise comparison\n  \n    f &lt;- as.formula(paste0(soil_var, \" ~ \", k_opt))\n  \n  pairs_df &lt;- games_howell_test(data = dat_subset,\n                                formula = f) %&gt;%\n    mutate(comparison = glue(\"{group1} - {group2}\"))\n  \n  cld_df &lt;- biostat::make_cld(p.adj ~ comparison,\n                              data = pairs_df)\n  \n  ### results\n  \n  results_list &lt;- list(pairs_df = pairs_df,\n                       cld = cld_df)\n  \n  return(results_list)\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#example-function-output",
    "href": "29-welch-pairwise.html#example-function-output",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.10 Example function output",
    "text": "19.10 Example function output\nThis is what the pairwise function I wrote above returns:\n\nph_test &lt;- compare_clust_pairwise(soil_var = \"ph1to1h2o\",\n                       k_opt = \"k_8\",\n                       df = val_dat) \n\nph_test\n\n$pairs_df\n# A tibble: 21 × 9\n   .y.       group1  group2  estimate conf.low conf.high    p.adj p.adj.signif\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n 1 ph1to1h2o clust_1 clust_2 -0.119     -1.66      1.42  1   e+ 0 ns          \n 2 ph1to1h2o clust_1 clust_3  0.00222   -1.50      1.50  1   e+ 0 ns          \n 3 ph1to1h2o clust_1 clust_4  1.16      -0.399     2.71  1.32e- 1 ns          \n 4 ph1to1h2o clust_1 clust_5  0.566     -0.926     2.06  6.68e- 1 ns          \n 5 ph1to1h2o clust_1 clust_6  1.39      -0.165     2.94  7.4 e- 2 ns          \n 6 ph1to1h2o clust_1 clust_7  0.625     -0.865     2.12  5.87e- 1 ns          \n 7 ph1to1h2o clust_2 clust_3  0.122     -0.451     0.694 9.81e- 1 ns          \n 8 ph1to1h2o clust_2 clust_4  1.28       0.953     1.60  0        ****        \n 9 ph1to1h2o clust_2 clust_5  0.685      0.220     1.15  6.27e- 4 ***         \n10 ph1to1h2o clust_2 clust_6  1.51       1.18      1.84  9.92e-13 ****        \n# ℹ 11 more rows\n# ℹ 1 more variable: comparison &lt;glue&gt;\n\n$cld\n    group cld spaced_cld\n1 clust_1 abc        abc\n2 clust_2   a        a__\n3 clust_3  ab        ab_\n4 clust_4   c        __c\n5 clust_5   b        _b_\n6 clust_6   c        __c\n7 clust_7   b        _b_\n\ncomps &lt;- ph_test[[\"pairs_df\"]]\n\ncomps %&gt;% \n  select(group1, group2, p.adj) %&gt;% \n  filter(p.adj &lt; 0.05)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#run-pairwise-comparisons",
    "href": "29-welch-pairwise.html#run-pairwise-comparisons",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.11 Run pairwise comparisons",
    "text": "19.11 Run pairwise comparisons\nHere I create a dataframe to hold the results of the pairwise comparisons, then use map2() to iterate over the variables and cluster sizes, running all the tests.\n\n# transform CaCO3 in the dataframe beforehand\nval_dat_trans &lt;- val_dat %&gt;% \n  mutate(caco3 = log10(caco3+1)) \n\n# dependent vars\nsoil_var &lt;- c(\"claytotal\", \"ph1to1h2o\", \"om_loi\", \"caco3\", \"dbthirdbar\")\n\n# independent vars all possible values of k (number of clusters)\nk_opt &lt;- as.character(glue(\"k_{c(4, 5, 6, 7, 8, 9, 10, 11)}\"))\n\n# create df with all combinations of var_names x clusters\nmod_template &lt;- tidyr::crossing(soil_var, k_opt)\n\n\n# run the pairwise comparisons for each var and cluster size\ndiffs_df &lt;- mod_template %&gt;%\n  mutate(comps_all = map2(\n    .x = soil_var,\n    .y = k_opt,\n    .f = compare_clust_pairwise\n  ))\n\n\n\n# unnest once to get the pairs_df and cld_df dfs \n# as their own columns\ndiffs_unnest &lt;- diffs_df %&gt;% \n  unnest(comps_all) %&gt;% \n  mutate(obj_names = names(comps_all))\n\n# want to save pairs_df and cld_df dat separately, so filtering\n# and then unnesting again to get rectangular data \npairs_dat &lt;- diffs_unnest %&gt;% \n  filter(obj_names == \"pairs_df\") %&gt;% \n  unnest(comps_all) %&gt;% \n  select(-obj_names)\n\nwrite_csv(pairs_dat, \"data/pca_welch_pairwise_results_all.csv\")\n\ncld_dat &lt;- diffs_unnest %&gt;% \n  filter(obj_names == \"cld\") %&gt;% \n  unnest(comps_all) %&gt;% \n  select(-obj_names)\n\nwrite_csv(cld_dat, \"data/cld_display_welch.csv\")\n\nNow need to count how many of the tests have an adjusted p-value &lt; 0.05. All of the p-values are already adjusted (see documentation for games_howell_test()\n\ncount_sig_comps &lt;- function(df){\n  \n  df %&gt;% \n    filter(p.adj&lt;0.05) %&gt;% \n    nrow()\n  \n}\n\nsig_diffs_df &lt;- pairs_dat %&gt;%\n  select(k_opt, soil_var, group1, group2, p.adj) %&gt;% \n  group_by(k_opt, soil_var) %&gt;%\n  nest(data = c(group1, group2, p.adj)) %&gt;%\n  mutate(n_sig_comps = map_int(data, count_sig_comps)) %&gt;%\n  mutate(\n    num_regions = as.numeric(str_extract(k_opt, \"[:digit:]+\")),\n    possible_comps = (num_regions * (num_regions - 1)) / 2,\n    alpha_comps = round(possible_comps * 0.05, digits = 0)\n  ) %&gt;%\n  select(soil_var,\n         k_opt,\n         num_regions,\n         data,\n         n_sig_comps,\n         possible_comps,\n         alpha_comps)\n\nsig_diffs_summary &lt;- sig_diffs_df %&gt;% \n  select(soil_var,\n         k_opt,\n         num_regions,\n         n_sig_comps,\n         possible_comps,\n         alpha_comps)\n\nwrite_csv(sig_diffs_summary, \"data/pca_welch_pairwise_comparisons_summary.csv\")",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "29-welch-pairwise.html#plot-comparisons",
    "href": "29-welch-pairwise.html#plot-comparisons",
    "title": "19  Welch ANOVA for PCA cluster pairwise comparisons",
    "section": "19.12 Plot comparisons",
    "text": "19.12 Plot comparisons\n\n19.12.1 All clusters\n\n\n\n\n\n\n\n\n\nFor context, this plot shows a black line for the total number of possible contrasts. Note that because we are showing each soil property variable separately, the “total possible” line illustrates the total number of possible comparisons for a single variable\n\n\n\n\n\n\n\n\n\nTwo other ways to contextualize the number of significant contrasts: 1) with a table, and 2) with a plot showing how the % significant contrasts (as a function of total possible) changes as the number of clusters goes up.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Welch ANOVA for PCA cluster pairwise comparisons</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html",
    "href": "36-update-welch-compare-variation.html",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "",
    "text": "20.1 Overview\nThis chapter is similar to 28 (compare variation demo), but here am using Welch ANOVA followed by calculation of epsilon squared as an effect size (which we can do because {effectsize} uses the F stats from the Welch’s ANOVA to estimate epsilon squared).\nThe goal is to use selected soil health indicators from the CIG dataset to compare the amount of variation explained when grouping the data by MLRA vs. taxonomy vs. soil health group +climate cluster (generated by k-means clustering on soils and climate data separately).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#models",
    "href": "36-update-welch-compare-variation.html#models",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.2 Models",
    "text": "20.2 Models\nResponse variables: soil health indicators (perhaps the NRCS tech note list?). Should we use just 1 year of data to avoid pulling in year-to-year variability? Could choose 2019, because this is the year for which we have PLFA data, and PLFA is on the NRCS list.\nVariables from the NRCS Soil Health Tech Note recommended methods:\n\nAggregate stability\nPMC\nPOXC\n(skip - weird thing with RRV poor extraction in CIG data) ACE Protein\nPLFA\n\n\nlibrary(tidyverse)\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(glue)\nlibrary(ggbeeswarm)\n\nWarning: package 'ggbeeswarm' was built under R version 4.3.3\n\nlibrary(lmerTest)\n\nWarning: package 'lmerTest' was built under R version 4.3.1\n\n\nLoading required package: lme4\n\n\nWarning: package 'lme4' was built under R version 4.3.1\n\n\nLoading required package: Matrix\n\n\nWarning: package 'Matrix' was built under R version 4.3.1\n\n\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nAttaching package: 'lmerTest'\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(rstatix)\n\nWarning: package 'rstatix' was built under R version 4.3.1\n\n\n\nAttaching package: 'rstatix'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.2\n\nlibrary(performance)\nlibrary(openxlsx)\n\nWarning: package 'openxlsx' was built under R version 4.3.1",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#data",
    "href": "36-update-welch-compare-variation.html#data",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.3 Data",
    "text": "20.3 Data\nAs part of the validation process for my k-means analysis, I determined the soil health group assigned to each CIG sampling area at the mapunit level (hillslope within management condition).\nThis file has MUKEYs associated with the CIG and NCSS validation points: cig_ncss_validation_pt_mukey_sample_id.csv\nAnd this file has the MUKEYs assocaited wit the SHI points: data/validation_data/NRCS-SHI/shi_site_pca_validation_points.csv\n\n# this has climate clusters by mukey, need to join with the other validation data\nclim_mukeys &lt;- read_csv(\"data/mukey_cluster_assignments_and_props_climate_only.csv\") %&gt;% \n  rename_with(.fn = ~str_replace(.x, \"k_\", \"clim_k_\"), .cols = contains(\"k_\")) %&gt;% \n  select(mukey, contains(\"clim_k_\"))\n\nRows: 6872 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): k_2, k_3, k_4, k_5, k_6\ndbl (6): mukey_short, anntavg_norm, annprcp_norm, x, y, mukey\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n20.3.1 Load data\n\n# this file created in \"extract_raster_values_at_validation_points.R\"\n# it is at the \"sample\" level. There are often multiple rows\n# per CIG sample because many of the map units that contain these \n# sample points have multiple components included in the analysis. \ncig_comps &lt;- read_csv(\"data/cig_incl_components_table.csv\") %&gt;% \n  # for now, just using 2019 data so we avoid the year-to-year variability we\n  # know is there, and b/c we have PLFA data for 2019 only\n  filter(sample_id %in% c(1:243))\n\n\n# this is for disambiguating which MUKEY should be assigned\n# to each CIG val_unit_id (mapunit), because sometimes the\n# GPS points landed in multiple mapunits\n# the code that handles this is in \"extract_raster_values_at_validation_points.R\ncig_mukey_votes &lt;- read_csv(\"data/cig_mukey_votes_for_validation.csv\") \n\n# cluster assigned to each CIG point. this dataset is created in \"prep_validation_data_for_pca_version.R\"\n# it is at the \"validation unit\" level (summarized to hillslope within management)\ncig_clus &lt;- read_csv(\"data/validation_data_pca_clusters_and_soil_props.csv\") %&gt;% \n  filter(str_detect(val_unit_id, \"CV\") |\n           str_detect(val_unit_id, \"SH\") |\n         str_detect(val_unit_id, \"UD\")) %&gt;%\n  mutate(region = str_extract(val_unit_id, \"[:alpha:]{2}\"),\n         soil_group = glue(\"grp-{k_8}\")) %&gt;% \n  select(val_unit_id, soil_group, k_8, region)\n\n# currently from csv saved by \"heatmap_validation_counties.R\", \n# but need to move this to a more logical place \ncig_mlras &lt;- read_csv(\"data/cig_mlra_by_val_unit_id.csv\") \n\ncig_val &lt;- left_join(cig_clus, cig_mlras, by = \"val_unit_id\") %&gt;%\n  # need to fix this NA created by the join, it happened\n  # because this RR4-CV site had a label change from A to B\n  # during the harmonization I did to align soil textures\n  mutate(\n    MLRA_NAME = case_when(\n      val_unit_id == \"RR4-CV-B\" ~ \"Red River Valley of the North\",\n      TRUE ~ MLRA_NAME\n    ),\n    mlra_short = case_when(\n      str_detect(MLRA_NAME, \"Eastern Iowa\") ~ \"E IA MN Till\",\n      str_detect(MLRA_NAME, \"Red River\") ~ \"Red River\",\n      str_detect(MLRA_NAME, \"Rolling Till Prairie\") ~ \"Rol Till Pr\",\n      str_detect(MLRA_NAME, \"Central Min\") ~ \"C MN Sandy\",\n      str_detect(MLRA_NAME, \"Gray Drift\") ~ \"N MN Gray\",\n      str_detect(MLRA_NAME, \"Central Iowa\") ~ \"C IA MN Till\",\n      TRUE ~ \"Fix me\"\n    )\n  )\n\n# want all the CIG lab observations (not averaged to the plot level)\nlab &lt;- read_csv(\"data/cig_lab_data_all_20230301.csv\") %&gt;%\n  mutate(val_unit_id = glue(\"{site}-{treatment}-{position}\")) %&gt;%\n  select(\n    sample_id,\n    val_unit_id,\n    region,\n    position,\n    corr_percent_stable_gr2mm,\n    ugC_g_day,\n    BG_activity,\n    NAG_activity,\n    P_activity,\n    poxc_mg_kg,\n    mean_protein_mg_g,\n    org_c_wt_percent, # added per reviewer 1 comment\n    mbc_ug_g_soil,\n    mbn_ug_g_soil,\n    total_living_microbial_biomass_plfa_ng_g,\n    total_bacteria_plfa_ng_g,\n    total_fungi_plfa_ng_g,\n    total_bacteria_percent_of_tot_biomass,\n    total_fungi_percent_of_tot_biomass\n  ) %&gt;% \n  filter(sample_id %in% c(1:243))\n\n# missing data due to sample contamination in 2019 samples\n# that were run for PLFA. Pulling MBN values from 2020\n# for this validation site only after confirming that the values \n# are in a similar range for this soil type, hillslope position,\n# and region\nmbn_mw4_cva &lt;-\n  read_csv(\"data/cig_lab_data_all_20230301.csv\") %&gt;%\n  mutate(val_unit_id = glue(\"{site}-{treatment}-{position}\")) %&gt;%\n  filter(sample_id %in% c(474:476)) %&gt;%\n  pull(mbn_ug_g_soil) %&gt;%\n  mean(., na.rm = TRUE)\n\n\n\n20.3.2 Summarize to mapunit level\nRecall that the lab data and the component data need to be summarized to the “mapunit” level. This involves averaging the soil properties and determining the majority taxonomic classification for each MUKEY based on the included components (recall that most MUKEYs have just 1 contributing component, so we just need to account for those that have &gt;1 contributing component to determine, for each level of taxonomy I look at, what the majority assignment should be).\nStarting with the taxonomy data:\n\n# how many unique mukeys are we working with?\n(cig_mukeys &lt;- cig_comps %&gt;%\n    pull(mukey) %&gt;%\n    unique() %&gt;%\n    length())\n\n[1] 37\n\n# Q: which mukeys have multiple components contributing data?\n# A: 16 unique mukeys\ncig_comps %&gt;% \n  select(mukey, cokey, taxclname:taxpartsize) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey) %&gt;% \n  count() %&gt;% \n  filter(n&gt;1)\n\n\n  \n\n\n# goal: assign each mukey a representative suborder\n# based on the suborders of the components within it.\n# should get back object w/ length 37 (# of unique mukeys)\ncig_comps %&gt;% \n  select(mukey, cokey, comppct_r, taxsuborder) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey, taxsuborder) %&gt;% \n  summarise(tot_percent = sum(comppct_r), .groups = \"drop\") %&gt;% \n  group_by(mukey) %&gt;% \n  slice_max(tot_percent)\n\n\n  \n\n\n\nFunction for determining representative taxonomic level . Using similar code to the example with suborder above, this function allows me to summarize the representative taxonomic level (order, suborder, etc.) based on % area.\n\n# calculate representative taxonomy\ncalc_rep_tax &lt;- function(tax_level, comp_df){\n  \n  comp_df %&gt;% \n  select(mukey, cokey, comppct_r, {{tax_level}}) %&gt;% \n  distinct() %&gt;% \n  group_by(mukey, {{tax_level}}) %&gt;% \n  summarise(tot_percent = sum(comppct_r), .groups = \"drop\") %&gt;% \n  group_by(mukey) %&gt;% \n  slice_max(tot_percent) %&gt;% \n    select(-tot_percent)\n  \n}\n\n\norder &lt;- calc_rep_tax(taxorder, cig_comps)\nsuborder &lt;- calc_rep_tax(taxsuborder, cig_comps)\ngrt_grp &lt;- calc_rep_tax(taxgrtgroup, cig_comps)\nsub_grp &lt;- calc_rep_tax(taxsubgrp, cig_comps)\nfamily &lt;- calc_rep_tax(taxclname, cig_comps)\n\ntax_joined &lt;- list(order, suborder, grt_grp, sub_grp, family) %&gt;% \n  purrr::reduce(., left_join, by = \"mukey\")\n\ntax_val_key &lt;- cig_comps %&gt;% \n  select(val_unit_id, mukey) %&gt;% \n  distinct()\n\nNow time to summarize the lab data to the mapunit level, this is easier than the taxonomy because we can just take the average. Recall that we are working with 2019 data only.\n\nlab_summary &lt;- lab %&gt;% \n  select(-sample_id) %&gt;% \n  group_by(val_unit_id) %&gt;% \n  summarise(across(where(is.numeric),\n                   ~mean(.x, na.rm = TRUE)))\n\n\n\n20.3.3 Join taxonomic and lab info with validation dataset\nHere, we create a dataset with one row for each validation unit in the CIG dataset. It includes information about representative taxonomic classification at different levels (order, suborder, great group) and also representative values for the soil health indicators we want to analyze further below when comparing variance explained by different stratification options (region, taxonomy, k-means group).\n\nval_mukey &lt;- left_join(cig_val, cig_mukey_votes, by = \"val_unit_id\") %&gt;% \n  select(-c(n, max_vote))\n\nval_mukey_lab &lt;- left_join(val_mukey, lab_summary, by = \"val_unit_id\")\n\nval_all &lt;- left_join(val_mukey_lab, tax_joined, by = \"mukey\") %&gt;%\n # on list from old key, this id no longer exists\n   filter(val_unit_id != \"RR4-CV-B\") %&gt;% \n  mutate(mbn_ug_g_soil = case_when(\n    # dealing with missing data, see note at end of \n    # \"load data\" code block\n    val_unit_id == \"MW4-CV-A\" ~ mbn_mw4_cva,\n    TRUE ~ mbn_ug_g_soil\n  ))\n\n\nval_all_clim &lt;- left_join(val_all, clim_mukeys, by = \"mukey\") %&gt;% \n  mutate(clim_k_3 = case_when(\n    clim_k_3 == \"Cluster_1\" ~ \"Northwest\",\n    clim_k_3 == \"Cluster_2\" ~ \"Central\",\n    clim_k_3 == \"Cluster_3\" ~ \"Southeast\"\n  ))",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#response-variables---exploratory-plots",
    "href": "36-update-welch-compare-variation.html#response-variables---exploratory-plots",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.4 Response variables - exploratory plots",
    "text": "20.4 Response variables - exploratory plots\n\nrvars &lt;- c(\"corr_percent_stable_gr2mm\",\n           \"org_c_wt_percent\",\n               \"ugC_g_day\",\n               \"poxc_mg_kg\",\n               \"mbc_ug_g_soil\",\n               \"mbn_ug_g_soil\", # include MBN?\n               \"total_living_microbial_biomass_plfa_ng_g\",\n               \"total_bacteria_plfa_ng_g\",\n               \"total_fungi_plfa_ng_g\"\n)\n\n\nval_long &lt;- val_all %&gt;% \n  select(val_unit_id, all_of(rvars)) %&gt;% \n  pivot_longer(-val_unit_id) %&gt;% \n  mutate( name = case_when(\n    name == \"corr_percent_stable_gr2mm\" ~ \"agg_stab\",\n    name == \"ugC_g_day\" ~ \"pmc_ugCgday\",\n    name == \"total_living_microbial_biomass_plfa_ng_g\" ~ \"PLFA total\",\n    name == \"total_bacteria_plfa_ng_g\" ~ \"PLFA bacteria\",\n    name == \"total_fungi_plfa_ng_g\" ~ \"PLFA fungi\",\n    name == \"org_c_wt_percent\" ~ \"SOC\",\n    TRUE ~ name\n    \n  ))\n\nval_long %&gt;% \n  ggplot() + \n  geom_histogram(aes(value), bins = 20) +\n  facet_wrap(vars(name), scales = \"free\") +\n  ggtitle(\"Distribution (not transformed)\")\n\n\n\n\n\n\n\nval_all_clim %&gt;% \n  select(val_unit_id, contains(\"percent_of\")) %&gt;% \n  pivot_longer(-val_unit_id) %&gt;% \n  ggplot() +\n  geom_histogram(aes(value), bins = 20) + \n  facet_wrap(vars(name))\n\n\n\n\n\n\n\nval_all_clim %&gt;% \n  select(val_unit_id, contains(\"percent_of\")) %&gt;% \n  pivot_longer(-val_unit_id) %&gt;% \n  group_by(name) %&gt;% \n  summarise(min = min(value), \n            max = max(value))",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#stratification-options",
    "href": "36-update-welch-compare-variation.html#stratification-options",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.5 Stratification options",
    "text": "20.5 Stratification options\nWe could stratify the CIG points based on several different levels of Soil Taxonomy. How do the CIG sampling points break down in terms of number per cluster, and number per taxonomic level(s) (order, suborder, etc)?\nFor k-means clusters/groups: here we see that there is only 1 representative from group 4, so probably won’t be able to use that group for any calculations. But the others are workable.\nFor the different taxonomic levels, suborder seems workable, it has &gt;= 5 validation units in any given level. I think order is too broad, and by the time we get to sub-group, we have two subgroups with just 1 validation unit each, so would have to drop those.\n\n20.5.1 Original tables\n\nFor soil cluster, suborder, and MLRA the minimum number of validation points we have in any given group is 5\n\n\nval_all_clim %&gt;% \n  count(k_8)\n\n\n  \n\n\nval_all_clim %&gt;% \n  count(mlra_short)\n\n\n  \n\n\nval_all_clim %&gt;% \n  count(taxorder)\n\n\n  \n\n\nval_all_clim %&gt;% \n  count(taxsuborder) \n\n\n  \n\n\nval_all_clim %&gt;% \n  count(taxgrtgroup)\n\n\n  \n\n\nval_all_clim %&gt;% \n  count(taxsubgrp)\n\n\n  \n\n\nval_all_clim %&gt;% count(taxclname)\n\n\n  \n\n\nxtabs(~region+taxorder, data = val_all_clim)\n\n      taxorder\nregion Alfisols Mollisols Vertisols\n    MW       13        11         0\n    RR        0        10        11\n    ST       13         7         0\n    SW        0        15         0\n\n\n\n\n20.5.2 New combinations in response to reviewer comments\n\nsoil cluster (8) + climate zone (3) = 9 classes to compare in our validation data\n\nMinimum number validation pts in group = 1 (soil cluster 4) which was true above too and we dropped it. After that the minimum number is n=5 validation points\n9 categories total, 8 if we drop the one with n=1 validation point\n\nMLRA + suborder = 10 classes to compare in our validation data (drop n=1, n=2 classes)\n\nCentral MN Sandy Outwash x Aquolls n=1\nRolling Till Prairie Aquolls n = 2\nCentral MN Sandy Outwash Udolls = 4\n12 categories total, 10 if we drop the categories listed above with n=1 and n=2 members\n\nsuborder + family texture = 7 classes to compare in our validation data\n\nAquerts x very fine n=3\n“The descriptive terms in the names of families are given in a consistent order, which is as follows: particle-size class, mineralogy class, cation-exchange activity class, calcareous and reaction class, soil temperature class, soil depth class, rupture resistance class, classes of coatings, and classes of cracks.” Pg 157 Soil Taxonomy 2nd ed. 1999\n7 categories total\n\nsuborder + climate zone(3) = 8 classes to compare in our validation data\n\nAqualfs x climate cluster 3 n=5\n8 categories total\n\ngreat group + climate zone(3) = 10 classes to compare in our validation data\n\nArgiaquolls x climate cluster 2 n=3, then two groups with n=5 validation points\n10 categories total\n\n\n\n# soil cluster & climate cluster\nval_all_clim %&gt;% \n  group_by(k_8, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  select(clim_k_3, k_8, n_val_pts, val_pt_ids) %&gt;% \n  arrange(clim_k_3) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Soil cluster within climate zone\") %&gt;% \n  cols_label(k_8 = \"Soil cluster\",\n             clim_k_3 = \"Climate zone\",\n             n_val_pts = \"Validation pts (n)\",\n             val_pt_ids = \"Validation pt IDs\")\n\n\n\n\n\n\n\n\nSoil cluster within climate zone\n\n\nClimate zone\nSoil cluster\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nCentral\n2\n11\nST1-CV-A, ST1-CV-B, ST2-CV-B, ST2-SH-B, ST3-CV-B, ST3-SH-B, SW1-SH-A, SW1-UD-A, SW2-SH-B, SW2-UD-A, SW3-SH-A\n\n\nCentral\n3\n6\nST1-UD-A, ST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nCentral\n4\n1\nSW3-SH-B\n\n\nCentral\n5\n5\nSW1-CV-B, SW1-SH-B, SW2-CV-B, SW3-CV-B, SW3-UD-B\n\n\nCentral\n7\n12\nST1-SH-A, ST1-SH-B, ST2-CV-A, ST2-SH-A, ST2-UD-A, ST3-CV-A, ST3-SH-A, ST3-UD-A, SW1-CV-A, SW2-CV-A, SW2-SH-A, SW3-CV-A\n\n\nNorthwest\n5\n11\nRR1-CV-B, RR1-SH-B, RR1-UD-B, RR3-CV-A, RR3-SH-B, RR4-CV-A, RR4-SH-A, RR4-SH-B, RR4-UD-A, RR5-SH-B, RR5-UD-B\n\n\nNorthwest\n6\n10\nRR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nSoutheast\n2\n13\nMW1-CV-A, MW1-SH-A, MW1-UD-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A\n\n\nSoutheast\n5\n11\nMW1-CV-B, MW1-SH-B, MW1-UD-B, MW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B\n\n\n\n\n\n\n\nclust_clim_gt &lt;- val_all_clim %&gt;% \n  group_by(k_8, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  select(clim_k_3, k_8, n_val_pts) %&gt;% \n  arrange(clim_k_3) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Soil cluster within climate zone\") %&gt;% \n  cols_label(k_8 = \"Soil cluster\",\n             clim_k_3 = \"Climate zone\",\n             n_val_pts = \"Validation pts (n)\") %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\nclust_clim_gt\n\n\n\n\n\n\n\n\nSoil cluster within climate zone\n\n\nClimate zone\nSoil cluster\nValidation pts (n)\n\n\n\n\nCentral\n2\n11\n\n\nCentral\n3\n6\n\n\nCentral\n4\n1\n\n\nCentral\n5\n5\n\n\nCentral\n7\n12\n\n\nNorthwest\n5\n11\n\n\nNorthwest\n6\n10\n\n\nSoutheast\n2\n13\n\n\nSoutheast\n5\n11\n\n\n\n\n\n\n\ngtsave(clust_clim_gt, \"figs/clust_climzone_counts.png\")\n\n\n# MLRA and suborder\nval_all_clim %&gt;% \n  group_by(MLRA_NAME, taxsuborder) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"MLRA + suborder\") %&gt;% \n  cols_label(\n    MLRA_NAME = \"MLRA name\",\n    taxsuborder = \"Suborder\",\n    n_val_pts = \"Validation pts (n)\",\n    val_pt_ids = \"Validation pt IDs\"\n  )\n\n\n\n\n\n\n\n\nMLRA + suborder\n\n\nMLRA name\nSuborder\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nCentral Iowa and Minnesota Till Prairies\nAquolls\n6\nSW1-CV-B, SW1-SH-B, SW2-CV-B, SW3-CV-B, SW3-SH-B, SW3-UD-B\n\n\nCentral Iowa and Minnesota Till Prairies\nUdolls\n9\nSW1-CV-A, SW1-SH-A, SW1-UD-A, SW2-CV-A, SW2-SH-A, SW2-SH-B, SW2-UD-A, SW3-CV-A, SW3-SH-A\n\n\nCentral Minnesota Sandy Outwash\nAquolls\n1\nST2-CV-B\n\n\nCentral Minnesota Sandy Outwash\nUdolls\n4\nST2-CV-A, ST2-SH-A, ST2-SH-B, ST2-UD-A\n\n\nEastern Iowa and Minnesota Till Prairies\nAqualfs\n5\nMW1-SH-A, MW1-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A\n\n\nEastern Iowa and Minnesota Till Prairies\nAquolls\n11\nMW1-CV-B, MW1-SH-B, MW1-UD-B, MW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B\n\n\nEastern Iowa and Minnesota Till Prairies\nUdalfs\n8\nMW1-CV-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A\n\n\nNorthern Minnesota Gray Drift\nUdalfs\n5\nST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nRed River Valley of the North\nAquerts\n11\nRR1-CV-B, RR1-SH-B, RR1-UD-B, RR3-CV-A, RR3-SH-B, RR4-CV-A, RR4-SH-A, RR4-SH-B, RR4-UD-A, RR5-SH-B, RR5-UD-B\n\n\nRed River Valley of the North\nAquolls\n10\nRR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nRolling Till Prairie\nAquolls\n2\nST3-CV-B, ST3-SH-B\n\n\nRolling Till Prairie\nUdalfs\n8\nST1-CV-A, ST1-CV-B, ST1-SH-A, ST1-SH-B, ST1-UD-A, ST3-CV-A, ST3-SH-A, ST3-UD-A\n\n\n\n\n\n\n\nmlra_suborder_gt &lt;- val_all_clim %&gt;% \n  group_by(MLRA_NAME, taxsuborder) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"MLRA + suborder\") %&gt;% \n  cols_label(\n    MLRA_NAME = \"MLRA name\",\n    taxsuborder = \"Suborder\",\n    n_val_pts = \"Validation pts (n)\"\n  ) %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\nmlra_suborder_gt\n\n\n\n\n\n\n\n\nMLRA + suborder\n\n\nMLRA name\nSuborder\nValidation pts (n)\n\n\n\n\nCentral Iowa and Minnesota Till Prairies\nAquolls\n6\n\n\nCentral Iowa and Minnesota Till Prairies\nUdolls\n9\n\n\nCentral Minnesota Sandy Outwash\nAquolls\n1\n\n\nCentral Minnesota Sandy Outwash\nUdolls\n4\n\n\nEastern Iowa and Minnesota Till Prairies\nAqualfs\n5\n\n\nEastern Iowa and Minnesota Till Prairies\nAquolls\n11\n\n\nEastern Iowa and Minnesota Till Prairies\nUdalfs\n8\n\n\nNorthern Minnesota Gray Drift\nUdalfs\n5\n\n\nRed River Valley of the North\nAquerts\n11\n\n\nRed River Valley of the North\nAquolls\n10\n\n\nRolling Till Prairie\nAquolls\n2\n\n\nRolling Till Prairie\nUdalfs\n8\n\n\n\n\n\n\n\ngtsave(mlra_suborder_gt, \"figs/mlra_suborder_counts.png\")\n\n\n# suborder + family texture\nval_all_clim %&gt;%\n  separate_wider_delim(\n    cols = taxclname,\n    names = c(\n      \"particle_size_class\",\n      \"mineralogy_class\",\n      \"cec_activity_class\",\n      \"subgroup\",\n      \"other\"\n    ),\n    delim = ',',\n    too_few = \"align_start\"\n  ) %&gt;%\n  group_by(taxsuborder, particle_size_class) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Suborder + particle size class\") %&gt;% \n  cols_label(\n    particle_size_class = \"Particle size class\",\n    taxsuborder = \"Suborder\",\n    n_val_pts = \"Validation pts (n)\",\n    val_pt_ids = \"Validation pt IDs\"\n  )\n\n\n\n\n\n\n\n\nSuborder + particle size class\n\n\nSuborder\nParticle size class\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nAqualfs\nFine-loamy\n5\nMW1-SH-A, MW1-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A\n\n\nAquerts\nFine\n8\nRR1-CV-B, RR1-SH-B, RR1-UD-B, RR3-CV-A, RR3-SH-B, RR4-SH-B, RR5-SH-B, RR5-UD-B\n\n\nAquerts\nVery-fine\n3\nRR4-CV-A, RR4-SH-A, RR4-UD-A\n\n\nAquolls\nFine-loamy\n17\nMW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B, ST2-CV-B, ST3-CV-B, ST3-SH-B, SW1-CV-B, SW1-SH-B, SW2-CV-B, SW3-CV-B, SW3-SH-B, SW3-UD-B\n\n\nAquolls\nFine-silty\n13\nMW1-CV-B, MW1-SH-B, MW1-UD-B, RR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nUdalfs\nFine-loamy\n21\nMW1-CV-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A, ST1-CV-A, ST1-CV-B, ST1-SH-A, ST1-SH-B, ST1-UD-A, ST3-CV-A, ST3-SH-A, ST3-UD-A, ST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nUdolls\nFine-loamy\n13\nST2-CV-A, ST2-SH-A, ST2-SH-B, ST2-UD-A, SW1-CV-A, SW1-SH-A, SW1-UD-A, SW2-CV-A, SW2-SH-A, SW2-SH-B, SW2-UD-A, SW3-CV-A, SW3-SH-A\n\n\n\n\n\n\n\nsuborder_texture_gt &lt;- val_all_clim %&gt;%\n  separate_wider_delim(\n    cols = taxclname,\n    names = c(\n      \"particle_size_class\",\n      \"mineralogy_class\",\n      \"cec_activity_class\",\n      \"subgroup\",\n      \"other\"\n    ),\n    delim = ',',\n    too_few = \"align_start\"\n  ) %&gt;%\n  group_by(taxsuborder, particle_size_class) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Suborder + particle size class\") %&gt;% \n  cols_label(\n    particle_size_class = \"Particle size class\",\n    taxsuborder = \"Suborder\",\n    n_val_pts = \"Validation pts (n)\"\n  ) %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\nsuborder_texture_gt\n\n\n\n\n\n\n\n\nSuborder + particle size class\n\n\nSuborder\nParticle size class\nValidation pts (n)\n\n\n\n\nAqualfs\nFine-loamy\n5\n\n\nAquerts\nFine\n8\n\n\nAquerts\nVery-fine\n3\n\n\nAquolls\nFine-loamy\n17\n\n\nAquolls\nFine-silty\n13\n\n\nUdalfs\nFine-loamy\n21\n\n\nUdolls\nFine-loamy\n13\n\n\n\n\n\n\n\ngtsave(suborder_texture_gt, \"figs/suborder_texture_counts.png\")\n\n\n# suborder + climate\nval_all_clim %&gt;% \n  group_by(taxsuborder, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  select(clim_k_3, taxsuborder, n_val_pts, val_pt_ids) %&gt;% \n  arrange(clim_k_3) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Suborder within climate zone\") %&gt;% \n  cols_label(\n    clim_k_3 = \"Climate zone\",\n    taxsuborder = \"Suborder\",\n    n_val_pts = \"Validation pts (n)\",\n    val_pt_ids = \"Validation pt IDs\"\n  )\n\n\n\n\n\n\n\n\nSuborder within climate zone\n\n\nClimate zone\nSuborder\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nCentral\nAquolls\n9\nST2-CV-B, ST3-CV-B, ST3-SH-B, SW1-CV-B, SW1-SH-B, SW2-CV-B, SW3-CV-B, SW3-SH-B, SW3-UD-B\n\n\nCentral\nUdalfs\n13\nST1-CV-A, ST1-CV-B, ST1-SH-A, ST1-SH-B, ST1-UD-A, ST3-CV-A, ST3-SH-A, ST3-UD-A, ST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nCentral\nUdolls\n13\nST2-CV-A, ST2-SH-A, ST2-SH-B, ST2-UD-A, SW1-CV-A, SW1-SH-A, SW1-UD-A, SW2-CV-A, SW2-SH-A, SW2-SH-B, SW2-UD-A, SW3-CV-A, SW3-SH-A\n\n\nNorthwest\nAquerts\n11\nRR1-CV-B, RR1-SH-B, RR1-UD-B, RR3-CV-A, RR3-SH-B, RR4-CV-A, RR4-SH-A, RR4-SH-B, RR4-UD-A, RR5-SH-B, RR5-UD-B\n\n\nNorthwest\nAquolls\n10\nRR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nSoutheast\nAqualfs\n5\nMW1-SH-A, MW1-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A\n\n\nSoutheast\nAquolls\n11\nMW1-CV-B, MW1-SH-B, MW1-UD-B, MW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B\n\n\nSoutheast\nUdalfs\n8\nMW1-CV-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A\n\n\n\n\n\n\n\nsuborder_climate_gt &lt;- val_all_clim %&gt;% \n  group_by(taxsuborder, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  select(clim_k_3, taxsuborder, n_val_pts) %&gt;% \n  arrange(clim_k_3) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Suborder within climate zone\") %&gt;% \n  cols_label(\n    clim_k_3 = \"Climate zone\",\n    taxsuborder = \"Suborder\",\n    n_val_pts = \"Validation pts (n)\"\n  ) %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\nsuborder_climate_gt\n\n\n\n\n\n\n\n\nSuborder within climate zone\n\n\nClimate zone\nSuborder\nValidation pts (n)\n\n\n\n\nCentral\nAquolls\n9\n\n\nCentral\nUdalfs\n13\n\n\nCentral\nUdolls\n13\n\n\nNorthwest\nAquerts\n11\n\n\nNorthwest\nAquolls\n10\n\n\nSoutheast\nAqualfs\n5\n\n\nSoutheast\nAquolls\n11\n\n\nSoutheast\nUdalfs\n8\n\n\n\n\n\n\n\ngtsave(suborder_climate_gt, \"figs/suborder_climate_counts.png\")\n\n\n# great group and climate\nval_all_clim %&gt;% \n  group_by(taxgrtgroup, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  select(clim_k_3, taxgrtgroup, n_val_pts, val_pt_ids) %&gt;% \n  arrange(clim_k_3) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Great Group within climate zone\") %&gt;% \n  cols_label(\n    clim_k_3 = \"Climate Zone\",\n    taxgrtgroup = \"Great Group\",\n    n_val_pts = \"Validation pts (n)\",\n    val_pt_ids = \"Validation pt IDs\"\n  )\n\n\n\n\n\n\n\n\nGreat Group within climate zone\n\n\nClimate Zone\nGreat Group\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nCentral\nArgiaquolls\n3\nST2-CV-B, ST3-CV-B, ST3-SH-B\n\n\nCentral\nEndoaquolls\n6\nSW1-CV-B, SW1-SH-B, SW2-CV-B, SW3-CV-B, SW3-SH-B, SW3-UD-B\n\n\nCentral\nHapludalfs\n13\nST1-CV-A, ST1-CV-B, ST1-SH-A, ST1-SH-B, ST1-UD-A, ST3-CV-A, ST3-SH-A, ST3-UD-A, ST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nCentral\nHapludolls\n13\nST2-CV-A, ST2-SH-A, ST2-SH-B, ST2-UD-A, SW1-CV-A, SW1-SH-A, SW1-UD-A, SW2-CV-A, SW2-SH-A, SW2-SH-B, SW2-UD-A, SW3-CV-A, SW3-SH-A\n\n\nNorthwest\nCalciaquerts\n5\nRR1-SH-B, RR3-CV-A, RR3-SH-B, RR5-SH-B, RR5-UD-B\n\n\nNorthwest\nCalciaquolls\n10\nRR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nNorthwest\nEpiaquerts\n6\nRR1-CV-B, RR1-UD-B, RR4-CV-A, RR4-SH-A, RR4-SH-B, RR4-UD-A\n\n\nSoutheast\nEndoaquolls\n11\nMW1-CV-B, MW1-SH-B, MW1-UD-B, MW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B\n\n\nSoutheast\nEpiaqualfs\n5\nMW1-SH-A, MW1-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A\n\n\nSoutheast\nHapludalfs\n8\nMW1-CV-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A\n\n\n\n\n\n\n\ngrtgrp_clim_gt &lt;- val_all_clim %&gt;% \n  group_by(taxgrtgroup, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  select(clim_k_3, taxgrtgroup, n_val_pts) %&gt;% \n  arrange(clim_k_3) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Great Group within climate zone\") %&gt;% \n  cols_label(\n    clim_k_3 = \"Climate zone\",\n    taxgrtgroup = \"Great Group\",\n    n_val_pts = \"Validation pts (n)\"\n  ) %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\ngrtgrp_clim_gt\n\n\n\n\n\n\n\n\nGreat Group within climate zone\n\n\nClimate zone\nGreat Group\nValidation pts (n)\n\n\n\n\nCentral\nArgiaquolls\n3\n\n\nCentral\nEndoaquolls\n6\n\n\nCentral\nHapludalfs\n13\n\n\nCentral\nHapludolls\n13\n\n\nNorthwest\nCalciaquerts\n5\n\n\nNorthwest\nCalciaquolls\n10\n\n\nNorthwest\nEpiaquerts\n6\n\n\nSoutheast\nEndoaquolls\n11\n\n\nSoutheast\nEpiaqualfs\n5\n\n\nSoutheast\nHapludalfs\n8\n\n\n\n\n\n\n\ngtsave(grtgrp_clim_gt, \"figs/grtgrp_clim_counts.png\")\n\n\n# MLRA and soil cluster\nval_all_clim %&gt;% \n  group_by(MLRA_NAME, k_8) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Soil cluster within MLRA\") %&gt;% \n  cols_label(\n    MLRA_NAME = \"MLRA name\",\n    k_8 = \"Soil cluster\",\n    n_val_pts = \"Validation pts (n)\",\n    val_pt_ids = \"Validation pt IDs\"\n  )\n\n\n\n\n\n\n\n\nSoil cluster within MLRA\n\n\nMLRA name\nSoil cluster\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nCentral Iowa and Minnesota Till Prairies\n2\n5\nSW1-SH-A, SW1-UD-A, SW2-SH-B, SW2-UD-A, SW3-SH-A\n\n\nCentral Iowa and Minnesota Till Prairies\n4\n1\nSW3-SH-B\n\n\nCentral Iowa and Minnesota Till Prairies\n5\n5\nSW1-CV-B, SW1-SH-B, SW2-CV-B, SW3-CV-B, SW3-UD-B\n\n\nCentral Iowa and Minnesota Till Prairies\n7\n4\nSW1-CV-A, SW2-CV-A, SW2-SH-A, SW3-CV-A\n\n\nCentral Minnesota Sandy Outwash\n2\n2\nST2-CV-B, ST2-SH-B\n\n\nCentral Minnesota Sandy Outwash\n7\n3\nST2-CV-A, ST2-SH-A, ST2-UD-A\n\n\nEastern Iowa and Minnesota Till Prairies\n2\n13\nMW1-CV-A, MW1-SH-A, MW1-UD-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A\n\n\nEastern Iowa and Minnesota Till Prairies\n5\n11\nMW1-CV-B, MW1-SH-B, MW1-UD-B, MW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B\n\n\nNorthern Minnesota Gray Drift\n3\n5\nST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nRed River Valley of the North\n5\n11\nRR1-CV-B, RR1-SH-B, RR1-UD-B, RR3-CV-A, RR3-SH-B, RR4-CV-A, RR4-SH-A, RR4-SH-B, RR4-UD-A, RR5-SH-B, RR5-UD-B\n\n\nRed River Valley of the North\n6\n10\nRR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nRolling Till Prairie\n2\n4\nST1-CV-A, ST1-CV-B, ST3-CV-B, ST3-SH-B\n\n\nRolling Till Prairie\n3\n1\nST1-UD-A\n\n\nRolling Till Prairie\n7\n5\nST1-SH-A, ST1-SH-B, ST3-CV-A, ST3-SH-A, ST3-UD-A\n\n\n\n\n\n\n\nmlra_cluster_gt &lt;- val_all_clim %&gt;% \n  group_by(MLRA_NAME, k_8) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Soil cluster within MLRA\") %&gt;% \n  cols_label(\n    MLRA_NAME = \"MLRA name\",\n    k_8 = \"Soil cluster\",\n    n_val_pts = \"Validation pts (n)\"\n  ) %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\nmlra_cluster_gt\n\n\n\n\n\n\n\n\nSoil cluster within MLRA\n\n\nMLRA name\nSoil cluster\nValidation pts (n)\n\n\n\n\nCentral Iowa and Minnesota Till Prairies\n2\n5\n\n\nCentral Iowa and Minnesota Till Prairies\n4\n1\n\n\nCentral Iowa and Minnesota Till Prairies\n5\n5\n\n\nCentral Iowa and Minnesota Till Prairies\n7\n4\n\n\nCentral Minnesota Sandy Outwash\n2\n2\n\n\nCentral Minnesota Sandy Outwash\n7\n3\n\n\nEastern Iowa and Minnesota Till Prairies\n2\n13\n\n\nEastern Iowa and Minnesota Till Prairies\n5\n11\n\n\nNorthern Minnesota Gray Drift\n3\n5\n\n\nRed River Valley of the North\n5\n11\n\n\nRed River Valley of the North\n6\n10\n\n\nRolling Till Prairie\n2\n4\n\n\nRolling Till Prairie\n3\n1\n\n\nRolling Till Prairie\n7\n5\n\n\n\n\n\n\n\ngtsave(mlra_cluster_gt, \"figs/mlra_cluster_counts.png\")\n\n\n# order + climate \n\nval_all_clim %&gt;% \n  group_by(taxorder, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  select(clim_k_3, taxorder, n_val_pts, val_pt_ids) %&gt;% \n  arrange(clim_k_3) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Order within climate zone\") %&gt;% \n  cols_label(\n    clim_k_3 = \"Climate zone\",\n    taxorder = \"Order\",\n    n_val_pts = \"Validation pts (n)\",\n    val_pt_ids = \"Validation pt IDs\"\n  )\n\n\n\n\n\n\n\n\nOrder within climate zone\n\n\nClimate zone\nOrder\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nCentral\nAlfisols\n13\nST1-CV-A, ST1-CV-B, ST1-SH-A, ST1-SH-B, ST1-UD-A, ST3-CV-A, ST3-SH-A, ST3-UD-A, ST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nCentral\nMollisols\n22\nST2-CV-A, ST2-CV-B, ST2-SH-A, ST2-SH-B, ST2-UD-A, ST3-CV-B, ST3-SH-B, SW1-CV-A, SW1-CV-B, SW1-SH-A, SW1-SH-B, SW1-UD-A, SW2-CV-A, SW2-CV-B, SW2-SH-A, SW2-SH-B, SW2-UD-A, SW3-CV-A, SW3-CV-B, SW3-SH-A, SW3-SH-B, SW3-UD-B\n\n\nNorthwest\nMollisols\n10\nRR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nNorthwest\nVertisols\n11\nRR1-CV-B, RR1-SH-B, RR1-UD-B, RR3-CV-A, RR3-SH-B, RR4-CV-A, RR4-SH-A, RR4-SH-B, RR4-UD-A, RR5-SH-B, RR5-UD-B\n\n\nSoutheast\nAlfisols\n13\nMW1-CV-A, MW1-SH-A, MW1-UD-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A\n\n\nSoutheast\nMollisols\n11\nMW1-CV-B, MW1-SH-B, MW1-UD-B, MW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B\n\n\n\n\n\n\n\norder_climate_gt &lt;- val_all_clim %&gt;% \n  group_by(taxorder, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  select(clim_k_3, taxorder, n_val_pts) %&gt;% \n  arrange(clim_k_3) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Order within climate zone\") %&gt;% \n  cols_label(\n    clim_k_3 = \"Climate zone\",\n    taxorder = \"Order\",\n    n_val_pts = \"Validation pts (n)\"\n  ) %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\n\n\norder_climate_gt\n\n\n\n\n\n\n\n\nOrder within climate zone\n\n\nClimate zone\nOrder\nValidation pts (n)\n\n\n\n\nCentral\nAlfisols\n13\n\n\nCentral\nMollisols\n22\n\n\nNorthwest\nMollisols\n10\n\n\nNorthwest\nVertisols\n11\n\n\nSoutheast\nAlfisols\n13\n\n\nSoutheast\nMollisols\n11\n\n\n\n\n\n\n\n\n\n# order + particle size\n\nval_all_clim %&gt;%\n  separate_wider_delim(\n    cols = taxclname,\n    names = c(\n      \"particle_size_class\",\n      \"mineralogy_class\",\n      \"cec_activity_class\",\n      \"subgroup\",\n      \"other\"\n    ),\n    delim = ',',\n    too_few = \"align_start\"\n  ) %&gt;%\n  group_by(taxorder, particle_size_class) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Order + particle size class\") %&gt;% \n  cols_label(\n    particle_size_class = \"Particle size class\",\n    taxorder = \"Order\",\n    n_val_pts = \"Validation pts (n)\",\n    val_pt_ids = \"Validation pt IDs\"\n  )\n\n\n\n\n\n\n\n\nOrder + particle size class\n\n\nOrder\nParticle size class\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nAlfisols\nFine-loamy\n26\nMW1-CV-A, MW1-SH-A, MW1-UD-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A, ST1-CV-A, ST1-CV-B, ST1-SH-A, ST1-SH-B, ST1-UD-A, ST3-CV-A, ST3-SH-A, ST3-UD-A, ST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nMollisols\nFine-loamy\n30\nMW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B, ST2-CV-A, ST2-CV-B, ST2-SH-A, ST2-SH-B, ST2-UD-A, ST3-CV-B, ST3-SH-B, SW1-CV-A, SW1-CV-B, SW1-SH-A, SW1-SH-B, SW1-UD-A, SW2-CV-A, SW2-CV-B, SW2-SH-A, SW2-SH-B, SW2-UD-A, SW3-CV-A, SW3-CV-B, SW3-SH-A, SW3-SH-B, SW3-UD-B\n\n\nMollisols\nFine-silty\n13\nMW1-CV-B, MW1-SH-B, MW1-UD-B, RR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nVertisols\nFine\n8\nRR1-CV-B, RR1-SH-B, RR1-UD-B, RR3-CV-A, RR3-SH-B, RR4-SH-B, RR5-SH-B, RR5-UD-B\n\n\nVertisols\nVery-fine\n3\nRR4-CV-A, RR4-SH-A, RR4-UD-A\n\n\n\n\n\n\n\norder_texture_gt &lt;- val_all_clim %&gt;%\n  separate_wider_delim(\n    cols = taxclname,\n    names = c(\n      \"particle_size_class\",\n      \"mineralogy_class\",\n      \"cec_activity_class\",\n      \"subgroup\",\n      \"other\"\n    ),\n    delim = ',',\n    too_few = \"align_start\"\n  ) %&gt;%\n  group_by(taxorder, particle_size_class) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Order + particle size class\") %&gt;% \n  cols_label(\n    particle_size_class = \"Particle size class\",\n    taxorder = \"Order\",\n    n_val_pts = \"Validation pts (n)\"\n  ) %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\norder_texture_gt\n\n\n\n\n\n\n\n\nOrder + particle size class\n\n\nOrder\nParticle size class\nValidation pts (n)\n\n\n\n\nAlfisols\nFine-loamy\n26\n\n\nMollisols\nFine-loamy\n30\n\n\nMollisols\nFine-silty\n13\n\n\nVertisols\nFine\n8\n\n\nVertisols\nVery-fine\n3\n\n\n\n\n\n\n\n\n\n# order + particle size? + climate? \n\nval_all_clim %&gt;%\n  separate_wider_delim(\n    cols = taxclname,\n    names = c(\n      \"particle_size_class\",\n      \"mineralogy_class\",\n      \"cec_activity_class\",\n      \"subgroup\",\n      \"other\"\n    ),\n    delim = ',',\n    too_few = \"align_start\"\n  ) %&gt;%\n  group_by(taxorder, particle_size_class, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            val_pt_ids = str_c(val_unit_id, collapse = \", \"),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Order + particle size class + climate\") %&gt;% \n  cols_label(\n    particle_size_class = \"Particle size class\",\n    taxorder = \"Order\",\n    clim_k_3 = \"Climate zone\",\n    n_val_pts = \"Validation pts (n)\",\n    val_pt_ids = \"Validation pt IDs\"\n  )\n\n\n\n\n\n\n\n\nOrder + particle size class + climate\n\n\nOrder\nParticle size class\nClimate zone\nValidation pts (n)\nValidation pt IDs\n\n\n\n\nAlfisols\nFine-loamy\nCentral\n13\nST1-CV-A, ST1-CV-B, ST1-SH-A, ST1-SH-B, ST1-UD-A, ST3-CV-A, ST3-SH-A, ST3-UD-A, ST4-CV-A, ST4-CV-B, ST4-SH-A, ST4-SH-B, ST4-UD-A\n\n\nAlfisols\nFine-loamy\nSoutheast\n13\nMW1-CV-A, MW1-SH-A, MW1-UD-A, MW2-CV-A, MW2-SH-A, MW2-UD-A, MW3-CV-A, MW3-SH-A, MW3-UD-A, MW4-CV-A, MW4-CV-B, MW4-SH-A, MW4-UD-A\n\n\nMollisols\nFine-loamy\nCentral\n22\nST2-CV-A, ST2-CV-B, ST2-SH-A, ST2-SH-B, ST2-UD-A, ST3-CV-B, ST3-SH-B, SW1-CV-A, SW1-CV-B, SW1-SH-A, SW1-SH-B, SW1-UD-A, SW2-CV-A, SW2-CV-B, SW2-SH-A, SW2-SH-B, SW2-UD-A, SW3-CV-A, SW3-CV-B, SW3-SH-A, SW3-SH-B, SW3-UD-B\n\n\nMollisols\nFine-loamy\nSoutheast\n8\nMW2-CV-B, MW2-SH-B, MW2-UD-B, MW3-CV-B, MW3-SH-B, MW3-UD-B, MW4-SH-B, MW4-UD-B\n\n\nMollisols\nFine-silty\nNorthwest\n10\nRR2-CV-A, RR2-CV-B, RR2-SH-A, RR2-SH-B, RR2-UD-A, RR3-CV-B, RR3-SH-A, RR3-UD-A, RR5-CV-B, RR6-SH-A\n\n\nMollisols\nFine-silty\nSoutheast\n3\nMW1-CV-B, MW1-SH-B, MW1-UD-B\n\n\nVertisols\nFine\nNorthwest\n8\nRR1-CV-B, RR1-SH-B, RR1-UD-B, RR3-CV-A, RR3-SH-B, RR4-SH-B, RR5-SH-B, RR5-UD-B\n\n\nVertisols\nVery-fine\nNorthwest\n3\nRR4-CV-A, RR4-SH-A, RR4-UD-A\n\n\n\n\n\n\n\norder_texture_clim_gt &lt;- val_all_clim %&gt;%\n  separate_wider_delim(\n    cols = taxclname,\n    names = c(\n      \"particle_size_class\",\n      \"mineralogy_class\",\n      \"cec_activity_class\",\n      \"subgroup\",\n      \"other\"\n    ),\n    delim = ',',\n    too_few = \"align_start\"\n  ) %&gt;%\n  group_by(taxorder, particle_size_class, clim_k_3) %&gt;% \n  summarise(n_val_pts = n(),\n            .groups = \"drop\") %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Order + particle size class + climate\") %&gt;% \n  cols_label(\n    clim_k_3 = \"Climate Zone\",\n    particle_size_class = \"Particle size class\",\n    taxorder = \"Order\",\n    n_val_pts = \"Validation pts (n)\"\n  ) %&gt;% \n  opt_stylize(color = \"blue\", add_row_striping = TRUE, style = 2)\n\norder_texture_clim_gt\n\n\n\n\n\n\n\n\nOrder + particle size class + climate\n\n\nOrder\nParticle size class\nClimate Zone\nValidation pts (n)\n\n\n\n\nAlfisols\nFine-loamy\nCentral\n13\n\n\nAlfisols\nFine-loamy\nSoutheast\n13\n\n\nMollisols\nFine-loamy\nCentral\n22\n\n\nMollisols\nFine-loamy\nSoutheast\n8\n\n\nMollisols\nFine-silty\nNorthwest\n10\n\n\nMollisols\nFine-silty\nSoutheast\n3\n\n\nVertisols\nFine\nNorthwest\n8\n\n\nVertisols\nVery-fine\nNorthwest\n3",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#create-additional-group-variables",
    "href": "36-update-welch-compare-variation.html#create-additional-group-variables",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.6 Create additional group variables",
    "text": "20.6 Create additional group variables\nAbove we explored how many validation points would be present in each class when we start grouping by multiple variables (suborder + MLRA or suborder + particle size class). I need to add these new grouping variables as columns in my dataset before I can run one-way ANOVAs below.\nOn 3/29/24 Nic and I talked again about which additional grouping variables we should included in the edited version of the manuscript. Decided we would do all the reasonable 2- and 3-way combinations of grouping variables listed below. By “reasonable” we mean that we aren’t going to do things that are circular / redundant like MLRA + Climate or MLRA + particle size. Here is the list of single variables:\n\nOrder\nSuborder\nClimate zone\nMLRA\nParticle size class (from family)\nKM cluster\n\n\nvar_grps &lt;- c(\"order\", \"suborder\", \"climate zone\", \"mlra\", \"partsize\")\n\n\ncombos_2way &lt;- expand_grid(first = var_grps, second = var_grps) %&gt;%\n  filter(\n    # drop b/c circular\n    !(first == second),!(first == \"climate zone\" & second == \"mlra\"),\n    !(first == \"mlra\" & second == \"climate zone\"),\n    !(first == \"order\" & second == \"suborder\"),\n    !(first == \"suborder\" & second == \"order\"),\n   # !(first == \"mlra\" & second == \"partsize\"),\n    !(first == \"partsize\" & second == \"mlra\"),\n# drop b/c duplicated\n    !(first == \"climate zone\" & second == \"order\"),\n    !(first == \"mlra\" & second == \"order\"),\n    !(first == \"partsize\" & second == \"order\"),\n    !(first == \"climate zone\" & second == \"suborder\"),\n    !(first == \"mlra\" & second == \"suborder\"),\n    !(first == \"partsize\" & second == \"climate zone\"),\n!(first == \"partsize\" & second == \"suborder\")\n  ) %&gt;%\n    mutate(two_way = glue(\"{first}_{second}\"))\n  \n  combos_3way &lt;-\n    expand_grid(first = combos_2way$first, second = combos_2way$two_way) %&gt;%\n    filter(# drop 3-way combos where one variable is repeated\n      !(str_detect(second, first)),\n      !(first == \"suborder\" & str_detect(second, \"order\"))) %&gt;%\n    distinct() %&gt;%\n    filter(!(first == \"climate zone\" & str_detect(second, \"mlra\")),\n           first != \"climate zone\") %&gt;%\n    mutate(combo_3way = glue(\"{first}_{second}\"))\n  \nc(combos_2way$two_way, combos_3way$combo_3way) %&gt;% \n  data.frame(to_test = .) %&gt;% \n  add_row(to_test = \"order\") %&gt;% \n  add_row(to_test = \"particle size class\")\n\n\n  \n\n\n\n”\n\nval_all_grps &lt;- val_all_clim %&gt;% \n  separate_wider_delim(\n    cols = taxclname,\n    names = c(\n      \"particle_size_class\",\n      \"mineralogy_class\",\n      \"cec_activity_class\",\n      \"subgroup\",\n      \"other\"\n    ),\n    delim = ',',\n    too_few = \"align_start\"\n  ) %&gt;% \n  mutate(\n    clim_k_3 = str_replace(clim_k_3, \"Cluster_\", \"\"),\n    km_soil_clim = glue(\"soil{k_8}_clim{clim_k_3}\"),\n    suborder_clim = glue(\"{taxsuborder}_clim{clim_k_3}\"),\n    suborder_partsize = glue(\"{taxsuborder}_{particle_size_class}\"),\n    suborder_mlra = glue(\"{taxsuborder}_{mlra_short}\"),\n    order_partsize = glue(\"{taxorder}_{particle_size_class}\"),\n    clim_order_partsize = glue(\"clim{clim_k_3}_{taxorder}_{particle_size_class}\"),\n    mlra_km = glue(\"{mlra_short}_soil{k_8}\"),\n    mlra_partsize = glue(\"{mlra_short}_{particle_size_class}\"),\n    partsize = particle_size_class,\n    clim_order = glue(\"{clim_k_3}_{taxorder}\"),\n    mlra_order = glue(\"{mlra_short}_{taxorder}\"),\n    order_partsize = glue(\"{taxorder}_{particle_size_class}\"),\n    clim_partsize = glue(\"{clim_k_3}_{particle_size_class}\"),\n    clim_suborder_partsize = glue(\"{clim_k_3}_{taxsuborder}_{particle_size_class}\")\n  )\n\nwrite_csv(val_all_grps, \"data/validation_data_all_grouping_factors.csv\")\n\n\nall_combos_to_test &lt;- val_all_grps %&gt;% \n  select(\n         mlra_short,\n         taxorder,\n         taxsuborder,\n         partsize,\n         soil_group, \n         mlra_short,\n         km_soil_clim:clim_suborder_partsize\n         )\n\ngrouping_factors &lt;- colnames(all_combos_to_test)\n\n\n# https://stackoverflow.com/questions/63947104/how-do-i-unquote-a-character-column-name-for-fableaggregate-key\ngroup_counts &lt;-\n  map(.x = grouping_factors, ~ count(x = all_combos_to_test,!!rlang::sym(.x)))\n\ngrp_counts_named &lt;- set_names(group_counts, nm = grouping_factors)\n  \nwrite.xlsx(grp_counts_named, \"data/counts_for_all_groupings.xlsx\")",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#crosstabs-w-ud",
    "href": "36-update-welch-compare-variation.html#crosstabs-w-ud",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.7 Crosstabs w/ UD",
    "text": "20.7 Crosstabs w/ UD\nWas originally thinking about whether we should include or exclude UD (undisturbed/unfarmed sites) because we know they are a major source of variance (they have much higher values across the board for all these indicators).\n\nxtabs(~soil_group + taxsuborder, data = val_all_grps)\n\n          taxsuborder\nsoil_group Aqualfs Aquerts Aquolls Udalfs Udolls\n     grp-2       5       0       3     10      6\n     grp-3       0       0       0      6      0\n     grp-4       0       0       1      0      0\n     grp-5       0      11      16      0      0\n     grp-6       0       0      10      0      0\n     grp-7       0       0       0      5      7\n\nxtabs(~taxgrtgroup + soil_group, data = val_all_grps)\n\n              soil_group\ntaxgrtgroup    grp-2 grp-3 grp-4 grp-5 grp-6 grp-7\n  Argiaquolls      3     0     0     0     0     0\n  Calciaquerts     0     0     0     5     0     0\n  Calciaquolls     0     0     0     0    10     0\n  Endoaquolls      0     0     1    16     0     0\n  Epiaqualfs       5     0     0     0     0     0\n  Epiaquerts       0     0     0     6     0     0\n  Hapludalfs      10     6     0     0     0     5\n  Hapludolls       6     0     0     0     0     7\n\nxtabs(~mlra_short + soil_group, data = val_all_grps)\n\n              soil_group\nmlra_short     grp-2 grp-3 grp-4 grp-5 grp-6 grp-7\n  C IA MN Till     5     0     1     5     0     4\n  C MN Sandy       2     0     0     0     0     3\n  E IA MN Till    13     0     0    11     0     0\n  N MN Gray        0     5     0     0     0     0\n  Red River        0     0     0    11    10     0\n  Rol Till Pr      4     1     0     0     0     5\n\nxtabs(~mlra_short + particle_size_class, data = val_all_grps)\n\n              particle_size_class\nmlra_short     Fine Fine-loamy Fine-silty Very-fine\n  C IA MN Till    0         15          0         0\n  C MN Sandy      0          5          0         0\n  E IA MN Till    0         21          3         0\n  N MN Gray       0          5          0         0\n  Red River       8          0         10         3\n  Rol Till Pr     0         10          0         0\n\n# xtabs(~suborder_mlra + mlra_km, data = val_all_grps, drop.unused.levels = TRUE, sparse = TRUE) \n# \n# xtabs(~km_soil_clim + order_partsize_clim, data = val_all_grps, sparse = TRUE, drop.unused.levels = TRUE)\n\n\nval_all_grps %&gt;% \n  count(km_soil_clim, order_partsize) %&gt;% \n  pivot_wider(names_from = \"order_partsize\",\n              values_from = n) %&gt;% \n  mutate(across(.cols = where(is.numeric), .fns = ~ifelse(is.na(.x), 0, .x)))\n\n\n  \n\n\nval_all_grps %&gt;% \n  count(km_soil_clim, clim_order_partsize) %&gt;% \n  mutate(across(.cols = c(\"km_soil_clim\", \"clim_order_partsize\"), \n                .fns = ~str_remove(.x, pattern = \"clim\"))) %&gt;% \n  pivot_wider(names_from = \"km_soil_clim\",\n              values_from = n) %&gt;% \n  mutate(across(.cols = where(is.numeric), .fns = ~ifelse(is.na(.x), 0, .x))) %&gt;% \n  gt() %&gt;% \n  data_color(\n    columns = contains(\"soil\"),\n    method = \"numeric\",\n    palette = \"viridis\",\n    domain = c(1, 15),\n    na_color = \"white\"\n  )\n\nWarning: Some values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\n\n\n\n\n\n\n\n\n\nclim_order_partsize\nsoil2_Central\nsoil2_Southeast\nsoil3_Central\nsoil4_Central\nsoil5_Central\nsoil5_Northwest\nsoil5_Southeast\nsoil6_Northwest\nsoil7_Central\n\n\n\n\nCentral_Alfisols_Fine-loamy\n2\n0\n6\n0\n0\n0\n0\n0\n5\n\n\nCentral_Mollisols_Fine-loamy\n9\n0\n0\n1\n5\n0\n0\n0\n7\n\n\nSoutheast_Alfisols_Fine-loamy\n0\n13\n0\n0\n0\n0\n0\n0\n0\n\n\nNorthwest_Vertisols_Fine\n0\n0\n0\n0\n0\n8\n0\n0\n0\n\n\nNorthwest_Vertisols_Very-fine\n0\n0\n0\n0\n0\n3\n0\n0\n0\n\n\nSoutheast_Mollisols_Fine-loamy\n0\n0\n0\n0\n0\n0\n8\n0\n0\n\n\nSoutheast_Mollisols_Fine-silty\n0\n0\n0\n0\n0\n0\n3\n0\n0\n\n\nNorthwest_Mollisols_Fine-silty\n0\n0\n0\n0\n0\n0\n0\n10\n0\n\n\n\n\n\n\n\n\n\nval_all_grps %&gt;% \n  count(mlra_km, suborder_mlra) %&gt;% \n  pivot_wider(names_from = \"suborder_mlra\",\n              values_from = n) %&gt;% \n  mutate(across(.cols = where(is.numeric), .fns = ~ifelse(is.na(.x), 0, .x))) %&gt;% \n  gt() %&gt;% \n  data_color(\n    columns = -c(\"mlra_km\"),\n    method = \"numeric\",\n    palette = \"viridis\",\n    domain = c(1, 15),\n    na_color = \"white\"\n  )\n\nWarning: Some values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\nSome values were outside the color scale and will be treated as NA\n\n\n\n\n\n\n\n\n\nmlra_km\nUdolls_C IA MN Till\nAquolls_C IA MN Till\nAquolls_C MN Sandy\nUdolls_C MN Sandy\nAqualfs_E IA MN Till\nUdalfs_E IA MN Till\nAquolls_E IA MN Till\nUdalfs_N MN Gray\nAquerts_Red River\nAquolls_Red River\nAquolls_Rol Till Pr\nUdalfs_Rol Till Pr\n\n\n\n\nC IA MN Till_soil2\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nC IA MN Till_soil4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nC IA MN Till_soil5\n0\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nC IA MN Till_soil7\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nC MN Sandy_soil2\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nC MN Sandy_soil7\n0\n0\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nE IA MN Till_soil2\n0\n0\n0\n0\n5\n8\n0\n0\n0\n0\n0\n0\n\n\nE IA MN Till_soil5\n0\n0\n0\n0\n0\n0\n11\n0\n0\n0\n0\n0\n\n\nN MN Gray_soil3\n0\n0\n0\n0\n0\n0\n0\n5\n0\n0\n0\n0\n\n\nRed River_soil5\n0\n0\n0\n0\n0\n0\n0\n0\n11\n0\n0\n0\n\n\nRed River_soil6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n10\n0\n0\n\n\nRol Till Pr_soil2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n2\n\n\nRol Till Pr_soil3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nRol Till Pr_soil7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#crosstabs-wout-ud",
    "href": "36-update-welch-compare-variation.html#crosstabs-wout-ud",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.8 Crosstabs w/out UD",
    "text": "20.8 Crosstabs w/out UD\n\nval_farm &lt;- val_all_grps %&gt;% \n  filter(!str_detect(val_unit_id, \"UD\"))\n\nxtabs(~soil_group + region, data = val_farm)\n\n          region\nsoil_group MW RR ST SW\n     grp-2  9  0  6  3\n     grp-3  0  0  4  0\n     grp-4  0  0  0  1\n     grp-5  7  8  0  4\n     grp-6  0  8  0  0\n     grp-7  0  0  6  4\n\nxtabs(~soil_group + taxsuborder, data = val_farm)\n\n          taxsuborder\nsoil_group Aqualfs Aquerts Aquolls Udalfs Udolls\n     grp-2       3       0       3      8      4\n     grp-3       0       0       0      4      0\n     grp-4       0       0       1      0      0\n     grp-5       0       8      11      0      0\n     grp-6       0       0       8      0      0\n     grp-7       0       0       0      4      6\n\nxtabs(~taxgrtgroup + soil_group, data = val_farm)\n\n              soil_group\ntaxgrtgroup    grp-2 grp-3 grp-4 grp-5 grp-6 grp-7\n  Argiaquolls      3     0     0     0     0     0\n  Calciaquerts     0     0     0     4     0     0\n  Calciaquolls     0     0     0     0     8     0\n  Endoaquolls      0     0     1    11     0     0\n  Epiaqualfs       3     0     0     0     0     0\n  Epiaquerts       0     0     0     4     0     0\n  Hapludalfs       8     4     0     0     0     4\n  Hapludolls       4     0     0     0     0     6\n\n\n\n20.8.1 Checking distributions\nTransformations needed (based on inspecting the plots below):\n\nNone: aggregate stability, POXC, SOC\nLog10: total biomass PLFA, total bacteria PLFA, total fungi PLFA, MBC, MBN, PMC\n\n\n20.8.1.1 SOC\n\nplot_transformations(var = org_c_wt_percent,\n                     df = val_all_clim,\n                     nbins = 15)\n\n\n\n\n\n\n\n\n\n\n20.8.1.2 Aggregate stability\n\nplot_transformations(var = corr_percent_stable_gr2mm,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n\n\n\n\n20.8.1.3 PLFA Indicators\n\nplot_transformations(var = total_living_microbial_biomass_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n\nplot_transformations(var = total_bacteria_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n\nplot_transformations(var = total_fungi_plfa_ng_g,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n\n\n\n\n20.8.1.4 Microbial biomass C & N (CFE)\n\nplot_transformations(var = mbc_ug_g_soil,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n\nplot_transformations(var = mbn_ug_g_soil,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n\n\n\n\n20.8.1.5 POXC\n\nplot_transformations(var = poxc_mg_kg,\n                     df = val_all,\n                     nbins = 15)\n\n\n\n\n\n\n\n\n\n\n20.8.1.6 PMC\n\nplot_transformations(var = ugC_g_day,\n                     df = val_all,\n                     nbins = 10)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#function-to-plot-model-checks",
    "href": "36-update-welch-compare-variation.html#function-to-plot-model-checks",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.9 Function to plot model checks",
    "text": "20.9 Function to plot model checks\n\ncheck_plots_anova &lt;- function(soil_var, strat, df, log_trans) {\n  \n  dat_no_na &lt;- df %&gt;%\n    select(val_unit_id,\n           all_of(strat),\n           all_of(soil_var)) %&gt;%\n    drop_na(all_of(soil_var))\n  \n  # checking if any groups are represented by 2 or less data points. Want to d\n  # drop these so we can actually calculate variance for the group\n\n  n_obs_per_group &lt;- dat_no_na %&gt;%\n    count(.data[[strat]])\n  \n  single_obs_groups &lt;- n_obs_per_group %&gt;%\n    filter(n &lt;= 2) %&gt;%\n    pull(.data[[strat]])\n  \n  if (length(single_obs_groups) == 0) {\n    dat_subset &lt;- dat_no_na\n    \n  } else{\n    dat_subset &lt;- dat_no_na %&gt;%\n      filter(!(.data[[strat]] %in% single_obs_groups))\n    \n  }\n  \n  if (log_trans){\n    \n    f &lt;- paste0(\"log(\", soil_var, \") ~ \", strat)\n    \n  }else{\n    \n    f &lt;- paste0(soil_var, \" ~ \", strat)\n    \n  }\n  \n  mod &lt;- lm(formula = f,\n            data = dat_subset)\n  \n  norm &lt;- plot(check_normality(mod))\n  homog &lt;- plot(check_homogeneity(mod))\n\n\n  return(list(f_used = f,\n              plots = list(norm, homog)))\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#models-to-check",
    "href": "36-update-welch-compare-variation.html#models-to-check",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.10 Models to check",
    "text": "20.10 Models to check\nWorking with these independent variables (stratification options):\n\nsoil_group (k-means);\nmlra_short\ntaxsuborder\ntaxgrtgroup\nSoil + climate clusters\nSuborder + climate\nSuborder + particle size class\nSuborder + MLRA\nOrder + particle size + climate zone\nSoil cluster within MLRA\n\n\n(dep_vars &lt;- c(\"corr_percent_stable_gr2mm\",\n               \"ugC_g_day\",\n               \"poxc_mg_kg\",\n               \"mbc_ug_g_soil\",\n               \"mbn_ug_g_soil\", # include MBN?\n               \"total_living_microbial_biomass_plfa_ng_g\",\n               \"total_bacteria_plfa_ng_g\",\n               \"total_fungi_plfa_ng_g\",\n               \"org_c_wt_percent\" \n))\n\n[1] \"corr_percent_stable_gr2mm\"               \n[2] \"ugC_g_day\"                               \n[3] \"poxc_mg_kg\"                              \n[4] \"mbc_ug_g_soil\"                           \n[5] \"mbn_ug_g_soil\"                           \n[6] \"total_living_microbial_biomass_plfa_ng_g\"\n[7] \"total_bacteria_plfa_ng_g\"                \n[8] \"total_fungi_plfa_ng_g\"                   \n[9] \"org_c_wt_percent\"                        \n\n# determined by inspecting histograms above w/ different\n# transformation options\nlog_trans &lt;- c(FALSE, \n               TRUE, \n               FALSE, \n               TRUE,\n               TRUE,\n               TRUE,\n               TRUE,\n               TRUE,\n               FALSE)\n\n\n20.10.1 K-means soil groups\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"soil_group\",\n    df = val_all_grps\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ soil_group\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ soil_group\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ soil_group\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ soil_group\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ soil_group\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ soil_group\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ soil_group\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ soil_group\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ soil_group\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n20.10.2 Region\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"region\",\n    df = val_all_grps\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ region\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ region\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ region\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ region\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ region\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ region\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ region\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ region\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ region\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n20.10.3 Suborder\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"taxsuborder\",\n    df = val_all_grps\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ taxsuborder\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ taxsuborder\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ taxsuborder\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ taxsuborder\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ taxsuborder\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ taxsuborder\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ taxsuborder\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ taxsuborder\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ taxsuborder\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n20.10.4 Great group\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"taxgrtgroup\",\n    df = val_all_grps\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ taxgrtgroup\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ taxgrtgroup\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ taxgrtgroup\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ taxgrtgroup\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ taxgrtgroup\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ taxgrtgroup\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ taxgrtgroup\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ taxgrtgroup\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ taxgrtgroup\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n20.10.5 Soil + climate clusters\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"km_soil_clim\",\n    df = val_all_grps\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ km_soil_clim\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ km_soil_clim\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ km_soil_clim\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ km_soil_clim\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ km_soil_clim\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ km_soil_clim\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ km_soil_clim\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ km_soil_clim\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ km_soil_clim\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n20.10.6 Suborder + climate\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"suborder_clim\",\n    df = val_all_grps\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ suborder_clim\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ suborder_clim\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ suborder_clim\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ suborder_clim\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ suborder_clim\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ suborder_clim\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ suborder_clim\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ suborder_clim\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ suborder_clim\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n20.10.7 Suborder + particle size class\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"suborder_partsize\",\n    df = val_all_grps\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ suborder_partsize\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ suborder_partsize\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ suborder_partsize\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ suborder_partsize\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ suborder_partsize\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ suborder_partsize\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ suborder_partsize\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ suborder_partsize\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ suborder_partsize\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n20.10.8 Suborder + MLRA\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"suborder_mlra\",\n    df = val_all_grps\n  )\n)\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ suborder_mlra\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ suborder_mlra\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ suborder_mlra\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ suborder_mlra\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ suborder_mlra\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ suborder_mlra\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ suborder_mlra\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ suborder_mlra\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ suborder_mlra\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n20.10.9 Order + particle size class + climate\n\nmap2(\n  .x = dep_vars,\n  .y = log_trans,\n  .f = ~ check_plots_anova(\n    soil_var = .x,\n    log_trans = .y,\n    strat = \"clim_order_partsize\",\n    df = val_all_grps\n  )\n)\n\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\nWarning: Could not recover model data from environment. Please make sure your\n  data is available in your workspace.\n  Trying to retrieve data from the model frame now.\n\n\n[[1]]\n[[1]]$f_used\n[1] \"corr_percent_stable_gr2mm ~ clim_order_partsize\"\n\n[[1]]$plots\n[[1]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[1]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[2]]\n[[2]]$f_used\n[1] \"log(ugC_g_day) ~ clim_order_partsize\"\n\n[[2]]$plots\n[[2]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[3]]\n[[3]]$f_used\n[1] \"poxc_mg_kg ~ clim_order_partsize\"\n\n[[3]]$plots\n[[3]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[3]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[4]]\n[[4]]$f_used\n[1] \"log(mbc_ug_g_soil) ~ clim_order_partsize\"\n\n[[4]]$plots\n[[4]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[4]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[5]]\n[[5]]$f_used\n[1] \"log(mbn_ug_g_soil) ~ clim_order_partsize\"\n\n[[5]]$plots\n[[5]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[5]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[6]]\n[[6]]$f_used\n[1] \"log(total_living_microbial_biomass_plfa_ng_g) ~ clim_order_partsize\"\n\n[[6]]$plots\n[[6]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[6]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[7]]\n[[7]]$f_used\n[1] \"log(total_bacteria_plfa_ng_g) ~ clim_order_partsize\"\n\n[[7]]$plots\n[[7]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[7]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[8]]\n[[8]]$f_used\n[1] \"log(total_fungi_plfa_ng_g) ~ clim_order_partsize\"\n\n[[8]]$plots\n[[8]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[8]]$plots[[2]]\n\n\n\n\n\n\n\n\n\n\n\n\n[[9]]\n[[9]]$f_used\n[1] \"org_c_wt_percent ~ clim_order_partsize\"\n\n[[9]]$plots\n[[9]]$plots[[1]]\n\n\n\n\n\n\n\n\n\n\n[[9]]$plots[[2]]",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#effect-sizes-for-variance-explained",
    "href": "36-update-welch-compare-variation.html#effect-sizes-for-variance-explained",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.11 Effect sizes for variance explained",
    "text": "20.11 Effect sizes for variance explained\nAdd updated references from reading notes on 2023-03-16 re: calculating effect sizes from F statistics (which is what {effectsize} is doing for us behind the scenes when we pass the Welch’s ANOVA results to epsilon_squared()\nI was originally planning on calculating eta-squared as my effect size, using either: effectsize::eta_squared() for ANOVA or effectsize::rank_eta_squared(). But in reading the documentation for eta_squared() from {effectsize}, I learned that there are other, less biased options for calculating this value: omega-squared and epsilon-squared.\nI found a very helpful, recent reference, Iacobucci et al., (2023), that explains the differences between eta, epsilon, and omega when used as effect sizes of variance explained. See their paper for the details, but the easiest way to think about it is that using epsilon-squared or omega-squared is essentially like reporting an adjusted R2 value instead of a regular R2 value. It’s better to use epsilon squared because the equation accounts for small sample size, which is relevant in our case.\nBelow is an example with MBC, this is just me figuring out what the different functions outputs look like, and how NAs are handled.\n\n20.11.1 Example\n\n## welch's ANOVA - MBC by soil_group (k-means)\n# need to drop grp-4 because only 1 observation, can't\n# calculate variance. Wrote this step into the function\n# calc_welch_eps_sq() below.\nsub_dat &lt;- val_all_grps %&gt;% \n  filter(soil_group != \"grp-4\")\n\n\ntest_welch &lt;- oneway.test(data = sub_dat,\n            formula = mbc_ug_g_soil ~ soil_group,\n           var.equal = FALSE,\n            na.action = \"na.omit\")\n\ntest_welch\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  mbc_ug_g_soil and soil_group\nF = 5.5237, num df = 4.000, denom df = 22.848, p-value = 0.002911\n\neffectsize::epsilon_squared(test_welch, partial = FALSE)\n\n`var.equal = FALSE` - effect size is an approximation.\n\n\n\n  \n\n\n\nBelow I test out Welch’s ANOVA followed by calculation of epsilon squared. This works, and after reading more from the {effectsize} documentation here I think I understand how. I can pass the result from oneway.test(var.equal = FALSE) to effectsize::epsilon_squared() and there is an intermediate step performed where the F statistic from oneway.test is converted to the effect size (could be partial eta-squared, omega-squared, or epsilon-squared)\n\n\n20.11.2 Setup dataframe\nTransformations needed:\n\nNone: aggregate stability, POXC, SOC\nLog10: total biomass PLFA, total bacteria PLFA, total fungi PLFA, MBC, MBN, PMC\n\n\n# grouping_factors is constructed above in section \"Create additional\n# grouping variables\" \nstrat_opts &lt;- grouping_factors\n\nmod_frame &lt;- tidyr::crossing(dep_vars, strat_opts)\n\n# apply the transformations I decided on above after looking at \n# \"checking distributions\" plots\ndat_trans &lt;- val_all_grps %&gt;% \n  mutate(across(.cols = c(total_living_microbial_biomass_plfa_ng_g,\n                          total_bacteria_plfa_ng_g,\n                          total_fungi_plfa_ng_g,\n                          mbc_ug_g_soil,\n                          mbn_ug_g_soil,\n                          ugC_g_day), \n                .fns = log10\n                ))\n# creating a farm dataset so I can compare results \n# for including vs. excluding UD sites\ndat_farm &lt;- dat_trans %&gt;% \n  filter(!str_detect(val_unit_id, \"UD\"))\n\n\n\n20.11.3 Function run ANOVA & calculate epsilon squared\n\ncalc_welch_eps_sq &lt;- function(soil_var,\n                              strat,\n                              df = dat_trans) {\n  # count observations (validation points) per group\n  # based on stratification variable provided\n  n_obs_per_group &lt;- df %&gt;%\n    count(.data[[strat]])\n  \n  # identify groups with 2 or less validation points\n  # want to drop these (because we need to be able to \n  # calculate variance\n  few_obs_groups &lt;- n_obs_per_group %&gt;%\n    filter(n &lt;= 2) %&gt;%\n    pull(.data[[strat]])\n  \n  if (length(few_obs_groups) == 0){\n    dat_subset &lt;- df\n    \n  }else{\n    dat_subset &lt;- df %&gt;%\n      filter(!(.data[[strat]] %in% few_obs_groups))\n  }\n    \n    my_formula &lt;- as.formula(paste0(soil_var, \" ~ \", strat))\n    \n    \n    mod_obj &lt;- oneway.test(\n      formula = my_formula,\n      data = dat_subset,\n      var.equal = FALSE,\n      na.action = \"na.omit\"\n    )\n    \n    eps_sq &lt;- effectsize::epsilon_squared(model = mod_obj,\n                                          # partial FALSE b/c one-way test\n                                          # we don't have other vars to\n                                          # break the variance down by\n                                          partial = FALSE)\n    \n    return(eps_sq)\n    \n}\n\n\n\n20.11.4 Calculate epsilon squared\n\n# quieted the message \"`var.equal = FALSE` - effect size is an approximation.\", it's a reminder that this effect size is an apporximation because we are estimating it based on the f statistic from our Welch's ANOVA\n\neps_inputs &lt;- mod_frame %&gt;% \n  rename(strat = strat_opts, \n         soil_var = dep_vars) %&gt;% \n  select(soil_var, strat) \n\n# returns bunch of reminders about how the eff size is \n# an approximation (b/c we are calculating it from F stat,\n# not directly from SS)\neps_results &lt;- eps_inputs %&gt;%\n  mutate(eps_results = map2(\n    .x = soil_var,\n    .y = strat,\n    .f = ~ calc_welch_eps_sq(\n      soil_var = .x,\n      strat = .y,\n      df = dat_trans\n    )\n  ))\n\n\neps_farm &lt;- eps_inputs %&gt;%\n  mutate(eps_results = map2(\n    .x = soil_var,\n    .y = strat,\n    .f = ~ calc_welch_eps_sq(\n      soil_var = .x,\n      strat = .y,\n      df = dat_farm\n    )\n  ))\n\n\n\n20.11.5 Clean up data\n\neps_long &lt;- eps_results %&gt;% \n  unnest(eps_results) %&gt;% \n  rename(epsilon_sq = Epsilon2)\n\n\nfarm_long &lt;- eps_farm %&gt;% \n  unnest(eps_results) %&gt;% \n  rename(epsilon_sq = Epsilon2)\n\n# eps_wide &lt;- eps_long %&gt;% \n#   pivot_wider(values_from = c(\"epsilon_sq\", \"type\"), \n#               names_from = \"strat\")\n\n\n\n20.11.6 Table of variance explained (W/ UD)\n\nvar_tbl_ud &lt;- eps_long %&gt;% \n  mutate(soil_var = case_when(\n    str_detect(soil_var, \"stable\") ~ \"Agg Stab\",\n    str_detect(soil_var, \"mbc\") ~ \"MBC\",\n    str_detect(soil_var, \"mbn\") ~ \"MBN\",\n    str_detect(soil_var, \"poxc\") ~ \"POXC\",\n    str_detect(soil_var, \"total_bact\") ~ \"Bact PLFA\",\n    str_detect(soil_var, \"total_fungi\") ~ \"Fungi PLFA\",\n    str_detect(soil_var, \"total_living\") ~ \"Total PLFA\",\n    str_detect(soil_var, \"ugC\") ~ \"PMC\",\n    str_detect(soil_var, \"org_c\") ~ \"SOC\"\n  )) %&gt;% \n  select(soil_var, strat, epsilon_sq) %&gt;% \n  pivot_wider(names_from = \"soil_var\",\n              values_from = \"epsilon_sq\") %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, digits = 2))) %&gt;% \n  gt() %&gt;% \n  tab_header(title = \"Epsilon squared (variation explained) with UD\") \n\n  \nvar_tbl_ud\n\n\n\n\n\n\n\n\nEpsilon squared (variation explained) with UD\n\n\nstrat\nAgg Stab\nMBC\nMBN\nSOC\nPOXC\nBact PLFA\nFungi PLFA\nTotal PLFA\nPMC\n\n\n\n\nclim_order\n0.11\n0.41\n0.53\n0.58\n0.80\n0.51\n0.44\n0.48\n0.00\n\n\nclim_order_partsize\n0.11\n0.53\n0.59\n0.66\n0.88\n0.71\n0.53\n0.71\n0.00\n\n\nclim_partsize\n0.11\n0.44\n0.64\n0.51\n0.62\n0.77\n0.63\n0.77\n0.00\n\n\nclim_suborder_partsize\n0.06\n0.47\n0.53\n0.61\n0.86\n0.67\n0.46\n0.68\n0.00\n\n\nkm_soil_clim\n0.16\n0.38\n0.52\n0.59\n0.81\n0.55\n0.46\n0.52\n0.00\n\n\nmlra_km\n0.51\n0.56\n0.70\n0.66\n0.89\n0.58\n0.46\n0.56\n0.00\n\n\nmlra_order\n0.30\n0.38\n0.52\n0.58\n0.81\n0.52\n0.49\n0.49\n0.00\n\n\nmlra_partsize\n0.32\n0.42\n0.58\n0.48\n0.48\n0.69\n0.55\n0.69\n0.00\n\n\nmlra_short\n0.37\n0.47\n0.63\n0.47\n0.35\n0.53\n0.52\n0.48\n0.00\n\n\norder_partsize\n0.16\n0.44\n0.61\n0.32\n0.31\n0.38\n0.21\n0.42\n0.00\n\n\npartsize\n0.11\n0.44\n0.66\n0.00\n0.12\n0.46\n0.30\n0.50\n0.00\n\n\nsoil_group\n0.10\n0.47\n0.49\n0.59\n0.60\n0.52\n0.26\n0.50\n0.00\n\n\nsuborder_clim\n0.08\n0.41\n0.53\n0.58\n0.80\n0.50\n0.43\n0.46\n0.00\n\n\nsuborder_mlra\n0.25\n0.38\n0.52\n0.57\n0.80\n0.48\n0.45\n0.45\n0.00\n\n\nsuborder_partsize\n0.09\n0.38\n0.53\n0.40\n0.46\n0.51\n0.30\n0.54\n0.00\n\n\ntaxorder\n0.03\n0.33\n0.45\n0.27\n0.25\n0.00\n0.07\n0.02\n0.03\n\n\ntaxsuborder\n0.04\n0.40\n0.54\n0.41\n0.44\n0.21\n0.25\n0.24\n0.00\n\n\n\n\n\n\n\ngtsave(var_tbl_ud, filename = \"variation_explained_with_ud.docx\")\n\n\n\n20.11.7 Table of variance explained (no UD)\n\nvar_tbl_farm &lt;- farm_long %&gt;% \n  mutate(soil_var = case_when(\n    str_detect(soil_var, \"stable\") ~ \"Agg Stab\",\n    str_detect(soil_var, \"mbc\") ~ \"MBC\",\n    str_detect(soil_var, \"mbn\") ~ \"MBN\",\n    str_detect(soil_var, \"poxc\") ~ \"POXC\",\n    str_detect(soil_var, \"total_bact\") ~ \"Bact PLFA\",\n    str_detect(soil_var, \"total_fungi\") ~ \"Fungi PLFA\",\n    str_detect(soil_var, \"total_living\") ~ \"Total PLFA\",\n    str_detect(soil_var, \"ugC\") ~ \"PMC\",\n    str_detect(soil_var, \"org_c\") ~ \"SOC\"\n  )) %&gt;% \n  select(soil_var, strat, epsilon_sq) %&gt;% \n  pivot_wider(names_from = \"soil_var\",\n              values_from = \"epsilon_sq\") %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, digits = 2))) %&gt;% \n  gt() %&gt;% \n tab_header(title = \"Epsilon squared (variation explained) no UD\") \n  \nvar_tbl_farm\n\n\n\n\n\n\n\n\nEpsilon squared (variation explained) no UD\n\n\nstrat\nAgg Stab\nMBC\nMBN\nSOC\nPOXC\nBact PLFA\nFungi PLFA\nTotal PLFA\nPMC\n\n\n\n\nclim_order\n0.33\n0.70\n0.75\n0.89\n0.88\n0.76\n0.56\n0.72\n0.59\n\n\nclim_order_partsize\n0.31\n0.78\n0.75\n0.93\n0.93\n0.73\n0.54\n0.69\n0.59\n\n\nclim_partsize\n0.28\n0.55\n0.70\n0.35\n0.38\n0.68\n0.60\n0.63\n0.45\n\n\nclim_suborder_partsize\n0.33\n0.79\n0.75\n0.92\n0.93\n0.78\n0.57\n0.72\n0.71\n\n\nkm_soil_clim\n0.48\n0.78\n0.74\n0.91\n0.89\n0.80\n0.53\n0.78\n0.61\n\n\nmlra_km\n0.60\n0.78\n0.78\n0.90\n0.90\n0.82\n0.58\n0.83\n0.62\n\n\nmlra_order\n0.50\n0.81\n0.76\n0.91\n0.89\n0.81\n0.69\n0.81\n0.66\n\n\nmlra_partsize\n0.49\n0.83\n0.79\n0.75\n0.56\n0.74\n0.66\n0.74\n0.63\n\n\nmlra_short\n0.54\n0.83\n0.83\n0.79\n0.61\n0.78\n0.70\n0.77\n0.71\n\n\norder_partsize\n0.26\n0.57\n0.70\n0.57\n0.45\n0.05\n0.16\n0.16\n0.52\n\n\npartsize\n0.08\n0.53\n0.72\n0.19\n0.12\n0.07\n0.27\n0.21\n0.40\n\n\nsoil_group\n0.52\n0.82\n0.73\n0.84\n0.75\n0.78\n0.35\n0.79\n0.62\n\n\nsuborder_clim\n0.37\n0.72\n0.76\n0.89\n0.89\n0.80\n0.58\n0.75\n0.72\n\n\nsuborder_mlra\n0.44\n0.81\n0.76\n0.90\n0.90\n0.83\n0.68\n0.82\n0.72\n\n\nsuborder_partsize\n0.32\n0.68\n0.71\n0.67\n0.63\n0.43\n0.22\n0.42\n0.65\n\n\ntaxorder\n0.16\n0.55\n0.70\n0.59\n0.47\n0.20\n0.30\n0.30\n0.53\n\n\ntaxsuborder\n0.38\n0.70\n0.76\n0.71\n0.70\n0.52\n0.41\n0.53\n0.73\n\n\n\n\n\n\n\ngtsave(var_tbl_farm, \"variation_explained_farms_only.docx\")\n\n\nstrat_opts &lt;- c(\"soil_group\", \"mlra_short\", \"taxsuborder\",\n                \"taxgrtgroup\", \"km_soil_clim\", \"suborder_clim\",\n                \"suborder_partsize\", \"suborder_mlra\", \"clim_order_partsize\",\n                \"mlra_km\")\n\n\ndat_farm %&gt;% \n  count(soil_group)\n\n\n  \n\n\ndat_farm %&gt;% \n  count(km_soil_clim)\n\n\n  \n\n\ndat_farm %&gt;% \n  count(mlra_short)\n\n\n  \n\n\ndat_farm %&gt;% \n  count(suborder_mlra)\n\n\n  \n\n\ndat_farm %&gt;% \n  count(mlra_km)\n\n\n  \n\n\ndat_farm %&gt;% \n  count(taxsuborder)\n\n\n  \n\n\ndat_farm %&gt;% \n  count(suborder_partsize)\n\n\n  \n\n\ndat_farm %&gt;% \n  count(clim_order_partsize)\n\n\n  \n\n\ndat_farm %&gt;% \n  count(taxgrtgroup)\n\n\n  \n\n\n\n\n\n20.11.8 Plots of variance explained\n\nall_pts_labelled &lt;- eps_long %&gt;% \n # filter(strat != \"taxgrtgroup\") %&gt;% \n  mutate(soil_var = case_when(\n    soil_var == \"mbc_ug_g_soil\" ~ \"MBC\",\n    soil_var == \"mbn_ug_g_soil\" ~ \"MBN\", \n    soil_var == \"corr_percent_stable_gr2mm\" ~ \"Aggregate Stab.\",\n    soil_var == \"poxc_mg_kg\" ~ \"POXC\", \n    soil_var == \"total_bacteria_plfa_ng_g\" ~ \"Bacteria PLFA\",\n    soil_var == \"total_fungi_plfa_ng_g\" ~ \"Fungi PLFA\",\n    soil_var == \"total_living_microbial_biomass_plfa_ng_g\" ~ \"Total Biomass PLFA\", \n    soil_var == \"poxc_mg_kg\" ~ \"POXC\",\n    soil_var == \"ugC_g_day\" ~ \"PMC\",\n    soil_var == \"org_c_wt_percent\" ~ \"SOC\"\n  ),\n  strat = case_when(\n    strat == \"mlra_short\" ~ \"MLRA\",\n    strat == \"soil_group\" ~ \"KM CLUSTER SOIL\",\n    strat == \"taxsuborder\" ~ \"SUBORDER\",\n    strat == \"km_soil_clim\" ~ \"KM CLUSTER SOIL x\\nCLIMATE\",\n    strat == \"suborder_clim\" ~ \"SUBORDER x\\nCLIMATE\",\n    strat == \"suborder_mlra\" ~ \"MLRA x\\nSUBORDER\",\n    strat == \"suborder_partsize\" ~ \"SUBORDER x\\nPARTICLE SIZE CLASS\",\n    strat == \"taxgrtgroup\" ~ \"GREAT GROUP\"\n  )) %&gt;%\n  group_by(soil_var) %&gt;%\n  mutate(max_epsilon = max(epsilon_sq),\n         top_strat = case_when(\n           epsilon_sq == max_epsilon ~ \"Y\",\n           TRUE ~ \"N\"\n         ), \n  # round for nice labels \n         epsilon_sq_lab = round(epsilon_sq, digits = 2))\n\n\n# saving in case I want to compare farm (no UD) vs. w/ UD results\n\nfarm_labelled &lt;- farm_long %&gt;%\n  filter(strat != \"taxgrtgroup\") %&gt;%\n  mutate(soil_var = case_when(\n    soil_var == \"mbc_ug_g_soil\" ~ \"MBC\",\n    soil_var == \"mbn_ug_g_soil\" ~ \"MBN\",\n    soil_var == \"corr_percent_stable_gr2mm\" ~ \"Aggregate Stab.\",\n    soil_var == \"poxc_mg_kg\" ~ \"POXC\",\n    soil_var == \"total_bacteria_plfa_ng_g\" ~ \"Bacteria PLFA\",\n    soil_var == \"total_fungi_plfa_ng_g\" ~ \"Fungi PLFA\",\n    soil_var == \"total_living_microbial_biomass_plfa_ng_g\" ~ \"Total Biomass PLFA\",\n    soil_var == \"poxc_mg_kg\" ~ \"POXC\",\n    soil_var == \"ugC_g_day\" ~ \"PMC\"\n  ),\n  strat = case_when(\n    strat == \"mlra_short\" ~ \"MLRA\",\n    strat == \"soil_group\" ~ \"KM CLUSTER SOIL\",\n    strat == \"taxsuborder\" ~ \"SUBORDER\",\n    strat == \"km_soil_clim\" ~ \"KM CLUSTER SOIL x\\nCLIMATE\",\n    strat == \"suborder_clim\" ~ \"SUBORDER x\\nCLIMATE\",\n    strat == \"suborder_mlra\" ~ \"MLRA x\\nSUBORDER\",\n    strat == \"suborder_partsize\" ~ \"SUBORDER x\\nPARTICLE SIZE CLASS\",\n    strat == \"taxgrtgroup\" ~ \"GREAT GROUP\"\n  )) %&gt;%\n  group_by(soil_var) %&gt;%\n  mutate(max_epsilon = max(epsilon_sq),\n         top_strat = case_when(\n           epsilon_sq == max_epsilon ~ \"Y\",\n           TRUE ~ \"N\"\n         ), \n  # round for nice labels \n         epsilon_sq_lab = round(epsilon_sq, digits = 2))",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#with-ud-variance-explained-plot",
    "href": "36-update-welch-compare-variation.html#with-ud-variance-explained-plot",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.12 (with UD) variance explained plot",
    "text": "20.12 (with UD) variance explained plot\nLike original, just soil cluster (KM Cluster), MLRA, and suborder\n\nplot_var_order &lt;-\n  c(\n    \"POXC\",\n    \"SOC\",\n  #  \"Total Biomass PLFA\",\n    \"Bacteria PLFA\",\n    \"MBC\",\n    \"MBN\",\n    \"Fungi PLFA\",\n    \"Aggregate Stab.\",\n    \"PMC\"\n  )\n\n\neps_bar_plot &lt;- all_pts_labelled %&gt;%\n  filter(soil_var != \"Total Biomass PLFA\",\n         strat %in% c(\"KM CLUSTER SOIL\",\n                      \"MLRA\",\n                      \"SUBORDER\"\n                    #  \"GREAT GROUP\"\n                      )) %&gt;% \n  ggplot() +\n  geom_col(aes(x = soil_var, y = epsilon_sq_lab, fill = strat),\n           position = position_dodge(),\n           width = 0.7, \n           color = \"black\") +\n  # geom_text(aes(x = 8.25, y = 0.85, label = \"A.\"), size = 3) +\n  #   geom_text(aes(x = 6.25, y = 0.85, label = \"B.\"), size = 3) +\n  #   geom_text(aes(x = 4.25, y = 0.85, label = \"C.\"), size = 3) +\n  #   geom_text(aes(x = 1.25, y = 0.85, label = \"D.\"), size = 3) +\n  scale_x_discrete(limits = rev(plot_var_order)) +\n  scale_y_continuous(breaks = seq(0, 0.9, 0.1), limits = c(0, 0.9)) +\n#   geom_vline(aes(xintercept = 1.5)) +\n#   geom_vline(aes(xintercept = 4.5)) +\n#   geom_vline(aes(xintercept = 6.5)) +\n  coord_cartesian(xlim = c(0, 0.7),\n                  clip = 'off') +\n  coord_flip() +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 1),\n    legend.text = element_text(size = 8),\n    legend.justification = c(1, 1)\n  ) +\n  xlab(\"\") +\n  ylab(\"Epsilon Squared\") +\n  #labs(caption = \"With undisturbed (Welch's ANOVA)\") +\n  scale_fill_manual(values = c('#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02','#a6761d','#666666')) +\n  guides(fill = guide_legend(title = \"Groups\")) \n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\nggsave(\"figs/epsilon_sq_barplot_ud_update20240323.png\", plot = eps_bar_plot, \n       width = 5, height = 5, units = \"in\")\n\nAdd climate / other grouping combinations (suborder + MLRA, suborder + particle size class, etc.)\n\nplot_var_order &lt;-\n  c(\n    \"POXC\",\n    \"SOC\",\n  #  \"Total Biomass PLFA\",\n    \"Bacteria PLFA\",\n    \"MBC\",\n    \"MBN\",\n    \"Fungi PLFA\",\n    \"Aggregate Stab.\",\n    \"PMC\"\n  )\n\n\neps_bar_plot_new &lt;- all_pts_labelled %&gt;%\n  filter(soil_var != \"Total Biomass PLFA\",\n         strat %in% c(\"KM CLUSTER SOIL x\\nCLIMATE\",\n                      #\"SUBORDER x\\nCLIMATE\",\n                      \"SUBORDER x\\nPARTICLE SIZE CLASS\",\n                      \"MLRA x\\nSUBORDER\")) %&gt;% \n  ggplot() +\n  geom_col(aes(x = soil_var, y = epsilon_sq_lab, fill = strat),\n           position = position_dodge(),\n           width = 0.7, \n           color = \"black\") +\n  # geom_text(aes(x = 8.25, y = 0.85, label = \"A.\"), size = 3) +\n  #   geom_text(aes(x = 6.25, y = 0.85, label = \"B.\"), size = 3) +\n  #   geom_text(aes(x = 4.25, y = 0.85, label = \"C.\"), size = 3) +\n  #   geom_text(aes(x = 1.25, y = 0.85, label = \"D.\"), size = 3) +\n  scale_x_discrete(limits = rev(plot_var_order)) +\n  scale_y_continuous(breaks = seq(0, 0.9, 0.1), limits = c(0, 0.9)) +\n#   geom_vline(aes(xintercept = 1.5)) +\n#   geom_vline(aes(xintercept = 4.5)) +\n#   geom_vline(aes(xintercept = 6.5)) +\n  coord_cartesian(xlim = c(0, 0.7),\n                  clip = 'off') +\n  coord_flip() +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 1),\n    legend.text = element_text(size = 8),\n    legend.justification = c(1, 1)\n  ) +\n  xlab(\"\") +\n  ylab(\"Epsilon Squared\") +\n  #labs(caption = \"With undisturbed (Welch's ANOVA)\") +\n  scale_fill_manual(values = c('#fb9a99','#33a02c','#b2df8a','#666666')) +\n  guides(fill = guide_legend(title = \"Groups\")) \n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\nggsave(\"figs/epsilon_sq_barplot_ud_update20240323_newgrps.png\", plot = eps_bar_plot_new, \n       width = 6, height = 5, units = \"in\")",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#no-ud-variance-explained-table-plot",
    "href": "36-update-welch-compare-variation.html#no-ud-variance-explained-table-plot",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.13 (No UD) variance explained table & plot",
    "text": "20.13 (No UD) variance explained table & plot\n\nfarm_long %&gt;% \n  filter(strat != \"taxgrtgroup\") %&gt;% \n  mutate(soil_var = case_when(\n    str_detect(soil_var, \"stable\") ~ \"Agg Stab\",\n    str_detect(soil_var, \"mbc\") ~ \"MBC\",\n    str_detect(soil_var, \"mbn\") ~ \"MBN\",\n    str_detect(soil_var, \"poxc\") ~ \"POXC\",\n    str_detect(soil_var, \"total_bact\") ~ \"Bact PLFA\",\n    str_detect(soil_var, \"total_fungi\") ~ \"Fungi PLFA\",\n    str_detect(soil_var, \"total_living\") ~ \"Total PLFA\",\n    str_detect(soil_var, \"ugC\") ~ \"PMC\",\n    str_detect(soil_var, \"org_c\") ~ \"SOC\"\n  )) %&gt;% \n  select(soil_var, strat, epsilon_sq) %&gt;% \n  pivot_wider(names_from = \"strat\",\n              values_from = \"epsilon_sq\") %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, digits = 2)))\n\n\n  \n\n\n\n\n# decided to use the same order as the above plot that also includes the UD data, to make comparison between them easier\n\n# f_plot_var_order &lt;- c(\"POXC\", \"Total Biomass PLFA\",\"Aggregate Stab.\", \"Bacteria PLFA\", \"MBC\", \"MBN\", \"Fungi PLFA\", \"PMC\")\n\n\nfarm_bar_plot &lt;- farm_labelled %&gt;%\n  filter(soil_var != \"Total Biomass PLFA\") %&gt;% \n  ggplot() +\n  geom_col(aes(x = soil_var, y = epsilon_sq_lab, fill = strat),\n           position = position_dodge(),\n           width = 0.7,\n           color = \"black\") +\n  # geom_text(aes(x = 1, y = 0.9, label = \"D.\"), size = 3) +\n  # geom_text(aes(x = 3, y = 0.9, label = \"C.\"), size = 3) +\n  # geom_text(aes(x = 7.5, y = 0.9, label = \"A.\"), size = 3) +\n  #   geom_text(aes(x = 5.5, y = 0.9, label = \"B.\"), size = 3) +\n  scale_x_discrete(limits = rev(plot_var_order)) + \n  scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1.05)) +\n # geom_vline(aes(xintercept = 1.5)) +\n #  geom_vline(aes(xintercept = 4.5)) +\n #  geom_vline(aes(xintercept = 6.5)) +\n  coord_flip() +\n  theme_bw() +\ntheme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 1),\n    legend.text = element_text(size = 8),\n    legend.justification = c(1, 1)\n  ) +\n  xlab(\"\") +\n  ylab(\"Epsilon Squared\") +\n  # labs(caption = \"Just farm data/no undisturbed (Welch's ANOVA)\") +\n  scale_fill_manual(values = c('#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02','#a6761d','#666666')) +\n  guides(fill = guide_legend(title = \"Grouping Method\"))\n\nggsave(\n  \"figs/epsilon_sq_barplot_farms_update20240317.png\",\n  plot = farm_bar_plot,\n  width = 5,\n  height = 5,\n  units = \"in\"\n)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  },
  {
    "objectID": "36-update-welch-compare-variation.html#what-is-going-on-with-pmc",
    "href": "36-update-welch-compare-variation.html#what-is-going-on-with-pmc",
    "title": "20  Welch ANOVA - compare variation explained",
    "section": "20.14 What is going on with PMC?",
    "text": "20.14 What is going on with PMC?\nPMC returns 0% variance explained for all three stratification options: K-means, Region, and Taxonomic sub-group. Is this really correct? Let’s double check below with the region stratification to make sure there isn’t something weird with the data.\n\nlm_pmc &lt;- lm(ugC_g_day ~ region, data = sub_dat, na.action = \"na.omit\")\n\naov_pmc &lt;- car::Anova(lm_pmc, type = 2)\n\n# big residuals number... that explains it\naov_pmc\n\n\n  \n\n\neffectsize::epsilon_squared(aov_pmc, partial = FALSE)\n\n\n  \n\n\n# what about eta-squared?\neffectsize::eta_squared(aov_pmc, partial = FALSE)\n\n\n  \n\n\n# what if we drop the UD sites?\n\nfarmdat &lt;- sub_dat %&gt;% filter(!str_detect(val_unit_id, \"UD\"))\n\nlm_farm &lt;- lm(ugC_g_day ~ region, data = farmdat, na.action = \"na.omit\")\n\naov_farm &lt;- car::Anova(lm_farm, type = 2)\n\n# big residuals number... that explains it\naov_farm\n\n\n  \n\n\neffectsize::epsilon_squared(aov_farm, partial = FALSE)\n\n\n  \n\n\n# what about eta-squared?\neffectsize::eta_squared(aov_farm, partial = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Welch ANOVA - compare variation explained</span>"
    ]
  }
]