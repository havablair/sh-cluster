# Compare variation explained

## Overview

The goal is to use selected soil health indicators from the CIG dataset to compare the amount of variation explained when grouping the data by geographic region (per the CIG design) vs. taxaonomy vs. soil health group (generated by k-means clustering).

This is meant to be a demonstration of how the soil health groups generated by k-means might be useful in helping to set benchmarks and expectations for soil health indicators measured on different types of soil.
The logic is that by reducing the within-group variation through stratification by group or region, we make it easier to detect **changes resulting from management differences**.

## Models

Response variables: soil health indicators (perhaps the NRCS tech note list?).
Should we use just 1 year of data to avoid pulling in year-to-year variability?
Could choose 2019, because this is the year for which we have PLFA data, and PLFA is on the NRCS list.

Variables from the NRCS Soil Health Tech Note recommended methods:

-   Aggregate stability
-   PMC
-   enzyme activities (BG, NAG, PASE) we have cellobiohydrolase data too, but skipping it b/c we have more missing data for this one (supply chain issues) and it's not on the NRCS list, which I'm using as my justification for choosing these ones
-   POXC
-   ACE Protein
-   PLFA

```{r setup}

library(tidyverse)
library(glue)
library(ggbeeswarm)
library(lmerTest)
library(rstatix)

```

## Data

As part of the validation process for my k-means analysis, I determined the soil health group assigned to each CIG sampling area at the mapunit level (hillslope within management condition).

### Load data

```{r data}
#| message: false
#| warning: false

# this file created in "extract_raster_values_at_validation_points.R"
# it is at the "sample" level
cig_comps <- read_csv("data/cig_incl_components_table.csv") %>% 
  # for now, just using 2019 data so we avoid the year-to-year variability we
  # know is there, and b/c we have PLFA data for 2019 only
  filter(sample_id %in% c(1:243))


# this is for disambiguating which MUKEY should be assigned
# to each CIG val_unit_id (mapunit), because sometimes the
# GPS points landed in multiple mapunits
# the code that handles this is in "extract_raster_values_at_validation_points.R
cig_mukey_votes <- read_csv("data/cig_mukey_votes_for_validation.csv") 

# cluster assigned to each CIG point. this dataset is created in "prep_validation_data_for_pca_version.R"
# it is at the "validation unit" level (summarized to hillslope within management)
cig_val <- read_csv("data/validation_data_pca_clusters_and_soil_props.csv") %>% 
  filter(str_detect(val_unit_id, "CV") |
           str_detect(val_unit_id, "SH") |
         str_detect(val_unit_id, "UD")) %>%
  mutate(region = str_extract(val_unit_id, "[:alpha:]{2}"),
         soil_group = glue("grp-{k_8}")) %>% 
  select(val_unit_id, soil_group, k_8, region)

# want all the CIG lab observations (not averaged to the plot level)
lab <- read_csv("../CIG/cig-main/cig_lab_data_all_20230301.csv") %>%
  mutate(val_unit_id = glue("{site}-{treatment}-{position}")) %>%
  select(
    sample_id,
    val_unit_id,
    region,
    position,
    corr_percent_stable_gr2mm,
    ugC_g_day,
    BG_activity,
    NAG_activity,
    P_activity,
    poxc_mg_kg,
    mean_protein_mg_g,
    mbc_ug_g_soil,
    mbn_ug_g_soil,
    total_living_microbial_biomass_plfa_ng_g,
    total_bacteria_plfa_ng_g,
    total_fungi_plfa_ng_g,
    total_bacteria_percent_of_tot_biomass,
    total_fungi_percent_of_tot_biomass
  ) %>% 
  filter(sample_id %in% c(1:243))

# missing data due to sample contamination in 2019 samples
# that were run for PLFA. Pulling MBN values from 2020
# for this validation site only after confirming that the values 
# are in a similar range for this soil type, hillslope position,
# and region
mbn_mw4_cva <-
  read_csv("../CIG/cig-main/cig_lab_data_all_20230301.csv") %>%
  mutate(val_unit_id = glue("{site}-{treatment}-{position}")) %>%
  filter(sample_id %in% c(474:476)) %>%
  pull(mbn_ug_g_soil) %>%
  mean(., na.rm = TRUE)



```

### Summarize to mapunit level

Recall that the lab data and the component data need to be summarized to the "mapunit" level.
This involves averaging the soil properties and determining the majority taxonomic classification for each MUKEY based on the included components (recall that most MUKEYs have just 1 contributing component, so we just need to account for those that have \>1 contributing component to determine, for each level of taxonomy I look at, what the majority assignment should be).

Starting with the taxonomy data:

```{r}
# how many unique mukeys are we working with?
(cig_mukeys <- cig_comps %>%
    pull(mukey) %>%
    unique() %>%
    length())

# Q: which mukeys have multiple components contributing data?
# A: 16 unique mukeys
cig_comps %>% 
  select(mukey, cokey, taxclname:taxpartsize) %>% 
  distinct() %>% 
  group_by(mukey) %>% 
  count() %>% 
  filter(n>1)

# goal: assign each mukey a representative suborder
# based on the suborders of the components within it.
# should get back object w/ length 37 (# of unique mukeys)
cig_comps %>% 
  select(mukey, cokey, comppct_r, taxsuborder) %>% 
  distinct() %>% 
  group_by(mukey, taxsuborder) %>% 
  summarise(tot_percent = sum(comppct_r), .groups = "drop") %>% 
  group_by(mukey) %>% 
  slice_max(tot_percent)


```

Function for determining representative taxonomic level .
Using similar code to the example with suborder above, this function allows me to summarize the representative taxonomic level (order, suborder, etc.) based on % area.

```{r}
# calculate representative taxonomy
calc_rep_tax <- function(tax_level, comp_df){
  
  comp_df %>% 
  select(mukey, cokey, comppct_r, {{tax_level}}) %>% 
  distinct() %>% 
  group_by(mukey, {{tax_level}}) %>% 
  summarise(tot_percent = sum(comppct_r), .groups = "drop") %>% 
  group_by(mukey) %>% 
  slice_max(tot_percent) %>% 
    select(-tot_percent)
  
}


order <- calc_rep_tax(taxorder, cig_comps)
suborder <- calc_rep_tax(taxsuborder, cig_comps)
grt_grp <- calc_rep_tax(taxgrtgroup, cig_comps)
sub_grp <- calc_rep_tax(taxsubgrp, cig_comps)
family <- calc_rep_tax(taxclname, cig_comps)

tax_joined <- list(order, suborder, grt_grp, sub_grp, family) %>% 
  purrr::reduce(., left_join, by = "mukey")

tax_val_key <- cig_comps %>% 
  select(val_unit_id, mukey) %>% 
  distinct()

```

Now time to summarize the lab data to the mapunit level, this is easier than the taxonomy because we can just take the average.
Recall that we are working with 2019 data only.

```{r}

lab_summary <- lab %>% 
  select(-sample_id) %>% 
  group_by(val_unit_id) %>% 
  summarise(across(where(is.numeric),
                   ~mean(.x, na.rm = TRUE)))
```

### Join taxonomic and lab info with validation dataset

Here, we create a dataset with one row for each validation unit in the CIG dataset.
It includes information about representative taxonomic classification at different levels (order, suborder, great group) and also representative values for the soil health indicators we want to analyze further below when comparing variance explained by different stratification options (region, taxonomy, k-means group).

```{r}


val_mukey <- left_join(cig_val, cig_mukey_votes, by = "val_unit_id") %>% 
  select(-c(n, max_vote))

val_mukey_lab <- left_join(val_mukey, lab_summary, by = "val_unit_id")

val_all <- left_join(val_mukey_lab, tax_joined, by = "mukey") %>%
 # on list from old key, this id no longer exists
   filter(val_unit_id != "RR4-CV-B") %>% 
  mutate(mbn_ug_g_soil = case_when(
    # dealing with missing data, see note at end of 
    # "load data" code block
    val_unit_id == "MW4-CV-A" ~ mbn_mw4_cva,
    TRUE ~ mbn_ug_g_soil
  ))


```

## Stratification options

We could stratify the CIG points based on several different levels of Soil Taxonomy.
How do the CIG sampling points break down in terms of number per cluster, and number per taxonomic level(s) (order, suborder, etc)?

For k-means clusters/groups: here we see that there is only 1 representative from group 4, so probably won't be able to use that group for any calculations.
But the others are workable.

For the different taxonomic levels, suborder seems workable, it has \>= 5 validation units in any given level.
I think order is too broad, and by the time we get to sub-group, we have two subgroups with just 1 validation unit each, so would have to drop those.

```{r}

val_all %>% 
  count(k_8)


val_all %>% 
  count(region)

val_all %>% 
  count(taxorder)

val_all %>% 
  count(taxsuborder)

val_all %>% 
  count(taxgrtgroup)

val_all %>% 
  count(taxsubgrp)

val_all %>% count(taxclname)

xtabs(~region+taxorder, data = val_all)


```

## Crosstabs w/ UD

Was originally thinking about whether we should include or exclude UD (undisturbed/unfarmed sites) because we know they are a major source of variance (they have much higher values across the board for all these indicators).

```{r}
xtabs(~soil_group + taxsuborder, data = val_all)

xtabs(~taxgrtgroup + soil_group, data = val_all)

```

## Crosstabs w/out UD

```{r}
val_farm <- val_all %>% 
  filter(!str_detect(val_unit_id, "UD"))

xtabs(~soil_group + region, data = val_farm)

xtabs(~soil_group + taxsuborder, data = val_farm)

xtabs(~taxgrtgroup + soil_group, data = val_farm)



```

## Plots

```{r}
#| warning: false

# keep only data points for soil regions where we have >1 validation area 
sub_dat <- val_all %>% 
  filter(k_8 %in% c(2, 3, 5:7))

# creating this so I can test difference between including/excluding the UD sites.
farmdat <- sub_dat %>% 
  filter(!str_detect(val_unit_id, "UD"))

sub_dat %>% 
  ggplot(aes(x = region, y = poxc_mg_kg)) + 
  geom_boxplot() +
  geom_quasirandom(aes(color = soil_group)) +
  theme_bw() +
  ggtitle("Region")

sub_dat %>% 
  ggplot(aes(x = soil_group, y = poxc_mg_kg)) +
  geom_boxplot() + 
  geom_quasirandom() +
  theme_bw() + 
  ggtitle("Cluster/Group")


sub_dat %>% 
  ggplot(aes(x = taxsuborder, y = poxc_mg_kg)) +
  geom_boxplot() + 
  geom_quasirandom(aes(color = soil_group)) +
  theme_bw() +
  ggtitle("Suborder")

sub_dat %>% 
  ggplot(aes(x = taxgrtgroup, y = poxc_mg_kg)) +
  geom_boxplot() + 
  geom_quasirandom(aes(color = soil_group)) +
  theme_bw() +
  ggtitle("Great Group") +
  theme(axis.text.x = element_text(angle = 15))


```

## Function to plot model checks

```{r}

check_plots_anova <- function(soil_var, strat, df, log_trans) {
  
  dat_no_na <- df %>%
    select(val_unit_id,
           all_of(strat),
           all_of(soil_var)) %>%
    drop_na(all_of(soil_var))
  
  # checking if any clusters are represented by 1 or less data points. Want to drop these
  # probably not necessary now that we dropped group 4 above,
  # the summary tables under "Stratification options" and
  # "Crosstabs" show that all groups have 3 or more members
  
  n_obs_per_group <- dat_no_na %>%
    count(.data[[strat]])
  
  single_obs_groups <- n_obs_per_group %>%
    filter(n <= 1) %>%
    pull(.data[[strat]])
  
  if (length(single_obs_groups) == 0) {
    dat_subset <- dat_no_na
    
  } else{
    dat_subset <- dat_no_na %>%
      filter(!(.data[[strat]] %in% single_obs_groups))
    
  }
  
  if (log_trans){
    
    f <- paste0("log(", soil_var, ") ~ ", strat)
    
  }else{
    
    f <- paste0(soil_var, " ~ ", strat)
    
  }
  
  mod <- lm(formula = f,
            data = dat_subset)
  
  check_plots <- performance::check_model(mod, check = c("normality", "homogeneity", "linearity"))  
  
  return(list(f_used = f,
              plots = check_plots))
  
}
```

## Models to check

Working with these independent variables (stratification options):

1.  soil_group (k-means);
2.  region
3.  taxsuborder
4.  taxgrtgroup

```{r}

(dep_vars <- c("corr_percent_stable_gr2mm",
               "ugC_g_day",
               "poxc_mg_kg",
               "mbc_ug_g_soil",
               "mbn_ug_g_soil", # include MBN?
               "total_living_microbial_biomass_plfa_ng_g",
               "total_bacteria_plfa_ng_g",
               "total_fungi_plfa_ng_g"
))

# determined by inspecting histograms below w/ different
# transformation options
log_trans <- c(FALSE, 
               TRUE, 
               FALSE, 
               TRUE,
               TRUE,
               TRUE,
               TRUE,
               TRUE)

```

### Checking distributions

Transformations needed:

-   None: aggregate stability, POXC
-   Log10: total biomass PLFA, total bacteria PLFA, total fungi PLFA, MBC, MBN, PMC

```{r}
#| echo: false

plot_transformations <- function(var,
                                 df,
                                 log10_adjust = 0,
                                 ln_adjust = 0,
                                 sqrt_adjust = 0,
                                 nbins = 30) {
  trans_df <- df %>%
    select(val_unit_id, {{var}}) %>%
    mutate(
      log10_trans = log10({{var}} + log10_adjust),
      ln_trans = log({{var}} + ln_adjust),
      sqrt_trans = sqrt({{var}} + sqrt_adjust)) %>%
  pivot_longer(cols = -c(val_unit_id))
  
  var_string <- deparse(substitute(var))


  trans_df %>%
    ggplot(aes(x = value)) +
    geom_histogram(bins = nbins) +
    facet_wrap(vars(name), scales = "free") +
    theme_bw() +
    ggtitle(glue("{var_string}"))

}
```

#### Aggregate stability

```{r}

plot_transformations(var = corr_percent_stable_gr2mm,
                     df = val_all,
                     nbins = 15)

```

#### PLFA Indicators

```{r}

plot_transformations(var = total_living_microbial_biomass_plfa_ng_g,
                     df = val_all,
                     nbins = 15)

plot_transformations(var = total_bacteria_plfa_ng_g,
                     df = val_all,
                     nbins = 15)

plot_transformations(var = total_fungi_plfa_ng_g,
                     df = val_all,
                     nbins = 15)

```

#### Microbial biomass C & N (CFE)

```{r}

plot_transformations(var = mbc_ug_g_soil,
                     df = val_all,
                     nbins = 15)

plot_transformations(var = mbn_ug_g_soil,
                     df = val_all,
                     nbins = 15)

```

#### POXC

```{r}
plot_transformations(var = poxc_mg_kg,
                     df = val_all,
                     nbins = 15)
```

#### PMC

```{r}
plot_transformations(var = ugC_g_day,
                     df = val_all,
                     nbins = 10)
```

### K-means soil groups

```{r}

map2(
  .x = dep_vars,
  .y = log_trans,
  .f = ~ check_plots_anova(
    soil_var = .x,
    log_trans = .y,
    strat = "soil_group",
    df = val_all
  )
)

```

### Region

```{r}
map2(
  .x = dep_vars,
  .y = log_trans,
  .f = ~ check_plots_anova(
    soil_var = .x,
    log_trans = .y,
    strat = "region",
    df = val_all
  )
)
```

### Suborder

```{r}
map2(
  .x = dep_vars,
  .y = log_trans,
  .f = ~ check_plots_anova(
    soil_var = .x,
    log_trans = .y,
    strat = "taxsuborder",
    df = val_all
  )
)
```

### Great Group

```{r}

map2(
  .x = dep_vars,
  .y = log_trans,
  .f = ~ check_plots_anova(
    soil_var = .x,
    log_trans = .y,
    strat = "taxgrtgroup",
    df = val_all
  )
)

```

## Function to test homogeneity of variance

Here I'm running the [Levene Test for Equality of Variances](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm) as a way to check the homogeneity of variance assumption required for ANOVA.
If our groups fail this test, would be better to do Kruskal-Wallis test.

```{r}

levene_test_clusts <- function(soil_var, strat, df) {
  
  ### prep data 
  
  dat_no_na <- df %>%
    select(val_unit_id,
           all_of(strat),
           all_of(soil_var)) %>%
    drop_na(all_of(soil_var)) 
  
  # can't do tests with only 1 obs in a
  # group (b/c need variance calc), so want to filter those out

  n_obs_per_cluster <- dat_no_na %>%
    count(.data[[strat]])

  single_obs_clusters <- n_obs_per_cluster %>%
    filter(n <= 1) %>%
    pull(.data[[strat]])

  if(length(single_obs_clusters) == 0){

    dat_subset <- dat_no_na

  }else{

    dat_subset <- dat_no_na %>%
      filter(!(.data[[strat]] %in% single_obs_clusters))

  }

  # gets rid of warning from levene_test about factor coercion
  dat_subset$strat_factor <- as.factor(dat_subset %>% pull(strat))

  ### perform levene test

  lev_df <- levene_test(data = dat_subset,
                        formula = as.formula(paste0(soil_var, " ~ ", "strat_factor")), center = median) %>%
    mutate(var = soil_var,
           strat_id = strat)

  return(lev_df)

}


```

## Run Levene Tests

Plan to use the results of these tests to determine which test is appropriate, ANOVA (parametric, variance homogeneous between groups) or Kruskal-Wallis (non-parametric, variance NOT homogeneous between groups).

In either case, what I'm trying to get to is a calculation of eta-squared, which "measures the proportion of the total variance in a dependent variable that is associated with the membership of different groups defined by an independent variable" [Richardson, 2011 Educational Research Review](https://doi.org/10.1016/j.edurev.2010.12.001)

When the p-value for the Levene's test is \<0.05 we reject the null hypothesis that the variances of our groups are homogeneous / equal.
This means that traditional ANOVA is **inappropriate**, and we should use a non-parametric / rank based ANOVA like the Kruskal-Wallis test and associated rank-based effect sizes / pariwise comparisons.

```{r}

strat_opts <- c("soil_group", "region", "taxsuborder",
                "taxgrtgroup")

lev_frame <- tidyr::crossing(dep_vars, strat_opts)

lev_all <- lev_frame %>% 
  mutate(lev = map2(.x = dep_vars, .y = strat_opts, 
                    .f = ~levene_test_clusts(
                      soil_var = .x, 
                      strat = .y, 
                      df = val_all
                    )))

lev_long <- list_rbind(lev_all$lev) %>% 
  mutate(test_type = case_when(
    p < 0.05 ~ "non-parametric", 
    p >= 0.05 ~ "parametric"
  ))



```

## Effect sizes for variance explained

I was originally planning on calculating eta-squared as my effect size, using either: [`effectsize::eta_squared()`](https://easystats.github.io/effectsize/reference/eta_squared.html) for ANOVA or [`effectsize::rank_eta_squared()`](https://easystats.github.io/effectsize/reference/rank_epsilon_squared.html)`.` But in reading the [documentation](https://easystats.github.io/effectsize/reference/eta_squared.html) for `eta_squared()` from `{effectsize}`, I learned that there are other, less biased options for calculating this value: omega-squared and epsilon-squared.

I found a very helpful, recent reference, Iacobucci et al., (2023), that explains the differences between eta, epsilon, and omega when used as effect sizes of variance explained.
See their paper for the details, but the easiest way to think about it is that using epsilon-squared or omega-squared is essentially like reporting an adjusted R2 value instead of a regular R2 value.
It's better to use epsilon squared because the equation accounts for small sample size, which is relevant in our case.

Below are a few examples using our dataset, this is just me figuring out what the different functions outputs look like, and how NAs are handled.

-   the non-parametric effect sizes are somewhat easier to code because it's one function call. For the parametric/ANOVA based ones, actually need to create the `lm` , run ANOVA, then pass that object to the appropriate effectsize function to get variance explained.
-   had I wanted to use the `rstatix::kruskal_effsize` , would need to drop NAs beforehand, the function does not do this by default. In contrast, the `{effectsize}` functions all do this, and give you a warning to let you know they did it (helpful).

### Examples

```{r}
## non-parametric (rank) version MBC by region 

# learned that kruskal_effsize doesn't drop NAs by default,
# confirming this with a test dataset.
no_na <- sub_dat %>% 
  filter(!is.na(mbc_ug_g_soil))

# does not drop NAs by default? get a different answer compared to the
# effectsize::rank_eta_squred if running this rstatix function on data with NAs
# in the response var column
rstatix::kruskal_effsize(data = no_na, 
                         formula = mbc_ug_g_soil ~ region)

# default behavior is to drop NAs
effectsize::rank_eta_squared(data = sub_dat, 
                             x = mbc_ug_g_soil ~ region)

# how does epsilon squared compare to eta squared?
effectsize::rank_epsilon_squared(data = sub_dat, 
                                 x = mbc_ug_g_soil ~ region)
```

Below I test out Welch's ANOVA followed by calculation of epsilon squared.
This works, and after reading more from the `{effectsize}` documentation [here](https://easystats.github.io/effectsize/reference/F_to_eta2.html) I think I understand how.
I can pass the result from `oneway.test(var.equal = FALSE)` to `effectsize::epsilon_squared()` and there is an intermediate step performed where the F statistic from `oneway.test` is converted to the effect size (could be partial eta-squared, omega-squared, or epsilon-squared)

```{r}
## parametric version MBC by soil_group (k-means)

# what about welch's ANOVA? 
test_welch2 <- rstatix::welch_anova_test(data = no_na,
                          formula = mbc_ug_g_soil ~ soil_group)



test_welch <- oneway.test(data = sub_dat,
            formula = mbc_ug_g_soil ~ soil_group,
           var.equal = FALSE,
            na.action = "na.omit")

effectsize::epsilon_squared(test_welch, partial = FALSE)


test_mod <- lm(formula = mbc_ug_g_soil ~ soil_group, 
               data = sub_dat)

levene_test(formula = mbc_ug_g_soil ~ soil_group, 
               data = sub_dat)

aov_obj <- car::Anova(mod = test_mod, type = 2)

effectsize::eta_squared(model = aov_obj,
                        # partial b/c one-way test 
                        # we don't have other vars to 
                        # break the results down by
                        partial = FALSE)

effectsize::epsilon_squared(model = aov_obj,
                        # partial b/c one-way test 
                        # we don't have other vars to 
                        # break the results down by
                        partial = FALSE)


```

### Function to calculate epsilon squared

Can specify parametric or non-parametric.

```{r}



calc_epsilon_squared_all <- function(soil_var, strat, type, df = sub_dat) {
  
  df_no_nas <- df %>% 
    drop_na(all_of(soil_var))
 
  my_formula <- as.formula(paste0(soil_var, " ~ ", strat))
  
  if (type == "parametric") {
    
    mod_obj <- lm(formula = my_formula,
                  data = df_no_nas)
    
    aov_obj <- car::Anova(mod = mod_obj, type = 2)
    
    eps_sq <- effectsize::epsilon_squared(model = aov_obj,
                        # partial FALSE b/c one-way test 
                        # we don't have other vars to 
                        # break the variance down by
                        partial = FALSE)
      
  } else if (type == "non-parametric") {
    
    eps_sq <- effectsize::rank_epsilon_squared(data = df_no_nas, 
                                 x = my_formula)
    
  } else {
    
    eps_sq <- "error-specify test type"
    
  }
  

  return(eps_sq)
  
}

calc_epsilon_squared_farms <- function(soil_var, strat, type, df_farm = farmdat){
  
  calc_epsilon_squared_all(soil_var, strat, type, df_farm)
  
}



```

### Calculate epsilon squared

We used the Levene Tests (above) above to determine of our data passed or failed the homogeneity of variance assumption required for traditional ANOVA.
Using the dataframe I generated from running the Levene Tests, I can iterate through all the groups and soil variables and use the epsilon squared function above to calculate the appropriate effect size (parametric or non-parametric).

```{r}

eps_inputs <- lev_long %>% 
  rename(strat = strat_id, 
         soil_var = var, 
         type = test_type) %>% 
  select(soil_var, strat, type) 

eps_results <- eps_inputs %>% 
  mutate(eps_results = pmap(eps_inputs, calc_epsilon_squared_all))


eps_farm <- eps_inputs %>% 
  mutate(eps_results = pmap(eps_inputs, calc_epsilon_squared_farms))


```

### Clean up data

```{r}


farm_long <- eps_farm %>% 
  unnest(eps_results) %>% 
  mutate(epsilon_sq = case_when(
    !is.na(rank_epsilon_squared) ~ rank_epsilon_squared,
    !is.na(Epsilon2) ~ Epsilon2, 
    TRUE ~ 99999
  )) %>% 
  select(-c(Epsilon2, rank_epsilon_squared, Parameter, CI, CI_high, CI_low)) %>% 
  mutate(type = case_when(
    type == "parametric" ~ "p", 
    type == "non-parametric" ~ "np"
  ))
  

eps_long <- eps_results %>% 
  unnest(eps_results) %>% 
  mutate(epsilon_sq = case_when(
    !is.na(rank_epsilon_squared) ~ rank_epsilon_squared,
    !is.na(Epsilon2) ~ Epsilon2, 
    TRUE ~ 99999
  )) %>% 
  select(-c(Epsilon2, rank_epsilon_squared, Parameter, CI, CI_high, CI_low)) %>% 
  mutate(type = case_when(
    type == "parametric" ~ "p", 
    type == "non-parametric" ~ "np"
  ))

eps_wide <- eps_long %>% 
  pivot_wider(values_from = c("epsilon_sq", "type"), 
              names_from = "strat")
```

### Plots of variance explained

```{r}
eps_long %>% 
  filter(strat != "taxgrtgroup") %>% 
  mutate(soil_var = case_when(
    soil_var == "BG_activity" ~ "BG Activity",
    soil_var == "NAG_activity" ~ "NAG Activity",
    soil_var == "mbc_ug_g_soil" ~ "MBC",
    soil_var == "mean_protein_mg_g" ~ "ACE Protein",
    soil_var == "P_activity" ~ "PASE activity", 
    soil_var == "corr_percent_stable_gr2mm" ~ "Aggregate Stab.",
    soil_var == "poxc_mg_kg" ~ "POXC", 
    soil_var == "total_bacteria_plfa_ng_g" ~ "Bacteria PLFA (Total)",
    soil_var == "total_fungi_plfa_ng_g" ~ "Fungi PLFA",
    soil_var == "total_living_microbial_biomass_plfa_ng_g" ~ "Biomass PLFA (Tot.)", 
    soil_var == "poxc_mg_kg" ~ "POXC",
    soil_var == "ugC_g_day" ~ "PMC"
  ),
  strat = case_when(
    strat == "region" ~ "G-REG",
    strat == "soil_group" ~ "KM",
    strat == "taxsuborder" ~ "T-SUB"
  )) %>% 
  ggplot() +
  geom_col(aes(x = strat, y = epsilon_sq)) +
  facet_wrap(vars(soil_var)) + 
  ggtitle("W/ UD")

farm_labelled <- farm_long %>% 
  filter(strat != "taxgrtgroup") %>% 
  mutate(soil_var = case_when(
    soil_var == "BG_activity" ~ "BG Activity",
    soil_var == "NAG_activity" ~ "NAG Activity",
    soil_var == "mbc_ug_g_soil" ~ "MBC",
    soil_var == "mean_protein_mg_g" ~ "ACE Protein",
    soil_var == "P_activity" ~ "PASE activity", 
    soil_var == "corr_percent_stable_gr2mm" ~ "Aggregate Stab.",
    soil_var == "poxc_mg_kg" ~ "POXC", 
    soil_var == "total_bacteria_plfa_ng_g" ~ "Bacteria PLFA",
    soil_var == "total_fungi_plfa_ng_g" ~ "Fungi PLFA",
    soil_var == "total_living_microbial_biomass_plfa_ng_g" ~ "Biomass PLFA (Tot.)", 
    soil_var == "poxc_mg_kg" ~ "POXC",
    soil_var == "ugC_g_day" ~ "PMC"
  ),
  strat = case_when(
    strat == "region" ~ "GEOG REGION",
    strat == "soil_group" ~ "KM GROUP",
    strat == "taxsuborder" ~ "SUBORDER"
  )) %>%
  group_by(soil_var) %>%
  mutate(max_epsilon = max(epsilon_sq),
         top_strat = case_when(
           epsilon_sq == max_epsilon ~ "Y", 
           TRUE ~ "N"
         ))

farm_labelled %>% 
  ggplot() +
  geom_col(aes(x = strat, y = epsilon_sq)) +
  facet_wrap(vars(soil_var)) +
  ggtitle("Without UD")

```

```{r}
reg_best <- farm_labelled %>%
  filter(strat == "GEOG REGION",
         top_strat == "Y") %>% 
  arrange(epsilon_sq)

km_best <- farm_labelled %>% 
  filter(strat == "KM GROUP", 
         top_strat == "Y") %>% 
  arrange(epsilon_sq)

sub_best <- farm_labelled %>% 
  filter(strat == "SUBORDER", 
         top_strat == "Y") %>% 
  arrange(epsilon_sq)

plot_var_order <- c(reg_best$soil_var, km_best$soil_var, 
                    sub_best$soil_var)


eps_bar_plot <- farm_labelled %>%
  ggplot() +
  geom_col(aes(x = soil_var, y = epsilon_sq, fill = strat),
           position = position_dodge(),
           width = 0.7) +
  geom_text(aes(x = 1, y = 0.6, label = "SUBORDER"), size = 3) +
  geom_text(aes(x = 3, y = 0.6, label = "KM GROUP"), size = 3) +
  geom_text(aes(x = 8, y = 0.57, label = "GEOG REGION"), size = 3) +
  scale_x_discrete(limits = rev(plot_var_order)) + 
  scale_y_continuous(breaks = seq(0, 0.7, 0.1)) +
  geom_vline(aes(xintercept = 1.5)) +
  geom_vline(aes(xintercept = 4.5)) +
  geom_text(aes(x = 11, y = 0.32, label = "Explains most variance?"),
            hjust = 0,
            size = 4) +
  coord_cartesian(xlim = c(0, 0.7),
                  clip = 'off') +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom") +
  xlab("") +
  ylab("Epsilon Squared") +
  scale_fill_brewer(type = "qual", palette = 2) +
  guides(fill = guide_legend(title = "Groups"))

ggsave("figs/epsilon_sq_barplot.png", plot = eps_bar_plot, 
       width = 5, height = 5, units = "in")
```

### What is going on with PMC?

PMC returns 0% variance explained for all three stratification options: K-means, Region, and Taxonomic sub-group.
Is this really correct?
Let's double check below with the region stratification to make sure there isn't something weird with the data.

```{r}

lm_pmc <- lm(ugC_g_day ~ region, data = sub_dat, na.action = "na.omit")

aov_pmc <- car::Anova(lm_pmc, type = 2)

# big residuals number... that explains it
aov_pmc

effectsize::epsilon_squared(aov_pmc, partial = FALSE)

# what about eta-squared?
effectsize::eta_squared(aov_pmc, partial = FALSE)

# what if we drop the UD sites?

farmdat <- sub_dat %>% filter(!str_detect(val_unit_id, "UD"))

lm_farm <- lm(ugC_g_day ~ region, data = farmdat, na.action = "na.omit")

aov_farm <- car::Anova(lm_farm, type = 2)

# big residuals number... that explains it
aov_farm

effectsize::epsilon_squared(aov_farm, partial = FALSE)

# what about eta-squared?
effectsize::eta_squared(aov_farm, partial = FALSE)

```

## ANOVA models POXC

Trying this after reading Mourtzinis et al., 2020 about stratifying producer fields to better explain soybean yield variability

```{r}
#| eval: false 


# using poxc as example 
# check for missing values
nrow(sub_dat %>% filter(is.na(poxc_mg_kg)))

# summarise to mapunit level (val_unit_ids)
poxc_test <- sub_dat %>% 
  filter(!is.na(poxc_mg_kg)) %>% 
  group_by(val_unit_id, region, k_8) %>% 
  summarise(mean_poxc = mean(poxc_mg_kg),
            .groups = "drop") %>% 
  mutate(k_8 = glue("cl_{k_8}"))

reglm <- lm(mean_poxc ~ region, data = poxc_test)
#regposlm <- lm(mean_poxc ~ region*position, data = poxc_test)
clustlm <- lm(mean_poxc ~ k_8, data = poxc_test)

performance::check_model(reglm, check = c("normality", "linearity", "homogeneity", "outliers"))

anova_reg <- car::Anova(reglm) %>% broom::tidy() %>% column_to_rownames("term")

# anova_regpos <- car::Anova(regposlm) %>% broom::tidy() %>% column_to_rownames("term")
  
anova_clust <- car::Anova(clustlm) %>% broom::tidy() %>% 
  column_to_rownames("term")

# what percentage of variation is explained by the "region" term
# vs. the "cluster" term vs. the region and cluster terms (and their interaction?)

ss_reg <- anova_reg['region', 'sumsq']/sum(anova_reg['sumsq'])*100

# ss_regpos <- (anova_regpos['region', 'sumsq'] + 
#     anova_regpos['position', 'sumsq'] + 
#     anova_regpos['region:position', 'sumsq'] ) /sum(anova_reg['sumsq'])*100

ss_clust <- anova_clust['k_8', 'sumsq']/sum(anova_clust['sumsq'])*100


ss_reg
ss_regpos
ss_clust
```

## ANOVA models MBC

```{r}
#| eval: false


# using poxc as example 
# check for missing values
nrow(sub_dat %>% filter(is.na(mbc_ug_g_soil)))

mbc_test <- sub_dat %>% 
  filter(!is.na(mbc_ug_g_soil)) %>% 
  group_by(val_unit_id, region, k_8, position) %>% 
  summarise(mean_mbc = mean(mbc_ug_g_soil),
            .groups = "drop") %>% 
  mutate(k_8 = glue("cl_{k_8}"))

reglm <- lm(mean_mbc ~ region, data = mbc_test)
regposlm <- lm(mean_mbc ~ region*position, data = mbc_test)
clustlm <- lm(mean_mbc ~ k_8, data = mbc_test)

anova_reg <- car::Anova(reglm) %>% broom::tidy() %>% column_to_rownames("term")

anova_regpos <- car::Anova(regposlm) %>% broom::tidy() %>% column_to_rownames("term")
  
anova_clust <- car::Anova(clustlm) %>% broom::tidy() %>% 
  column_to_rownames("term")

# what percentage of variation is explained by the "region" term
# vs. the "cluster" term vs. the region and cluster terms (and their interaction?)

ss_reg <- anova_reg['region', 'sumsq']/sum(anova_reg['sumsq'])*100

ss_regpos <- (anova_regpos['region', 'sumsq'] + 
    anova_regpos['position', 'sumsq'] + 
    anova_regpos['region:position', 'sumsq'] ) /sum(anova_reg['sumsq'])*100

ss_clust <- anova_clust['k_8', 'sumsq']/sum(anova_clust['sumsq'])*100


ss_reg
ss_regpos
ss_clust
```

## (Old) Calculate coefficient of variation

Old stuff here but wanted to keep track of the cvequality package link in case it's useful at some other point... My idea was to use coefficient of variation (%) to compare the spread of the points when grouped by geographic region vs. soil group.
I found a helpful R package to do this (which includes stats citations): [`{cvequality}`](https://cran.r-project.org/web/packages/cvequality/vignettes/how_to_test_CVs.html)
